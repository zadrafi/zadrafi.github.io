[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to the blog! Here you’ll find discussions on statistical science, methodology, and applications of statistical methods in research and practice.\n\n\n\nStatistical Inference - Foundations of statistical reasoning\nSensitivity Analysis - Exploring assumption impacts\n\nData Management - Best practices for statistical workflows\nStatistical Workflow - Reproducible research methods"
  },
  {
    "objectID": "blog.html#browse-by-topic",
    "href": "blog.html#browse-by-topic",
    "title": "Blog",
    "section": "",
    "text": "Statistical Inference - Foundations of statistical reasoning\nSensitivity Analysis - Exploring assumption impacts\n\nData Management - Best practices for statistical workflows\nStatistical Workflow - Reproducible research methods"
  },
  {
    "objectID": "posts/statistics/misplacedpower.html",
    "href": "posts/statistics/misplacedpower.html",
    "title": "Misplaced Confidence in Observed Power",
    "section": "",
    "text": "Two months ago, a study came out in JAMA which compared the effectiveness of the antidepressant escitalopram to placebo for long-term major adverse cardiac events (MACE).\nThe authors explained in the methods section of their paper how they calculated their sample size and what differences they were looking for between groups.\nFirst, they used some previously published data to get an idea for incidence rates,\n\n“Because previous studies in this field have shown conflicting results, there was no appropriate reference for power calculation within the designated sample size. The KAMIR study reported a 10.9% incidence of major adverse cardiac events (MACE) over 1 year… Therefore, approximately 50% MACE incidence was expected during a 5-year follow-up.”\n\nThen, they calculated their sample size based on some differences they were interested in finding,\n\n“Assuming 2-sided tests, α = .05, and a follow-up sample size of 300, the expected power was 70% and 96% for detecting 10% and 15% group differences, respectively.”\n\nSo far so good.\nThen, we get to the results,\n\n\n\n\n“A significant difference was found: composite MACE incidence was 40.9% (61/149) in the escitalopram group and 53.6% (81/151) in the placebo group (hazard ratio [HR], 0.69; 95% CI, 0.49-0.96; P = .03). The model assumption was met (Schoenfeld P = .48). The estimated statistical power to detect the observed difference in MACE incidence rates between the 2 groups was 89.7%.”\n\nOuch. This issue ended up bothering me so much that I wrote a letter to the editor (LTE) to point out the issue. Unfortunately, the LTE got rejected, but Andrew Althouse suggested that I discuss this over at DataMethods, so I did, and I also discussed it on Twitter but also wanted to publish the LTE on my blog. Here it is.\nThis letter has now been preprinted on arXiv.\nIn a similar tale, a group of surgeons published a methodological article advocating this practice of calculating observed power, which I further discuss here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Less Likely",
    "section": "",
    "text": "Discussions on statistical science and the applications of statistical methods"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Less Likely",
    "section": "1.1 Recent Posts",
    "text": "1.1 Recent Posts\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nYour Models Are Neither Useful Nor Approximate\n\n\n\nstatistics\n\n\n\nA discussion about models and the assumptions that underlie them.\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Stata\n\n\nA simple guide on how to produce consonance functions in Stata.\n\n\n\n\n\n\n\n\n\n\n\n\nStan\n\n\nSome sample scripts\n\n\n\n\n\n\n\n\n\n\n\n\nYour Models Are Neither Useful Nor Approximate\n\n\n\nstatistics\n\n\n\nA discussion about models and the assumptions that underlie them.\n\n\n\n\n\n\n\n\n\n\n\n\nStan\n\n\nSome sample scripts\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Stata\n\n\nA simple guide on how to produce consonance functions in Stata.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#latest-in-statistics",
    "href": "index.html#latest-in-statistics",
    "title": "Less Likely",
    "section": "1.2 Latest in Statistics",
    "text": "1.2 Latest in Statistics\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#topics-we-cover",
    "href": "index.html#topics-we-cover",
    "title": "Less Likely",
    "section": "1.3 Topics We Cover",
    "text": "1.3 Topics We Cover\n\nSensitivity Analysis: Exploring how conclusions change under different assumptions\nStatistical Assumptions: Critical examination of model assumptions\nRobust Methods: Statistics that work when assumptions fail\nCausal Inference: Drawing causal conclusions from observational data\nBootstrap Methods: Resampling techniques for uncertainty quantification\nData Management: Best practices for statistical workflows"
  },
  {
    "objectID": "index.html#stay-connected",
    "href": "index.html#stay-connected",
    "title": "Less Likely",
    "section": "1.4 Stay Connected",
    "text": "1.4 Stay Connected\n\n\n\n1.4.1 Subscribe\nGet updates on new posts via RSS or follow on Twitter :::\n\n\n1.4.2 Contribute\nFound an issue or want to contribute? Visit the GitHub repository :::"
  },
  {
    "objectID": "posts/statistics/interim_analyses.html",
    "href": "posts/statistics/interim_analyses.html",
    "title": "Simulation of a Two-Group Parallel-Arm RCT with Interim Analyses",
    "section": "",
    "text": "Recently Andrew Althouse informed me that he was going to simulate a two-group parallel-arm randomized trial with interim analyses using the rpact R package, so I offered to also help in constructing the R code to do so. He already has a number of R scripts on his GitHub repo for doing similar simulations, which can be viewed here and a number of tweets explaining these simulations. For this example, his goal was to simulate a trial where the outcome was binary and the probability of death for each group could be tuned in addition to:\n\n\nthe total number of participants\n\n\n\nthe number of interim analyses\n\n\n\nthe schedule of the interim analyses\n\n\n\nthe group-sequential design used\n\n\nalong with the usual trial analysis parameters such as:\n\n\nthe \\(\\alpha\\)-level\n\n\n\nthe type of test (1-sided vs. 2-sided).\n\n\nThe goal was to be able to produce a table of various statistics such as:\n\n\nthe odds ratio\n\n\n\nthe confidence limits\n\n\n\nthe \\(P\\)-value\n\n\n\nthe number of successes\n\n\nfor each of the interim analyses specified.\n\nThe function below is a reflection of our efforts to do so, and also returns several plots from the rpact package for the design that is chosen along with a plot comparing the design to other designs. In order to get similar results, you will need to load the R function first, and then simply enter the proper inputs. While there may be more efficient ways to write the code, for example using lapply() instead of for loops, we have chosen not to do so, and we have also tried to minimize the number of R packages necessary for the function to work but the following will be required:\n\n\nrpact\n\n\n\nstringr\n\n\nYou can quickly install and load both using:\n\nreq_packs &lt;- c(\"rpact\", \"stringr\", \"ggplot2\")\ninstall.packages(req_packs)\nlapply(req_packs, library, character.only = TRUE)\n\n\n1 Setting up the Function\n\n#' @title Simulation of a Two-Group Parallel-Arm Trial With Interim Analyses\n#' @docType Custom function for simulation from the rpact package\n#' @author Andrew Althouse with edits by Zad Rafi\n#' NOTE: If you want to confirm \"type 1 error\" under different stopping rules,\n#' make death = in the two treatment arms (e.g. no treatment effect)\n#' NOTE: I have set this one up to test the power for a treatment that would reduce mortality\n#' from 40% in control group (1) to 30% in treatment group (2)\n#' NOTE: Trial Design Parameters - Part 1\n#' Here we will specify the basics: total N patients to enroll, and death rate for each treatment arm\n#' NOTE: Trial Design Parameters - Part 2\n#' Here we will define the interim analysis strategy and stopping rules\n#' For this trial we will include provisions for efficacy stopping only (no pre-specified futility stopping)\n#' We will use the rpact package to compute the stopping/success thresholds at the interim and final analysis\n#' NOTE: Required packages: rpact and stringr\n#' @param nSims # The number of simulations, the default is 1000\n#' @param nPatients # here is where you specify the planned max number of patients you want included in each RCT\n#' @param death1 # here is where you specify the event rate for patients receiving 'treatment 1' in these trial\n#' @param death2 # here is where you specify the event rate for patients receiving 'treatment 2' in these trials\n#' @param nLooks # here is where you put the number of looks that will take place (INCLUDING the final analysis)\n#' @param analyses_scheduled # schedule of interim analyses\n#' @param sided # Whether the test is 1-sided or 2-sided\n#' @param alpha # Specified alpha level, the default is 0.05\n#' @param informationRates #\n#' @param trials # The total number of trials you wish to load in the table results. \n#' @param typeOfDesign # The type of design.\n#' @param seed # Argument to set the seed for the simulations\n#' @return list of dataframes and plots\n#' @example See below this code block\ninterim_sim &lt;- function(nPatients = 1000, death1 = 0.4, death2 = 0.3, \n                        nLooks = 4, analyses_scheduled = c(0.25, 0.50, 0.75, 1),\n                        sided = 1, alpha = 0.05, \n                        informationRates = analyses_scheduled, \n                        typeOfDesign = \"asOF\", nSims = 1000, trials = 10,\n                        seed = 1031) {\n\nefficacy_thresholds &lt;- numeric(nLooks)\ndesign &lt;- getDesignGroupSequential(\n  sided = sided, alpha = alpha,\n  informationRates = analyses_scheduled,\n  typeOfDesign = typeOfDesign)\ndesign_2 &lt;- getDesignGroupSequential(typeOfDesign = \"P\") # Pocock\ndesign_3 &lt;- getDesignGroupSequential(typeOfDesign = \"asP\") # Alpha-spending Pocock\ndesign_4 &lt;- getDesignGroupSequential(typeOfDesign = \"OF\") # O'Brien-Fleming\ndesignSet &lt;- getDesignSet(designs = c(design, design_2,\n                                      design_3, design_4),\n                          variedParameters = \"typeOfDesign\")\n\nRNGkind(kind = \"L'Ecuyer-CMRG\")\nset.seed(seed)\nfor (j in 1:nLooks) {\n  efficacy_thresholds[j] &lt;- design$stageLevels[j]\n}\n\nanalyses_nPatients &lt;- analyses_scheduled * nPatients\nefficacy_thresholds\npb &lt;- txtProgressBar(min = 0, max = nSims, initial = 0, style = 3)\ntrialnum &lt;- numeric(nSims)\nor &lt;- data.frame(matrix(ncol = nLooks, nrow = nSims))\nlcl &lt;- data.frame(matrix(ncol = nLooks, nrow = nSims))\nucl &lt;- data.frame(matrix(ncol = nLooks, nrow = nSims))\npval &lt;- data.frame(matrix(ncol = nLooks, nrow = nSims))\nsuccess &lt;- data.frame(matrix(ncol = nLooks, nrow = nSims))\n\nstrings &lt;- c(\"OR_%d\", \"LCL_%d\", \"UCL_%d\",\n             \"Pval_%d\", \"Success_%d\")\n\ncolnames(or) &lt;- sprintf(\"OR_%d\", (1:nLooks))\ncolnames(lcl) &lt;- sprintf(\"LCL_%d\", (1:nLooks))\ncolnames(ucl) &lt;- sprintf(\"UCL_%d\", (1:nLooks))\ncolnames(pval) &lt;- sprintf(\"Pval_%d\", (1:nLooks))\ncolnames(success) &lt;- sprintf(\"Success_%d\", (1:nLooks))\noverall_success &lt;- numeric(nSims)\ndf &lt;- data.frame(trialnum, or, lcl, ucl,\n                 pval, success, overall_success)\n\nRNGkind(kind = \"L'Ecuyer-CMRG\")\nset.seed(seed)\ntime &lt;- system.time(\nfor (i in 1:nSims) {\n  trialnum[i] &lt;- i\n  pid &lt;- seq(1, nPatients, by = 1)\n  treatment &lt;- rep(1:2, nPatients / 2)\n  deathprob &lt;- numeric(nPatients)\n  deathprob[treatment == 1] &lt;- death1\n  deathprob[treatment == 2] &lt;- death2\n  death &lt;- rbinom(nPatients, 1, deathprob)\n  trialdata &lt;- data.frame(cbind(pid, treatment, death))\n\n  for (j in 1:nLooks) {\n    analysisdata &lt;- subset(trialdata, pid &lt;= analyses_nPatients[j])\n    model &lt;- glm(death ~ treatment,\n                 family = binomial(link = \"logit\"),\n                 data = analysisdata)\n    or[i, j] &lt;- exp(summary(model)$coefficients[2])\n    lcl[i, j] &lt;- exp(confint.default((model))[2, 1])\n    ucl[i, j] &lt;- exp(confint.default((model))[2, 2])\n    pval[i, j] &lt;- summary(model)$coefficients[8]\n    success[i, j] &lt;- ifelse(or[i, j] &lt; 1 & pval[i, j] &lt; efficacy_thresholds[j], 1, 0)\n  }\n  overall_success[i] &lt;- 0\n  for (j in 1:nLooks) {\n    if (success[i, j] == 1) {\n      overall_success[i] &lt;- 1\n    }\n  }\n    setTxtProgressBar(pb, i)\n})\n\ndf &lt;- data.frame(trialnum, or, lcl, ucl, pval,\n                 success, overall_success)\nsimulation_results &lt;- data.frame(matrix(vector(),\n  nrow = nPatients, ncol = (length(df))))\ncolnames(simulation_results) &lt;- c(\"trialnum\", (do.call(\n  rbind,\n  lapply(1:length(strings),\n    FUN = function(j) {\n      (do.call(rbind, lapply(1:nLooks,\n        FUN = function(i) ((sprintf(strings, i)))\n      )[]))[, j]\n    }\n  )\n)), \"overall_success\")\nsimulation_results[intersect(names(df), names(simulation_results))] &lt;- (df[intersect(\n  names(df),\n  names(simulation_results)\n)])\nsimulation_results &lt;- as.data.frame(simulation_results)\n\noutputs &lt;- simulation_results[, c(-1, -length(simulation_results))]\n\ncols &lt;- as.character(1:nLooks)\nrows &lt;- as.character(1:length(strings))\ninterim &lt;- matrix(\n  nrow = length(strings), ncol = nLooks,\n  dimnames = list((rows), (cols)), \n  data = rep(0, (length(strings) * nLooks)))\n\nfor (i in cols) {\n  interim[, i] &lt;- str_subset(colnames(outputs), i)\n}\n\ncolnames(interim) &lt;- sprintf(\"Interim_Look_%d\", (1:nLooks))\ncolnames(simulation_results)[1] &lt;- c(\"TrialNum\")\ncolnames(simulation_results)[length(simulation_results)] &lt;- c(\"Overall_Success\")\nsimulation_results_trials &lt;- head(simulation_results, trials)\nresults &lt;- list(\n  summary(design),\n  summary(time),\n  plot(design, 1) +\n    theme_light() +\n    theme(\n      plot.title = element_text(size = 14, face = \"bold\"),\n      axis.title = element_text(size = 12, face = \"bold\")\n    ),\n  plot(designSet, type = 1) +\n    theme_light() +\n    theme(\n      plot.title = element_text(size = 14, face = \"bold\"),\n      axis.title = element_text(size = 12, face = \"bold\")\n    ),\n  table(overall_success),\n  simulation_results_trials)\n\nnames(results) &lt;- c(\n  \"Design Summary\", \"Time to complete simulation\", \n  \"Main Design Plot\", \"Plot of Various Designs\", \n  \"Table of Overall Success\", \"Simulation Results\")\n\nreturn(results)\n}\n\n\n\n2 A Simulated Example\n\n\nresults &lt;- interim_sim(nPatients = 1000, death1 = 0.4, death2 = 0.3, nLooks = 4,\n                       analyses_scheduled = c(0.25, 0.50, 0.75, 1),\n                       sided = 1, alpha = 0.025, typeOfDesign = \"asOF\",\n                       informationRates =  c(0.25, 0.50, 0.75, 1),\n                       nSims = 1000, trials = 20, seed = 1031)\n\n\n\n3 Examining the Results\nresults[1:5]\n#&gt; $`Design Summary`\n#&gt; \n#&gt; $`Time to complete simulation`\n#&gt;    user  system elapsed \n#&gt;   7.053   0.672   7.723 \n#&gt; \n#&gt; $`Main Design Plot`\n#&gt; NULL\n#&gt; \n#&gt; $`Plot of Various Designs`\n#&gt; NULL\n#&gt; \n#&gt; $`Table of Overall Success`\n#&gt; overall_success\n#&gt;   0   1 \n#&gt; 134 866\n\n\nTo examine the results, I used the kableExtra package, though this is not necessary, and simply using the following script will suffice\n\ntable_results &lt;- results[[6]]\nView(table_results) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterim_Look_1\n\n\n\n\nInterim_Look_2\n\n\n\n\nInterim_Look_3\n\n\n\n\nInterim_Look_4\n\n\n\n\n\n\n\nTrialNum\n\n\nOR_1\n\n\nLCL_1\n\n\nUCL_1\n\n\nPval_1\n\n\nSuccess_1\n\n\nOR_2\n\n\nLCL_2\n\n\nUCL_2\n\n\nPval_2\n\n\nSuccess_2\n\n\nOR_3\n\n\nLCL_3\n\n\nUCL_3\n\n\nPval_3\n\n\nSuccess_3\n\n\nOR_4\n\n\nLCL_4\n\n\nUCL_4\n\n\nPval_4\n\n\nSuccess_4\n\n\nOverall_Success\n\n\n\n\n\n\n1\n\n\n0.936\n\n\n0.565\n\n\n1.55\n\n\n0.797\n\n\n0\n\n\n0.868\n\n\n0.601\n\n\n1.255\n\n\n0.453\n\n\n0\n\n\n1.036\n\n\n0.767\n\n\n1.398\n\n\n0.818\n\n\n0\n\n\n0.891\n\n\n0.687\n\n\n1.157\n\n\n0.387\n\n\n0\n\n\n0\n\n\n\n\n2\n\n\n0.87\n\n\n0.519\n\n\n1.459\n\n\n0.598\n\n\n0\n\n\n0.757\n\n\n0.525\n\n\n1.092\n\n\n0.136\n\n\n0\n\n\n0.745\n\n\n0.551\n\n\n1.007\n\n\n0.056\n\n\n0\n\n\n0.721\n\n\n0.555\n\n\n0.936\n\n\n0.014\n\n\n1\n\n\n1\n\n\n\n\n3\n\n\n0.59\n\n\n0.35\n\n\n0.996\n\n\n0.048\n\n\n0\n\n\n0.555\n\n\n0.382\n\n\n0.806\n\n\n0.002\n\n\n0\n\n\n0.676\n\n\n0.5\n\n\n0.916\n\n\n0.011\n\n\n0\n\n\n0.686\n\n\n0.529\n\n\n0.89\n\n\n0.005\n\n\n1\n\n\n1\n\n\n\n\n4\n\n\n0.652\n\n\n0.385\n\n\n1.103\n\n\n0.111\n\n\n0\n\n\n0.671\n\n\n0.461\n\n\n0.976\n\n\n0.037\n\n\n0\n\n\n0.725\n\n\n0.533\n\n\n0.988\n\n\n0.042\n\n\n0\n\n\n0.75\n\n\n0.576\n\n\n0.976\n\n\n0.032\n\n\n0\n\n\n0\n\n\n\n\n5\n\n\n0.674\n\n\n0.398\n\n\n1.142\n\n\n0.143\n\n\n0\n\n\n0.763\n\n\n0.526\n\n\n1.108\n\n\n0.155\n\n\n0\n\n\n0.835\n\n\n0.617\n\n\n1.132\n\n\n0.246\n\n\n0\n\n\n0.764\n\n\n0.587\n\n\n0.994\n\n\n0.045\n\n\n0\n\n\n0\n\n\n\n\n6\n\n\n0.525\n\n\n0.309\n\n\n0.892\n\n\n0.017\n\n\n0\n\n\n0.622\n\n\n0.427\n\n\n0.907\n\n\n0.014\n\n\n0\n\n\n0.668\n\n\n0.491\n\n\n0.909\n\n\n0.01\n\n\n0\n\n\n0.625\n\n\n0.479\n\n\n0.815\n\n\n0.001\n\n\n1\n\n\n1\n\n\n\n\n7\n\n\n0.742\n\n\n0.433\n\n\n1.269\n\n\n0.275\n\n\n0\n\n\n0.769\n\n\n0.525\n\n\n1.125\n\n\n0.175\n\n\n0\n\n\n0.76\n\n\n0.558\n\n\n1.037\n\n\n0.083\n\n\n0\n\n\n0.719\n\n\n0.551\n\n\n0.938\n\n\n0.015\n\n\n1\n\n\n1\n\n\n\n\n8\n\n\n0.661\n\n\n0.394\n\n\n1.108\n\n\n0.116\n\n\n0\n\n\n0.437\n\n\n0.301\n\n\n0.632\n\n\n0\n\n\n1\n\n\n0.535\n\n\n0.397\n\n\n0.721\n\n\n0\n\n\n1\n\n\n0.551\n\n\n0.425\n\n\n0.714\n\n\n0\n\n\n1\n\n\n1\n\n\n\n\n9\n\n\n1.288\n\n\n0.76\n\n\n2.183\n\n\n0.348\n\n\n0\n\n\n1.113\n\n\n0.769\n\n\n1.612\n\n\n0.571\n\n\n0\n\n\n0.87\n\n\n0.645\n\n\n1.173\n\n\n0.361\n\n\n0\n\n\n0.771\n\n\n0.596\n\n\n0.999\n\n\n0.049\n\n\n0\n\n\n0\n\n\n\n\n10\n\n\n0.783\n\n\n0.466\n\n\n1.316\n\n\n0.356\n\n\n0\n\n\n0.701\n\n\n0.484\n\n\n1.015\n\n\n0.06\n\n\n0\n\n\n0.655\n\n\n0.482\n\n\n0.89\n\n\n0.007\n\n\n1\n\n\n0.614\n\n\n0.472\n\n\n0.798\n\n\n0\n\n\n1\n\n\n1\n\n\n\n\n11\n\n\n0.577\n\n\n0.344\n\n\n0.968\n\n\n0.037\n\n\n0\n\n\n0.639\n\n\n0.444\n\n\n0.921\n\n\n0.016\n\n\n0\n\n\n0.56\n\n\n0.415\n\n\n0.754\n\n\n0\n\n\n1\n\n\n0.577\n\n\n0.446\n\n\n0.748\n\n\n0\n\n\n1\n\n\n1\n\n\n\n\n12\n\n\n0.538\n\n\n0.315\n\n\n0.919\n\n\n0.023\n\n\n0\n\n\n0.629\n\n\n0.434\n\n\n0.913\n\n\n0.015\n\n\n0\n\n\n0.576\n\n\n0.426\n\n\n0.78\n\n\n0\n\n\n1\n\n\n0.561\n\n\n0.431\n\n\n0.731\n\n\n0\n\n\n1\n\n\n1\n\n\n\n\n13\n\n\n0.79\n\n\n0.474\n\n\n1.315\n\n\n0.364\n\n\n0\n\n\n0.763\n\n\n0.531\n\n\n1.095\n\n\n0.142\n\n\n0\n\n\n0.789\n\n\n0.588\n\n\n1.06\n\n\n0.115\n\n\n0\n\n\n0.791\n\n\n0.611\n\n\n1.025\n\n\n0.076\n\n\n0\n\n\n0\n\n\n\n\n14\n\n\n0.535\n\n\n0.318\n\n\n0.902\n\n\n0.019\n\n\n0\n\n\n0.69\n\n\n0.477\n\n\n0.999\n\n\n0.049\n\n\n0\n\n\n0.583\n\n\n0.43\n\n\n0.792\n\n\n0.001\n\n\n1\n\n\n0.621\n\n\n0.477\n\n\n0.809\n\n\n0\n\n\n1\n\n\n1\n\n\n\n\n15\n\n\n0.695\n\n\n0.419\n\n\n1.152\n\n\n0.158\n\n\n0\n\n\n0.744\n\n\n0.516\n\n\n1.073\n\n\n0.114\n\n\n0\n\n\n0.73\n\n\n0.541\n\n\n0.985\n\n\n0.04\n\n\n0\n\n\n0.64\n\n\n0.493\n\n\n0.83\n\n\n0.001\n\n\n1\n\n\n1\n\n\n\n\n16\n\n\n1.116\n\n\n0.657\n\n\n1.896\n\n\n0.685\n\n\n0\n\n\n0.763\n\n\n0.526\n\n\n1.108\n\n\n0.155\n\n\n0\n\n\n0.69\n\n\n0.509\n\n\n0.935\n\n\n0.017\n\n\n0\n\n\n0.69\n\n\n0.532\n\n\n0.896\n\n\n0.005\n\n\n1\n\n\n1\n\n\n\n\n17\n\n\n0.533\n\n\n0.32\n\n\n0.886\n\n\n0.015\n\n\n0\n\n\n0.585\n\n\n0.407\n\n\n0.839\n\n\n0.004\n\n\n0\n\n\n0.679\n\n\n0.504\n\n\n0.913\n\n\n0.011\n\n\n0\n\n\n0.622\n\n\n0.48\n\n\n0.806\n\n\n0\n\n\n1\n\n\n1\n\n\n\n\n18\n\n\n0.613\n\n\n0.364\n\n\n1.033\n\n\n0.066\n\n\n0\n\n\n0.692\n\n\n0.478\n\n\n1\n\n\n0.05\n\n\n0\n\n\n0.78\n\n\n0.577\n\n\n1.055\n\n\n0.107\n\n\n0\n\n\n0.705\n\n\n0.543\n\n\n0.914\n\n\n0.008\n\n\n1\n\n\n1\n\n\n\n\n19\n\n\n0.841\n\n\n0.502\n\n\n1.409\n\n\n0.511\n\n\n0\n\n\n0.768\n\n\n0.531\n\n\n1.11\n\n\n0.16\n\n\n0\n\n\n0.685\n\n\n0.506\n\n\n0.927\n\n\n0.014\n\n\n0\n\n\n0.665\n\n\n0.511\n\n\n0.864\n\n\n0.002\n\n\n1\n\n\n1\n\n\n\n\n20\n\n\n0.275\n\n\n0.159\n\n\n0.476\n\n\n0\n\n\n1\n\n\n0.449\n\n\n0.31\n\n\n0.651\n\n\n0\n\n\n1\n\n\n0.498\n\n\n0.369\n\n\n0.672\n\n\n0\n\n\n1\n\n\n0.498\n\n\n0.384\n\n\n0.646\n\n\n0\n\n\n1\n\n\n1\n\n\n\n\n\n\nAbbreviations: \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n TrialNum: Trial Number | OR: Odds Ratio | LCL: Lower Confidence Level | UCL: Upper Confidence Level | Pval: P-value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 Statistical Environment\n\nThe analyses were run on:\n\nsi &lt;- sessionInfo()\nprint(si, RNG = TRUE, locale = TRUE)\n#&gt; R version 4.5.0 (2025-04-11)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; Random number generation:\n#&gt;  RNG:     L'Ecuyer-CMRG \n#&gt;  Normal:  Inversion \n#&gt;  Sample:  Rejection \n#&gt;  \n#&gt; locale:\n#&gt; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#&gt; \n#&gt; time zone: America/New_York\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt;  [1] splines   grid      stats4    parallel  stats     graphics  grDevices\n#&gt;  [8] utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] cli_3.6.5             texPreview_2.1.0      tinytex_0.57         \n#&gt;  [4] rmarkdown_2.29        brms_2.22.0           bootImpute_1.2.2     \n#&gt;  [7] knitr_1.50            boot_1.3-31           gtsummary_2.2.0      \n#&gt; [10] reshape2_1.4.4        ProfileLikelihood_1.3 ImputeRobust_1.3-1   \n#&gt; [13] gamlss_5.4-22         gamlss.dist_6.1-1     gamlss.data_6.0-6    \n#&gt; [16] mvtnorm_1.3-3         performance_0.14.0    summarytools_1.1.4   \n#&gt; [19] rpact_4.2.1           tidybayes_3.0.7       htmltools_0.5.8.1    \n#&gt; [22] Statamarkdown_0.9.2   car_3.1-3             carData_3.0-5        \n#&gt; [25] qqplotr_0.0.6         ggcorrplot_0.1.4.1    mitml_0.4-5          \n#&gt; [28] pbmcapply_1.5.1       Amelia_1.8.3          Rcpp_1.0.14          \n#&gt; [31] blogdown_1.21         doParallel_1.0.17     iterators_1.0.14     \n#&gt; [34] foreach_1.5.2         lattice_0.22-7        bayesplot_1.12.0     \n#&gt; [37] wesanderson_0.3.7     VIM_6.2.2             colorspace_2.1-1     \n#&gt; [40] here_1.0.1            progress_1.2.3        loo_2.8.0            \n#&gt; [43] mi_1.1                Matrix_1.7-3          broom_1.0.8          \n#&gt; [46] yardstick_1.3.2       svglite_2.2.1         Cairo_1.6-2          \n#&gt; [49] cowplot_1.1.3         mgcv_1.9-3            nlme_3.1-168         \n#&gt; [52] xfun_0.52             broom.mixed_0.2.9.6   reticulate_1.42.0    \n#&gt; [55] kableExtra_1.4.0      posterior_1.6.1       checkmate_2.3.2      \n#&gt; [58] parallelly_1.45.0     miceFast_0.8.5        randomForest_4.7-1.2 \n#&gt; [61] missForest_1.5        miceadds_3.17-44      quantreg_6.1         \n#&gt; [64] SparseM_1.84-2        MCMCpack_1.7-1        MASS_7.3-65          \n#&gt; [67] coda_0.19-4.1         latex2exp_0.9.6       rstan_2.32.7         \n#&gt; [70] StanHeaders_2.32.10   lubridate_1.9.4       forcats_1.0.0        \n#&gt; [73] stringr_1.5.1         dplyr_1.1.4           purrr_1.0.4          \n#&gt; [76] readr_2.1.5           tibble_3.2.1          ggplot2_3.5.2        \n#&gt; [79] tidyverse_2.0.0       ggtext_0.1.2          concurve_2.7.7       \n#&gt; [82] showtext_0.9-7        showtextdb_3.0        sysfonts_0.8.9       \n#&gt; [85] future.apply_1.11.3   future_1.58.0         tidyr_1.3.1          \n#&gt; [88] magrittr_2.0.3        mice_3.18.0           rms_8.0-0            \n#&gt; [91] Hmisc_5.2-3          \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] dichromat_2.0-0.1       nnet_7.3-20             TH.data_1.1-3          \n#&gt;   [4] vctrs_0.6.5             digest_0.6.37           png_0.1-8              \n#&gt;   [7] shape_1.4.6.1           proxy_0.4-27            magick_2.8.6           \n#&gt;  [10] fontLiberation_0.1.0    withr_3.0.2             ggpubr_0.6.0           \n#&gt;  [13] survival_3.8-3          doRNG_1.8.6.2           emmeans_1.11.1         \n#&gt;  [16] MatrixModels_0.5-4      systemfonts_1.2.3       ragg_1.4.0             \n#&gt;  [19] zoo_1.8-14              V8_6.0.4                ggdist_3.3.3           \n#&gt;  [22] DEoptimR_1.1-3-1        Formula_1.2-5           prettyunits_1.2.0      \n#&gt;  [25] rematch2_2.1.2          httr_1.4.7              rstatix_0.7.2          \n#&gt;  [28] globals_0.18.0          rstudioapi_0.17.1       extremevalues_2.4.1    \n#&gt;  [31] pan_1.9                 generics_0.1.4          base64enc_0.1-3        \n#&gt;  [34] curl_6.2.3              mitools_2.4             desc_1.4.3             \n#&gt;  [37] xtable_1.8-4            svUnit_1.0.6            pracma_2.4.4           \n#&gt;  [40] evaluate_1.0.3          hms_1.1.3               glmnet_4.1-9           \n#&gt;  [43] lmtest_0.9-40           robustbase_0.99-4-1     matrixStats_1.5.0      \n#&gt;  [46] svgPanZoom_0.3.4        class_7.3-23            pillar_1.10.2          \n#&gt;  [49] caTools_1.18.3          compiler_4.5.0          stringi_1.8.7          \n#&gt;  [52] jomo_2.7-6              minqa_1.2.8             plyr_1.8.9             \n#&gt;  [55] crayon_1.5.3            abind_1.4-8             metadat_1.4-0          \n#&gt;  [58] sp_2.2-0                mathjaxr_1.8-0          rapportools_1.2        \n#&gt;  [61] twosamples_2.0.1        sandwich_3.1-1          whisker_0.4.1          \n#&gt;  [64] codetools_0.2-20        multcomp_1.4-28         textshaping_1.0.1      \n#&gt;  [67] bcaboot_0.2-3           openssl_2.3.3           flextable_0.9.9        \n#&gt;  [70] QuickJSR_1.7.0          e1071_1.7-16            gridtext_0.1.5         \n#&gt;  [73] lme4_1.1-37             fs_1.6.6                itertools_0.1-3        \n#&gt;  [76] listenv_0.9.1           Rdpack_2.6.4            pkgbuild_1.4.8         \n#&gt;  [79] estimability_1.5.1      ggsignif_0.6.4          tzdb_0.5.0             \n#&gt;  [82] pkgconfig_2.0.3         tools_4.5.0             rbibutils_2.3          \n#&gt;  [85] viridisLite_0.4.2       DBI_1.2.3               numDeriv_2016.8-1.1    \n#&gt;  [88] fastmap_1.2.0           scales_1.4.0            officer_0.6.10         \n#&gt;  [91] opdisDownsampling_1.0.1 insight_1.3.0           rpart_4.1.24           \n#&gt;  [94] farver_2.1.2            reformulas_0.4.1        survminer_0.5.0        \n#&gt;  [97] yaml_2.3.10             foreign_0.8-90          lifecycle_1.0.4        \n#&gt; [100] askpass_1.2.1           backports_1.5.0         Brobdingnag_1.2-9      \n#&gt; [103] timechange_0.3.0        gtable_0.3.6            arrayhelpers_1.1-0     \n#&gt; [106] metafor_4.8-0           jsonlite_2.0.0          bitops_1.0-9           \n#&gt; [109] qqconf_1.3.2            zip_2.3.3               ranger_0.17.0          \n#&gt; [112] RcppParallel_5.1.10     polspline_1.1.25        bridgesampling_1.1-2   \n#&gt; [115] survMisc_0.5.6          distributional_0.5.0    pander_0.6.6           \n#&gt; [118] details_0.4.0           KMsurv_0.1-6            rappdirs_0.3.3         \n#&gt; [121] glue_1.8.0              tcltk_4.5.0             gdtools_0.4.2          \n#&gt; [124] rprojroot_2.0.4         mcmc_0.9-8              gridExtra_2.3          \n#&gt; [127] R6_2.6.1                arm_1.14-4              labeling_0.4.3         \n#&gt; [130] km.ci_0.5-6             vcd_1.4-13              clipr_0.8.0            \n#&gt; [133] cluster_2.1.8.1         rngtools_1.5.2          nloptr_2.2.1           \n#&gt; [136] rstantools_2.4.0        tidyselect_1.2.1        htmlTable_2.4.3        \n#&gt; [139] tensorA_0.36.2.1        xml2_1.3.8              inline_0.3.21          \n#&gt; [142] fontBitstreamVera_0.1.1 furrr_0.3.1             laeken_0.5.3           \n#&gt; [145] pryr_0.1.6              fontquiver_0.2.1        data.table_1.17.4      \n#&gt; [148] htmlwidgets_1.6.4       RColorBrewer_1.1-3      rlang_1.1.6            \n#&gt; [151] uuid_1.2-1\n\n\n\n5 Article Citation"
  },
  {
    "objectID": "posts/statistics/s-values.html",
    "href": "posts/statistics/s-values.html",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "",
    "text": "The \\(P\\)-value doesn’t have many fans. There are those who don’t understand it, often treating it as a measure it’s not, whether that’s a posterior probability, the probability of getting results due to chance alone, or some other bizarre/incorrect interpretation. [1;2;3] Then there are those who dislike it because they think the concept is too difficult to understand or because they see it as a noisy statistic we’re not interested in.\nHowever, the groups of people mentioned above aren’t mutually exclusive. Many who dislike and criticize the \\(P\\)-value also do not understand its properties and behavior. This is unfortunate, given how important and widely used they are. In this article, which could also have been titled, \\(P\\)-values: More Than You Ever Wanted to Know, I take on the task of explaining:"
  },
  {
    "objectID": "posts/statistics/s-values.html#some-definitions-descriptions",
    "href": "posts/statistics/s-values.html#some-definitions-descriptions",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "1.1 Some Definitions & Descriptions",
    "text": "1.1 Some Definitions & Descriptions\n\nThe \\(P\\)-value is the probability of getting a result (specifically, a test statistic) at least as extreme as what was observed if every model assumption, in addition to the targeted test hypothesis (usually a null hypothesis), used to compute it were correct. [4;3;5]\nA simple, mathematically rigorous definition of a \\(P\\)-value (for those interested) is given by Stark (2015).\n\n\nLet \\(P\\) be the probability distribution of the data \\(X\\), which takes values in the measurable space \\(\\mathcal{X}\\). Let \\(\\left\\{R_{\\alpha}\\right\\}_{\\alpha \\in[0,1]}\\) be a collection of \\(P\\) -measurable subsets of \\(\\mathcal{X}\\) such that (1) \\(P\\left(R_{\\alpha}\\right)=\\alpha\\) and (2) If \\(\\alpha^{\\prime}&lt;\\alpha\\) then \\(R_{\\alpha^{\\prime}} \\subset R_{\\alpha}\\). Then the \\(P\\)-value of \\(H_{0}\\) for data \\(X=x\\) is inf \\(_{\\alpha \\in[0,1]}\\left\\{\\alpha: x \\in R_{\\alpha}\\right\\}\\).\n\n\nA descriptive but technical definition is given by Sander Greenland below. The description can seem dense, so feel free to skip over it for now and revisit it after reading the rest of the post.\n\n\nA single \\(P\\)-value \\(p\\) is the quantile location of a directional measure of divergence \\(t\\) = \\(t(y;M)\\) of the data point \\(y\\) (usually, the vector in \\(n\\)-space formed by \\(n\\) individual observations) from a test model manifold \\(M\\) in the \\(n\\)-dimensional expectation space defined the logical structure of the data generator (“experiment” or causal structure) that produced the data \\(y\\). \\(M\\) is the subset of the \\(Y\\)-space into which the conjunction of the model constraints (assumptions) force the data expectation or predict where y would be were there no ‘random’ variability. I also use \\(M\\) to denote the set of all the model constraints, as well as their conjunction.\nWith this logical set-up, the observed \\(P\\)-value is the quantile \\(p\\) for the observed value \\(t\\) of \\(T\\) = \\(t(Y;M)\\). This \\(p\\) is read off a reference distribution \\(F = F(t;M)\\) for \\(T\\) derived from \\(M\\). This formulation is essentially that of the “value of P” appearing in Pearson’s seminal 1900 paper on goodness-of-fit tests. Notably, his famed chi-squared statistic is the squared Euclidean distance from \\(y\\) to \\(M\\), with coordinates expressed in standard-deviation units derived from \\(M\\).\nMore broadly, the statistic \\(T\\) can be taken as a measure of divergence of a more general embedding or background model manifold \\(A\\) (which includes all ‘auxiliary’ assumptions) from a more restrictive model \\(M\\), with the goodness-of-fit case taking \\(A\\) as a saturated model covering the entire observation space, and the more common “hypothesis testing” case taking M as the conjunction of an unsaturated \\(A\\) with a targeted ‘test’ constraint (or set of constraints) \\(H\\). This \\(H\\) is logically independent of \\(A\\) and consistent with \\(A\\), with \\(M\\) = \\(H\\) & \\(A\\) in logical terms, or \\(M\\) = \\(H\\) + \\(A\\) in set-theoretic terms with + being union (in particular, we assume no element in \\(H\\) is entailed or contradicted by \\(A\\) and no element in \\(A\\) is entailed or contradicted by \\(H\\))."
  },
  {
    "objectID": "posts/statistics/s-values.html#misleading-definitions",
    "href": "posts/statistics/s-values.html#misleading-definitions",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "1.2 Misleading Definitions",
    "text": "1.2 Misleading Definitions\n\nIt is very common to see the \\(P\\)-value defined as\n\n\nThe probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.\n\n\nIndeed, this is the actual definition currently given on the Wikipedia page for the topic, however, it is inadequate and misleading because it hides and reifies the other assumptions used to compute the \\(P\\)-value and exclusively focuses on the null hypothesis.\nThe test hypothesis (often the null hypothesis) is only one component of the entire model that is being tested. This is reflected in the first definition I gave above, which explicitly emphasizes that every model assumption must be true. Thus, the \\(P\\)-value is sensitive to all these assumptions and their violation(s)."
  },
  {
    "objectID": "posts/statistics/s-values.html#auxilliary-assumptions",
    "href": "posts/statistics/s-values.html#auxilliary-assumptions",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "1.3 Auxilliary Assumptions",
    "text": "1.3 Auxilliary Assumptions\n\nSome of these key assumptions behind the computation of a \\(P\\)-value are that some sort of random process was employed (random sampling, random assignment, etc.), that there are no uncontrolled sources of bias (confounding, programming errors, equipment defects, sparse-data bias)[6] in the results, and that the test hypothesis (often the null hypothesis) is correct. Some of these assumptions can be seen in the figure below from [7], which will be discussed later on. This entire set of assumptions is generally referred to as the test model, and that is because the entire assumed model is being tested.\n\n\n\n\nConditional versus unconditional interpretations of P-values, S-values, and compatibility intervals (CIs). (A) Conditional interpretation, in which background model assumptions, such as no systematic error, are assumed to be correct; thus,the information provided by the P-value and S-value is targeted towards the test hypothesis. (B)Unconditional interpretation, in which no aspect of the statistical model is assumed to be correct; thus,the information provided by the P-value and S-value is targeted toward the entire test model.\n\n\n\nWe often start from the position that all those assumptions are correct (hence, we “condition” on them, even though they are often not correct)[7] when calculating the \\(P\\)-value, so that any deviation of the data from what was expected under those assumptions would be purely random error. But in reality such deviations could also be the result of any assumptions being false, including but not limited to the test hypothesis.\n\n\nNote: “Conditioning” here refers to taking the assumptions in the model as given, and should not be confused with conditional probability.\n\n\nFor example, in high-energy physics, neutrinos were found in one study to be faster than light due to the resulting large test statistic and corresponding small \\(P\\)-value, but this result was later found to be a result of a defect in the fiber-optic timing system for that experiment.[8] Thus, the low \\(P\\)-value was not because the assumed null hypothesis was false, but instead due to a bias in the procedure.\nSo the \\(P\\)-value cannot be the probability of one of these assumptions, such as “the probability of getting results due to chance alone.” A statement like this is backwards because it’s quantifying one of the assumptions behind the computation of a \\(P\\)-value.\nThis assumption of chance causing the results is assumed to be true (aka 100%) along with several other things, when calculating the \\(P\\)-value, but this does not mean it is actually correct and the calculation of the \\(P\\)-value cannot be the probability of one of those assumptions."
  },
  {
    "objectID": "posts/statistics/s-values.html#probability-of-what",
    "href": "posts/statistics/s-values.html#probability-of-what",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "1.4 Probability of What?",
    "text": "1.4 Probability of What?\n\nIt is also important to clarify that \\(P\\)-values are not probabilities of data or parameter values, which many like to say to differentiate from probabilities of hypotheses. Rather, \\(P\\)-values are probabilities of “data features”, such as test statistics (i.e. a z-score or \\(\\chi^{2}\\) statistic) or can be interpreted as the percentile at which the observed test statistic falls within the expected distribution for the test statistic, assuming all the model assumptions are true.[9;10]"
  },
  {
    "objectID": "posts/statistics/s-values.html#properties-uniformity",
    "href": "posts/statistics/s-values.html#properties-uniformity",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "1.5 Properties (Uniformity)",
    "text": "1.5 Properties (Uniformity)\n\nA \\(P\\)-value is considered to be valid if over repeated trials it would be uniform when the tested hypothesis and all other assumptions used to compute the \\(P\\)-value are correct (see the histogram below to see what this looks like). Typically, this test hypothesis is a null hypothesis where the tested parameter value is usually 0 or 1, but this property applies to any test hypothesis for any parameter value. Thus, there is the random variable \\(P\\), which (when valid) follows this uniform distribution, and the realization of this random variable, \\(p\\), which is the observed \\(P\\)-value. The latter is what most researchers are interpreting from studies.\nThus, if we were to simulate two variables that are practically the same (meaning there’s no difference between them) and then compare them, say, using a t-test, and we were to iterate this process 10000 times and plot the distribution of the observed P-values, it would be uniform, indicating that any P-value within the interval from 0-1 is just as likely as any other to be observed.\n\n#' @title Simulation of valid P-values where test hypothesis is true\n#' @param X The first variable we are simulating \n#' @param Y The second variable we are simulating \n#' @param n.sim # The number of simulations\n#' @param t The object storing the t-test results\n#' @param t.sim # Empty numeric vector to contain values\n#' @param n.samp # Sample size in each group\n#' @NOTE The null hypothesis does not have to be 0, it can be any value.\n\nn.sim &lt;- 10000\nt.sim &lt;- numeric(n.sim)\nn.samp &lt;- 1000\n\nfor (i in 1:n.sim) {\n  X &lt;- rnorm(n.samp, mean = 0, sd = 1)\n  Y &lt;- rnorm(n.samp, mean = 0, sd = 1)\n  df &lt;- data.frame(X, Y)\n  t &lt;- t.test(X, Y, mu = 0, paired = FALSE, \n              var.equal = TRUE, data = df)\n  t.sim[i] &lt;- t[[3]]\n}\n#&gt; Error in theme_less(): could not find function \"theme_less\"\n\nMany frequentist statisticians do not consider \\(P\\)-values to be valid/useful if they fail to meet this validity criterion of being uniform, hence they do not recognize variants such as the posterior predictive \\(P\\)-value (which concentrates around values such as 0.5, rather than being uniform) to be valid.\nIndeed, there have been great efforts to calibrate the \\(P\\)-value which ranges from mathematical solutions such as taking the \\((1 + [-e*p*\\log(p)]^{-1})^{-1}\\) which gives the lower bound on the conditional type I error,[11;12] to taking the \\(C_{1}(K):=\\sqrt{K}-1\\) of the \\(P\\)-value (the square-root calibrator), yielding a test martingale,[13] or even empirically attempting to recalibrate the \\(P\\)-value by collecting observed \\(P\\)-values from observational studies with negative controls (“test-hypotheses where the exposure is not believed to cause the outcome”) and using them to calculate the empirical null distribution.[14]\nThe latter is done since observational studies are prone to several more biases than controlled, randomized experiments, thus the observed \\(P\\)-values and estimated effect sizes are used to calculate the systematic errors within the sampling distribution and are used for recalibration of the \\(P\\)-value. Whether or not this approach is effective, however, is a different matter.[15] In short, calibration is an often sought-out property of \\(P\\)-values.\nMany frequentist statisticians do not consider \\(P\\)-values to be valid/useful if they fail to meet this validity criterion of being uniform, hence they do not recognize variants such as the posterior predictive \\(P\\)-value (which concentrates around values such as 0.5, rather than being uniform) to be valid.\nIndeed, there have been great efforts to calibrate the \\(P\\)-value which ranges from mathematical solutions such as taking the \\((1 + [-e*p*\\log(p)]^{-1})^{-1}\\) which gives the lower bound on the conditional type I error,[11;12] to taking the \\(C_{1}(K):=\\sqrt{K}-1\\) of the \\(P\\)-value (the square-root calibrator), yielding a test martingale,[13] or even empirically attempting to recalibrate the \\(P\\)-value by collecting observed \\(P\\)-values from observational studies with negative controls (“test-hypotheses where the exposure is not believed to cause the outcome”) and using them to calculate the empirical null distribution.[14]\nThe latter is done since observational studies are prone to several more biases than controlled, randomized experiments, thus the observed \\(P\\)-values and estimated effect sizes are used to calculate the systematic errors within the sampling distribution and are used for recalibration of the \\(P\\)-value. Whether or not this approach is effective, however, is a different matter.[15] In short, calibration is an often sought-out property of \\(P\\)-values."
  },
  {
    "objectID": "posts/statistics/s-values.html#the-decision-theoretic-approach",
    "href": "posts/statistics/s-values.html#the-decision-theoretic-approach",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "2.1 The Decision-Theoretic Approach",
    "text": "2.1 The Decision-Theoretic Approach\n\nMany researchers interpret the \\(P\\)-value in a behavioral, decision-guiding way such as being statistically significant or not (defined below) depending on whether observed p from a study (the realization of the random variable \\(P\\)) falls below a fixed cutoff level (\\(\\alpha\\), which is the maximum tolerable type I error rate).[16]"
  },
  {
    "objectID": "posts/statistics/s-values.html#statistical-significance",
    "href": "posts/statistics/s-values.html#statistical-significance",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "2.2 Statistical Significance",
    "text": "2.2 Statistical Significance\n\nThus, in this approach, users do not care how small or large the observed \\(P\\)-value \\(p\\) is, but simply, whether or not it fell beneath the pre-specified \\(\\alpha\\) level (often 0.05). If it falls below \\(\\alpha\\) they behave inline with the rejection of this test hypothesis, and if it fails to fall below \\(\\alpha\\), then they must behave in a manner where they accept this test hypothesis. The phrase statistical significance, simply indicates that the observed \\(P\\)-value \\(p\\) fell below this pre-specified \\(\\alpha\\) level, and nothing else. It does not indicate any meaningful significance on its own.\nThe pioneers of this approach, Jerzy Neyman and Egon Pearson, define this behavioral guidance in their 1933 paper, “On the Problem of the Most Efficient Tests of Statistical Hypotheses”[16]\n\n\nWithout hoping to know whether each separate hypothesis is true or false, we may search for rules to govern our behavior with regard to them, in following which we insure that, in the long run of experience, we shall not be too often wrong.\n\n\nThis decision-making framework may be useful in certain scenarios,[17] where some sort of randomization is possible, where experiments can be repeated, and where there is large control over the experimental conditions, with one of the most notable historical examples being Egon Pearson (son of Karl Pearson and coauthor of Jerzy Neyman) using it to improve quality control in industrial settings.\nContrary to some claims,[18] this approach does NOT require exact replications of the experiments, instead, it requires that a valid \\(\\alpha\\) level is used consistently.[16;19] In this approach, the exact, observed \\(P\\)-value from a study is not as relevant and cannot validly be interpreted without an entire set of studies that are compared to the fixed error rate (\\(\\alpha\\)).\n\n\n\n\nFrom left to right: Ronald A. Fisher, Jerzy Neyman, and Egon Pearson."
  },
  {
    "objectID": "posts/statistics/s-values.html#the-inductive-approach",
    "href": "posts/statistics/s-values.html#the-inductive-approach",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "2.3 The Inductive Approach",
    "text": "2.3 The Inductive Approach\n\nOthers interpret the \\(P\\)-value \\(p\\) in an inductive inferential/evidential (Fisherian) way,[20;21] as a continuous measure of evidence against the very test hypothesis and entire model (all assumptions) used to compute it (let’s go with this for now, even though there are some problems with this interpretation, more on that below).\nThis interpretation as a continuous measure of evidence against the test hypothesis and the entire model used to compute it can be seen in the figure below from[7]. In one framework (left panel), we may assume certain assumptions to be true (“conditioning” on them, i.e, use of random assignment), and in the other (right panel), we question all assumptions, hence the “unconditional” interpretation. Unlike the Neyman-Pearson approach, this inferential approach allows interpretation of \\(P\\)-values from single studies, and indeed, lower values of it are taken as more evidence against the tested hypothesis."
  },
  {
    "objectID": "posts/statistics/s-values.html#null-hypothesis-significance-testing",
    "href": "posts/statistics/s-values.html#null-hypothesis-significance-testing",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "2.4 Null-Hypothesis Significance Testing",
    "text": "2.4 Null-Hypothesis Significance Testing\n\nHowever, it is also worth pointing out that most individuals do not interpret \\(P\\)-values from a Neyman-Pearson or Fisherian standpoint, rather, they fuse both approaches together, which is what we commonly know today as “null-hypothesis significance testing.” This approach is regarded by most as being a incompatible hybrid given that it often confuses error rates (\\(\\alpha\\), \\(\\beta\\)), which are fixed before a study, with the \\(P\\)-value, which is not a fixed error-rate, and the fusion of these approaches often has been blamed for the replication crisis in science by many statisticians. Though some believe these approaches can be reconciled and are useful.[22]\n\n\n\n\nConditional versus unconditional interpretations of P-values, S-values, and compatibility intervals (CIs). (A) Conditional interpretation, in which background model assumptions, such as no systematic error, are assumed to be correct; thus,the information provided by the P-value and S-value is targeted towards the test hypothesis. (B)Unconditional interpretation, in which no aspect of the statistical model is assumed to be correct; thus,the information provided by the P-value and S-value is targeted toward the entire test model.\n\n\n\nBack to the Fisherian approach, the interpretation of the \\(P\\)-value as a continuous measure of evidence against the test model that produced it shouldn’t be confused with other statistics that serve as support measures. Likelihood ratios and Bayes factors are absolute measures of evidence for a model compared to another model, whereas the \\(P\\)-value is a relative measure of “evidence” (more on that below) that can be tricky to interpret.[23;24;25] Indeed, this is why the \\(P\\)-value is converted by some Bayesians to a lower bound of the Bayes factor by taking \\(-e*p*\\log(p)\\).[11;12]"
  },
  {
    "objectID": "posts/statistics/s-values.html#measure-of-compatibility",
    "href": "posts/statistics/s-values.html#measure-of-compatibility",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "2.5 Measure of Compatibility",
    "text": "2.5 Measure of Compatibility\n\nThe \\(P\\)-value is not an absolute measure of evidence for a model (such as the null/alternative model), it is a continuous measure of the compatibility of the observed data with the model used to compute it.[3]\nIf it’s high, it means the observed data are very compatible with the model used to compute it. If it’s very low, then it indicates that the data are not as compatible with the model used to calculate it, and this low compatibility may be due to random variation and/or it may be due to a violation of assumptions (such as the null model not being true, not using randomization, a programming error or equipment defect such as that seen with neutrinos, etc.).\nLow compatibility of the data with the model can be implied as evidence against the test hypothesis, if we accept the rest of the model used to compute the \\(P\\)-value. Thus, lower \\(P\\)-values from a Fisherian perspective are seen as stronger evidence against the test hypothesis given the rest of the model."
  },
  {
    "objectID": "posts/statistics/s-values.html#estimation-and-intervals",
    "href": "posts/statistics/s-values.html#estimation-and-intervals",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "3.1 Estimation and Intervals",
    "text": "3.1 Estimation and Intervals\n\nA common criticism put forth by many is that \\(P\\)-values are useless, given that they cannot tell you the size of the effect and because they are confounded by sample size and effect size, and that researchers should instead give compatibility (confidence) intervals. However, this criticism is nonsensical as they can both be given and serve different purposes.\nA \\(P\\)-value for a particular parameter value gives the compatibility between the test model in question, which will vary from one parameter value to the next, and the data. An interval estimate such as a 95% frequentist interval simply gives the region of parameter values with \\(P\\)-values above the corresponding \\(\\alpha\\) level, and which are more consistent with the data than the parameter values outside the interval limits. An interval estimate by itself does not explicitly tell one how consistent a parameter value is with the data, which the \\(P\\)-value does."
  },
  {
    "objectID": "posts/statistics/s-values.html#overstating-the-evidence",
    "href": "posts/statistics/s-values.html#overstating-the-evidence",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "3.2 Overstating the Evidence",
    "text": "3.2 Overstating the Evidence\n\n\\(P\\)-values are routinely criticized for overstating the amount of evidence from a study. Such statements are also often given using Bayesian arguments, of which many are skeptical. However, the \\(P\\)-value cannot overstate evidence as it is simply providing the location at which the test statistic fell in the expected distribution, given that every model assumption were true. It is simply indicative of how surprising/extreme the observed result was, given certain assumptions.\nAny overstating of evidence, is not an issue of the statistic itself, but rather users. If we treat the \\(P-\\) value as nothing more or less than a continuous measure of compatibility of the observed data with the model used to compute it (observed \\(p\\)) given certain model assumptions, we won’t run into some of the common misinterpretations such as “the \\(P\\)-value is the probability of a hypothesis”, or the “probability of chance alone”, or “the probability of being incorrect”.[3]\nIndeed, many of the “problems” commonly associated with the \\(P\\)-value are not due to the actual statistic itself, but rather researchers’ misinterpretations of what it is and what it means for a study.\nThe answer to these misconceptions may be compatibilism, with less compatibility (smaller \\(P\\)-values) indicating a poor fit between the data and the test model and hence more evidence against the test hypothesis.\nA \\(P\\)-value of 0.04 means that assuming that all the assumptions of the model used to compute the \\(P\\)-value are correct, we won’t get data (a test statistic) at least as extreme as what was observed by random variation more than 4% of the time.\nTo many, such low compatibility between the data and the model may lead them to reject the test hypothesis (the null hypothesis)."
  },
  {
    "objectID": "posts/statistics/s-values.html#mismatch-with-direction",
    "href": "posts/statistics/s-values.html#mismatch-with-direction",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "4.1 Mismatch With Direction",
    "text": "4.1 Mismatch With Direction\n\nIf you recall from above, I wrote that the \\(P\\)-value is seen by many as being a continuous measure of evidence against the test hypothesis and model. Technically speaking, it would be incorrect to define it this way because as the \\(P\\)-value goes up (with the highest value being 1 or 100%), there is less evidence against the test hypothesis since the data are more compatible with the test model. 1 = perfect compatibility of the data with the test model.\nAs the \\(P\\)-value gets lower (with the lowest value being 0), there is less compatibility between the data and the model, hence more evidence against the test hypothesis used to compute \\(p\\).\nThus, saying that \\(P\\)-values are measures of evidence against the hypothesis used to compute them is a backward definition. This definition would be correct if higher \\(P\\)-values inferred more evidence against the test hypothesis and vice versa."
  },
  {
    "objectID": "posts/statistics/s-values.html#difficulties-due-to-scale",
    "href": "posts/statistics/s-values.html#difficulties-due-to-scale",
    "title": "P-values Are Tough And S-values Can Help",
    "section": "4.2 Difficulties Due to Scale",
    "text": "4.2 Difficulties Due to Scale\n\nAnother problem with \\(P\\)-values and their interpretation is scaling. Since the statistic is meant to be a continuous measure of compatibility (and relative evidence against the test model + hypothesis), we would hope that differences between \\(P\\)-values would be equal (on an additive scale), as this makes it easier to interpret.\nFor example, the difference between 0 and 10 dollars is the same as the difference between 90 and 100 dollars, in that both are a difference of 10 dollars. And this property remains consistent across various intervals, 120 and 130, 1,000,000 and 1,000,010.\nUnfortunately, this doesn’t apply to the \\(P\\)-value because it is on the inverse-exponential scale. The difference between a \\(P\\)-value of 0.01 and 0.10 is not the same as the difference between 0.90 and 0.99.\n\n\n\n\nA gaussian probability densitiy with the standard deviations annotated. Data points further away from the mean, are more extreme and unlikely events. I also must admit that this is one of my favorite figures of a gaussian distribution.\n\n\n\n\nFor example, with a normal distribution (above), a z-score of 0 results in a \\(P\\)-value of 1 (perfect compatibility). If we now move to a z-score of 1, the \\(P\\)-value is 0.31. Thus, we saw a dramatic decrease from a \\(P\\)-value of 1 to 0.31 with one z-score. A 0.69 decrease in the \\(P\\)-value.\nNow let’s move from a z-score of 1 to a z-score of 2. We saw a decrease of 0.69 with the change in one z-score before, so the new \\(P\\)-value must be 0.31 - 0.69 = -0.38 right? No. The \\(P\\)-value for a z-score of 2 is 0.045. The \\(P\\)-value for a z-score of 3 is 0.003. Even though we’ve only been moving by one z-score at a time, the changes in \\(P\\)-values don’t remain constant; the decreases become larger and larger.\nThus, the difference between the \\(P\\)-values of 0.01 and 0.10, in terms of z-score, is substantially larger than the difference between 0.90 and 0.99. Again, this makes it difficult to interpret as a statistic across the board, especially as a continuous measure. This can further be seen in the figure from Rafi & Greenland (2020)."
  },
  {
    "objectID": "posts/statistics/bootstrap.html",
    "href": "posts/statistics/bootstrap.html",
    "title": "Confidence, Posteriors, and the Bootstrap",
    "section": "",
    "text": "1 The Power of the Bootstrap\n\nThe bootstrap [1;2] is an incredibly powerful method of approximation. Indeed, frequentists are even able to approximate posterior distributions using it, In an excerpt from the paper below, Efron describes how these procedures are similar and gives examples using the parametric bootstrap.\n\n\nThe point of this brief note is to say that in some situations the bootstrap, in particular the parametric bootstrap, offers an easier path toward the calculation of Bayes posterior distributions. An important but somewhat under-appreciated article by Newton and Raftery (1994) made the same point using nonparametric bootstrapping. By “going parametric,” we can illustrate more explicitly the bootstrap/MCMC connection. The arguments here will be made mainly in terms of a simple example, with no attempt at the mathematical justifications seen in Newton and Raftery.\nIt is not really surprising that the bootstrap and MCMC share some overlapping territory. Both are general-purpose computer-based algorithmic methods for assessing statistical accuracy, and both enable the statistician to deal effectively with nuisance parameters, obtaining inferences for the interesting part of the problem. On the less salubrious side, both share the tendency of general-purpose algorithms toward overuse.\nOf course the two methodologies operate in competing inferential realms: frequentist for the bootstrap, Bayesian for MCMC. Here, as in Newton and Raftery, we leap over that divide, bringing bootstrap calculations to bear on the Bayesian world. The working assumption is that we have a Bayesian prior in mind and the only question is how to compute its posterior distribution. Arguments about the merits of Bayesian versus frequentist analysis will not be taken up here, except for our main point that the two camps share some common ground.\n\n\nHowever, they can not only approximate posterior distributions, but confidence distributions too. Confidence distributions are an incredibly dense and technical topic and is practically an umbrella term to refer to multiple concepts, for example, they can refer to confidence curves, confidence densities and deviance functions. For a definitive source to learn more about what they are, see the following [3;4]\n\nIn a paper prepared for Efron’s Fisher lecture, he writes that confidence distributions may provide the key to syncretism between frequentist and bayesian motivations:\n\n\nFiducial distributions, or confidence densities, of- fer a way to finesse this difficulty. A good argument can be made that the confidence density is the posterior density for the parameter of interest, after all of the nuisance parameters have been integrated out in an objective way. If this argument turns out to be valid, then our progress in constructing approximate confidence intervals, and approximate confidence densities, could lead to an easier use of Bayesian thinking in practical problems. This is all quite speculative, but here is a safe prediction for the 21st century: statisticians will be asked to solve bigger and more complicated prob- lems. I believe that there is a good chance that objective Bayes methods will be developed for such problems, and that something like fiducial infer- ence will play an important role in this develop- ment. Maybe Fisher’s biggest blunder will become a big hit in the 21st century!\n\n\nIndeed, some authors have shown that the bootstrap distribution is equal to the confidence distribution because it meets the definition of a confidence distribution. [5;6;7]. Some have even referred to posteriors as a quick and dirty approximation to confidence distributions. [8]\n\nProperties of Confidence Distributions\n\nXie and Singh, as summarized by Fraser define these properties as:\n\n\nThe first is the classical definition CL and defines the confidence distribution function as the distribution function version of the confidence quantile function \\(θ\\) ̃\\(β\\), where \\((−∞,θ ̃β(y0))\\) is a \\(β\\) level confidence interval.\n\n\n\nThe second definition (2.1) identifies a confidence distribution function \\(H(θ;y)\\) as a distribution function in θ for each given y and as a pivot with a uniform distribution for eachθ. But this also closely identifies with what Fisher (1930) offered: fiducial but with Fisher’s promotion replaced by a claim that the argument is pure frequentist. Of course it is pure frequentist just as Fisher (1930) was pure frequentist, except for Fisher’s accompanying claim to having purer probabilities, which was then a direct confrontation to the Bayes aficionados of the time.\n\n\n\nThe third definition (2.2) essentially gives just the quantile equivalent say \\(θ ̃u(y\\)) of the distribution function where u is Uniform (0,1). Aren’t confidence quantile and confidence upper bound just different labelling for the same object? Confidence distributions can have many properties: the distribution function should of course be \\(Uniform(0,1)\\), but also it should inherit continuity when present in the model, should use all available information, and should generally be sensible. These properties aren’t really addressed in the authors’ proposal. A promotion of confidence distributions should acknowledge these inherent issues and also mention marginal and conditional conflicts as discussed in the literature.\n\n\nThe bootstrap distribution and the asymptotic consonance distribution would be defined as:\n\\[H_{n}(\\theta)=1-P\\left(\\hat{\\theta}-\\hat{\\theta}^{*} \\leq \\hat{\\theta}-\\theta | \\mathbf{x}\\right)=P\\left(\\hat{\\theta}^{*} \\leq \\theta | \\mathbf{x}\\right)\\]\n\nEfron writes,\n\n\nAs the authors point out, all of this has something to do with the bootstrap. Let \\(θˆ∗i,i=1,2,...,B\\) represent B bootstrap replications ofθˆ, an estimator of parameter θ(possibly in the presence of nuisance parameters). The \\(α\\)-th empirical quantile of the θˆ∗’s is then theupper endpoint of a first-order accurate-levelαconfidence interval. In this sense, the bootstrap distribution is an approximate confidence distribution. The BCa density, Efron & Tibshirani (1998), improves the confidence accuracy by reweighting the \\(Bθˆ∗\\) values. Let Gˆbe this empirical cdf, and \\(z0\\) and a be the bias correction and acceleration constants in my 1987 paper. Rather than equal weights 1/B, the BCa density puts weight proportional to \\(φ(zθi/(1+azθi)−z0)(1+azθi)2φ(zθi+z0)[zθi=%−1Gˆ(θˆ∗i)−z0]onθˆ∗i\\). The reweighted bootstrap distribution then becomes a second-order accurate confidence distribution. Efron (2012) discusses this construction in the context of objective Bayes inference.\n\n\nFraser, however, argues that the connection is nothing special\n\n\nThe bootstrap.The bootstrap as in Section 2.3 provides an approximation to the distributions described by a model, and in doing this the bootstrap can also eliminate the influence of nuisance parameters. It can be applied to statistics or to pivots, with faster effect using suitable pivots. It can be used with least squares, or with maximum like lihood statistics, or with statistical quantities, or anywhere where distributions are wanted. Ofcourse confidence calculations are just one such use but there are many others including of course testing. So there is no particular attachment of the bootstrap to confidence distribution functions other than providing an approximate means of calculation for such.\n\n\nCertain bootstrap methods such as the BCa method and t-bootstrap method also yield second order accuracy of consonance distributions.\n\\[H_{n}(\\theta)=1-P\\left(\\frac{\\hat{\\theta}^{*}-\\hat{\\theta}}{\\widehat{S E}^{*}\\left(\\hat{\\theta}^{*}\\right)} \\leq \\frac{\\hat{\\theta}-\\theta}{\\widehat{S E}(\\hat{\\theta})} | \\mathbf{x}\\right)\\]\nHere, I demonstrate how to use these particular bootstrap methods to arrive at consonance curves and densities.\nWe’ll use the Iris dataset and construct a function that’ll yield a correlation coefficient and we will try to estimate the confidence intervals for them.\n\n\n\n2 The Nonparametric Bootstrap\n\niris &lt;- datasets::iris\nfoo &lt;- function(data, indices) {\n  dt &lt;- data[indices, ]\n  c(cor(dt[, 1], dt[, 2], method = \"p\"))\n}\n\nWe can now use the curve_boot() method to construct a function. The default method used for this function is the “Bca” method provided by the bcaboot package (also written by Efron).\n\nI will suppress the output of the function because it is unnecessarily long. But we’ve placed all the estimates into a list object called y.\nThe first item in the list will be the consonance distribution constructed by typical means, while the third item will be the bootstrap approximation to the consonance distribution.\n\nggplot2::qplot(boot::boot(iris, foo, 1000)$t, geom = \"histogram\") +\n  theme_light()\n\nggcurve(data = y[[2]], type = \"cd\", nullvalue = TRUE, fill= \"red\") +\n  theme_light()\n\n\nWe can also print out a table for TeX documents\n\nztable(gg &lt;- curve_table(data = y[[1]]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLower Limit\nUpper Limit\nInterval Width\nInterval Level (%)\nCDF\nP-value\nS-value (bits)\n\n\n\n\n2500\n-0.14\n-0.10\n0.05\n25.0\n0.62\n0.75\n0.42\n\n\n5000\n-0.17\n-0.07\n0.10\n50.0\n0.75\n0.50\n1.00\n\n\n7500\n-0.20\n-0.03\n0.17\n75.0\n0.88\n0.25\n2.00\n\n\n8000\n-0.21\n-0.02\n0.19\n80.0\n0.90\n0.20\n2.32\n\n\n8500\n-0.22\n-0.01\n0.21\n85.0\n0.92\n0.15\n2.74\n\n\n9000\n-0.24\n0.00\n0.25\n90.0\n0.95\n0.10\n3.32\n\n\n9500\n-0.26\n0.03\n0.29\n95.0\n0.98\n0.05\n4.32\n\n\n9750\n-0.28\n0.05\n0.33\n97.5\n0.99\n0.03\n5.32\n\n\n9900\n-0.30\n0.08\n0.38\n99.0\n1.00\n0.01\n6.64\n\n\n\n\n\nMore bootstrap replications will lead to a smoother function. But for now, we can compare these two functions to see how similar they are.\n\n\nIf we wanted to look at the bootstrap standard errors, we could do so by loading the fifth item in the list\n\n\nwhere in the top row, theta is the point estimate, and sdboot is the bootstrap estimate of the standard error, sdjack is the jacknife estimate of the standard error. z0 is the bias correction value and a is the acceleration constant.\nThe values in the second row are essentially the internal standard errors of the estimates in the top row.\n\nOne can also construct the confidence density, here I provide an example from the pvaluefunction R package The consonance curve and density are nearly identical. With more bootstrap replications, they are very likely to converge. [aadland2015ajcn]\n\nlibrary(\"pvaluefunctions\")\n#&gt; Error in library(package, pos = pos, lib.loc = lib.loc, character.only = TRUE, : there is no package called 'pvaluefunctions'\nresult &lt;- bcaboot::bcajack(\n  x = iris, B = 10000, func = foo,\n  alpha = c((1:99) / 100), verbose = FALSE\n)\n\nbca &lt;- data.frame(result[[\"lims\"]][, 1])\n\n\nggplot2::qplot(bca$result...lims......1., geom = \"density\") +\n  theme_less()\n\n\n\n\n3 The Parametric Bootstrap\n\nFor the examples above, we mainly used nonparametric bootstrap methods. Here I show an example using the parametric Bca bootstrap and the results it yields.\n\ndata(diabetes, package = \"bcaboot\")\nX &lt;- diabetes$x\ny &lt;- scale(diabetes$y, center = TRUE, scale = FALSE)\nlm.model &lt;- lm(y ~ X - 1)\nmu.hat &lt;- lm.model$fitted.values\nsigma.hat &lt;- stats::sd(lm.model$residuals)\nt0 &lt;- summary(lm.model)$adj.r.squared\ny.star &lt;- sapply(mu.hat, rnorm, n = 1000, sd = sigma.hat)\ntt &lt;- apply(y.star, 1, function(y) summary(lm(y ~ X - 1))$adj.r.squared)\nb.star &lt;- y.star %*% X\n\nNow, we’ll use the same function, but set the method to bcapar for the parametric method.\n\nbcadf &lt;- (bcaboot::bcapar(\n  t0 = t0, alpha = c((1:99) / 100),\n  tt = tt, bb = b.star, cd = 1\n)[[\"lims\"]][, 1])\n\nNow we can look at our outputs.\n\nggplot2::qplot((unlist(bcadf)), geom = \"density\") +\n  theme_less()\n\n\nThat concludes our demonstration of the bootstrap method to approximate consonance functions.\n\n\n\n4 References\n\n\n\n\n\n\n\n\nReferences\n\n1. Efron B. (1979). “Bootstrap methods: Another look at the jackknife.” The Annals of Statistics. 7:1–26. doi: 10.1214/aos/1176344552. https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full.\n\n\n2. Cox DR, Efron B. (2017). “Statistical thinking for 21st century scientists.” Science Advances. 3:e1700768. doi: 10.1126/sciadv.1700768.\n\n\n3. Cole SR, Edwards JK, Greenland S. (2020). “Surprise!” American Journal of Epidemiology. doi: 10/gg63md.\n\n\n4. Brown HK, Hussain-Shamsy N, Lunsky Y, Dennis C-LE, Vigod SN. (2017). “The association between antenatal exposure to selective serotonin reuptake inhibitors and autism: A systematic review and meta-analysis.” The Journal of Clinical Psychiatry. 78:e48–e58. doi: 10.4088/JCP.15r10194.\n\n\n5. Efron B. (1998). “R. A. Fisher in the 21st century (Invited paper presented at the 1996 R. A. Fisher Lecture).” Statistical Science. 13:95–122. doi: 10/cxg354.\n\n\n6. Efron B, Narasimhan B. (2018). “The automatic construction of bootstrap confidence intervals.” :17.\n\n\n7. Xie M, Singh K. (2013). “Confidence Distribution, the Frequentist Distribution Estimator of a Parameter: A Review.” International Statistical Review. 81:3–39. doi: 10.1111/insr.12000.\n\n\n8. Fraser DaS. (2011). “Is Bayes Posterior just Quick and Dirty Confidence?” Statistical Science. 26:299–316. doi: 10/df4n7n."
  },
  {
    "objectID": "posts/statistics/stata.html",
    "href": "posts/statistics/stata.html",
    "title": "Using Stata",
    "section": "",
    "text": "Although concurve was originally designed to be used in R, it is possible to achieve very similar results in Stata. We can use some datasets that are built into Stata to show how to achieve this. I’ll use the Statamarkdown R package so that I can obtain Stata outputs using RMarkdown via my Stata 16 package.\nFirst, let’s load the auto2 dataset which contains data about cars and their characteristics.\n\n\n\nCode\nsysuse auto2\n#&gt; . sysuse auto2\n#&gt; (1978 automobile data)\n#&gt; \n#&gt; .\n\n\n\nBrowse the data set in your data browser to get more familiar with some of the variables. Let’s say we’re interested in the relationship between miles per gallon and price. We could fit a very simple linear model to assess that relationship.\nFirst, let’s visualize the data with a scatter plot.\n\n\n\nCode\nsysuse auto2\n#&gt; . sysuse auto2\n#&gt; (1978 automobile data)\n#&gt; \n#&gt; . scatter price mpg, mcolor(dkorange) scale( 0.70)\n#&gt; \n#&gt; . graph export \"scatter.svg\", replace\n#&gt; file scatter.svg saved as SVG format\n#&gt; \n#&gt; .\n\n\n\n\n\nscatter\n\n\nThat’s what our data looks like. Clearly there seems to be an inverse relationship between miles per gallon and price.\nNow we could fit a very simple linear model with miles per gallon being the predictor and price being the outcome and get some estimates of the relationship.\n\n\n\nCode\nsysuse auto2\n#&gt; . sysuse auto2\n#&gt; (1978 automobile data)\n#&gt; \n#&gt; . regress price mpg \n#&gt; \n#&gt;       Source |       SS           df       MS      Number of obs   =        74\n#&gt; -------------+----------------------------------   F(1, 72)        =     20.26\n#&gt;        Model |   139449474         1   139449474   Prob &gt; F        =    0.0000\n#&gt;     Residual |   495615923        72  6883554.48   R-squared       =    0.2196\n#&gt; -------------+----------------------------------   Adj R-squared   =    0.2087\n#&gt;        Total |   635065396        73  8699525.97   Root MSE        =    2623.7\n#&gt; \n#&gt; ------------------------------------------------------------------------------\n#&gt;        price | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n#&gt; -------------+----------------------------------------------------------------\n#&gt;          mpg |  -238.8943   53.07669    -4.50   0.000    -344.7008   -133.0879\n#&gt;        _cons |   11253.06   1170.813     9.61   0.000     8919.088    13587.03\n#&gt; ------------------------------------------------------------------------------\n#&gt; \n#&gt; .\n\n\n\nThat’s what our output looks like.\nOur output also gives us 95% consonance (confidence) intervals by default. But suppose we wished to fit a fractional polynomial model and graph it and get the confidence bands, here’s what we would do.\n\n\n\nCode\nsysuse auto2\n#&gt; . sysuse auto2\n#&gt; (1978 automobile data)\n#&gt; \n#&gt; . mfp: glm price mpg\n#&gt; \n#&gt; Deviance for model with all terms untransformed = 1373.079, 74 observations\n#&gt; \n#&gt; Variable     Model (vs.)   Deviance  Dev diff.   P      Powers   (vs.)\n#&gt; ----------------------------------------------------------------------\n#&gt; mpg          Lin.   FP2    1373.079    19.565  0.000+   1         -2 -2\n#&gt;              FP1           1356.927     3.413  0.182    -2        \n#&gt;              Final         1356.927                     -2\n#&gt; \n#&gt; \n#&gt; Transformations of covariates:\n#&gt; \n#&gt; -&gt; gen double Impg__1 = X^-2-.2204707671 if e(sample) \n#&gt;    (where: X = mpg/10)\n#&gt; \n#&gt; Final multivariable fractional polynomial model for price\n#&gt; --------------------------------------------------------------------\n#&gt;     Variable |    -----Initial-----          -----Final-----\n#&gt;              |   df     Select   Alpha    Status    df    Powers\n#&gt; -------------+------------------------------------------------------\n#&gt;          mpg |    4     1.0000   0.0500     in      2     -2\n#&gt; --------------------------------------------------------------------\n#&gt; \n#&gt; Generalized linear models                         Number of obs   =         74\n#&gt; Optimization     : ML                             Residual df     =         72\n#&gt;                                                   Scale parameter =    5533697\n#&gt; Deviance         =  398426217.4                   (1/df) Deviance =    5533697\n#&gt; Pearson          =  398426217.4                   (1/df) Pearson  =    5533697\n#&gt; \n#&gt; Variance function: V(u) = 1                       [Gaussian]\n#&gt; Link function    : g(u) = u                       [Identity]\n#&gt; \n#&gt;                                                   AIC             =    18.3909\n#&gt; Log likelihood   = -678.4632599                   BIC             =   3.98e+08\n#&gt; \n#&gt; ------------------------------------------------------------------------------\n#&gt;              |                 OIM\n#&gt;        price | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n#&gt; -------------+----------------------------------------------------------------\n#&gt;      Impg__1 |   13163.85   2013.016     6.54   0.000      9218.41    17109.29\n#&gt;        _cons |   5538.395   289.7737    19.11   0.000     4970.449    6106.341\n#&gt; ------------------------------------------------------------------------------\n#&gt; Deviance = 1356.927.\n#&gt; \n#&gt; . twoway (fpfitci price mpg, estcmd(glm) fcolor(dkorange%20) alcolor(%40))  || scatter pri\n#&gt; &gt; ce mpg, mcolor(dkorange) scale(0.75)\n#&gt; \n#&gt; . graph export \"mfp.svg\", replace \n#&gt; file mfp.svg saved as SVG format\n#&gt; \n#&gt; .\n\n\n\nThat’s what our model looks graphed.\n\n\n\nTrace plot of imputed datasets.\n\n\n\n\n\nfractional polynomial model\n\n\nNow suppose we got a single estimate (point or interval) for a parameter, and we wanted all the intervals for it at every level.\nHere’s the code that we’ll be using to achieve that in Stata.\n\n\nThat’s a lot and may seem intimidating at first, but I’ll explain it line by line.\n\n\n\nCode\npostfile topost level pvalue svalue lointerval upinterval using my_new_data, replace\n#&gt; . postfile topost level pvalue svalue lointerval upinterval using my_new_data, replace\n#&gt; \n#&gt; .\n\n\n\n“postfile” is the command that will be responsible for pasting the data from our overall loop into a new dataset. Here, we are telling Stata that the internal Stata memory used to hold these results (the post) will be named “topost” and that it will have five variables, “level”, “pvalue”, “svalue”, “lointerval”, and “upinterval.”\n\n“level” will contain the consonance level that corresponds to the limits of the interval, with “lointerval” being the lower bound of the interval and “upinterval” being the upper bound.\n“pvalue”is computed by taking 1 - “level”, which is alpha.\n“svalue”is computed by taking the \\(-log_{2}\\) of the computed P-value, and this column will be used to plot the surprisal function.\n“my_new_data” is the filename that we’ve assigned to our new dataset.\n“replace” indicates that if there is an existing filename that already exists, we’re willing to relace it.\n\nHere are the next few major lines\n\n\n\nCode\nforvalues i = 10/99.9 { \n\n\n\nThe command “forvalues” is responsible for taking a set of numbers that we provide it, and running the contents within the braces through those numbers. So here, we’ve set the local macro “i” to contain numbers between 10 and 99.99 for our consonance levels. Why 10? Stata cannot compute consonance intervals lower than 10%.\nOur next line contains the actual contents of what we want to do. Here, it says that we will run a simple linear regression where mpg is the predictor and where price is the outcome, and that the outputs for each loop will be suppressed, hence the “quiet.”\nThen, we have the command “level” with the local macro “i” inside of it. As you may already know, “level” dictates the consonance level that Stata provides us. By default, this is set to 95%, but here, we’ve set it “i”, which we established via “forvalues” as being set to numbers between 10 and 99.\nThe next line two lines\n\n\n\nCode\n      matrix E = r(table)\n      matrix list E\n#&gt; .           matrix E = r(table)\n#&gt; \n#&gt; .           matrix list E\n#&gt; \n#&gt; symmetric E[1,1]\n#&gt;     c1\n#&gt; r1   .\n#&gt; \n#&gt; .\n\n\n\nindicate that we will take variables of a certain class r(), (this class contains the interval bounds we need) and place them within a matrix called E. Then we will list the contents of this matrix.\n\n\n\nCode\npost topost (`i') (1-`i'/100) ( ln(1-`i'/100)/ln(2) * -1) (E[5,1]) (E[6,1])\n#&gt; . post topost (`i') (1-`i'/100) ( ln(1-`i'/100)/ln(2) * -1) (E[5,1]) (E[6,1])\n#&gt; post topost not found\n#&gt; r(111);\n#&gt; \n#&gt; r(111);\n\n\n\nFrom the contents of this matrix list, we will take the estimates from the fifth and sixth rows (look at the last two paranthesis of this line of code above and then the image below) in the first column which contain our consonance limits, with the fifth row containing the lower bound of the interval and the sixth containing the upper bound.\n\n\n\nTrace plot of imputed datasets.\n\n\n\n\n\nTrace plot of imputed datasets.\n\n\n\nWe will place the contents from the fifth row into the second variable we set originally for our new dataset, which was “lointerval.” The contents of the sixth row will be placed into “upinterval.”\nAll potential values of “i” (10-99) will be placed into the first variable that we set, “level”. From this first variable, we can compute the second variable we set up, which was “Pvalue” and we’ve done that here by subtracting “level” from 1 and then dividing the whole equation by 100, so that our P-value can be on the proper scale. Our third variable, which is the longest, computes the “Svalue” by using the previous variable, the “Pvalue” and taking the \\(-log_{2}\\) of it.\nThe relationships between the variables on this line and the variables we set up in the very first line are dictated by the order of the commands we have set, and therefore they correspond to the same order.\n“post topost” is writing the results from each loop as new observations in this data structure.\nWith that, our loop has concluded, and we can now tell Stata that “post” is no longer needed\npostclose topost\nWe then tell Stata to clear its memory to make room for the new dataset we just created and we can list the contents of this new dataset.\nuse my_new_data, clear\nlist\nNow we have an actual dataset with all the consonance intervals at all the levels we wanted, ranging from 10% all the way up to 99%.\nIn order to get a function, we’ll need to be able to graph these results, and that can be tricky since for each observation we have one y value (the consonance level), and two x values, the lower bound of the interval and the upper bound of the interval.\nSo a typical scatterplot will not work, since Stata will only accept one x value. To bypass this, we’ll have to use a paired-coordinate scatterplot which will allow us to plot two different y variables and two different x variables.\nOf course, we don’t need two y variables, so we can set both options to the variable “level”, and then we can set our first x variable to “lointerval” and the second x variable to “upinterval.”\nThis can all be done with the following commands, which will also allow us to set the title and subtitle of the graph, along with the titles of the axes.\n\n\n\nCode\ntwoway (pcscatter level lointerval level upinterval), \n#&gt; . twoway (pcscatter level lointerval level upinterval), \n#&gt; variable level not found\n#&gt; r(111);\n#&gt; \n#&gt; r(111);\n\n\n\nHowever, I would recommend using the menu to customize the plots as much as possible. Simply go to the Graphics menu and select Twoway Graphs. Then create a new plot definition, and select the Advanced plots and choose a paired coordinate scatterplot and fill in the y variables, both of which will be “levels” and the x variables, which will be “lointerval” and “upinterval”.\n\n\n\n\n\nSo now, here’s what our confidence/consonance function looks like.\n\nclear\nsysuse auto2\npostfile topost level pvalue svalue lointerval upinterval using my_new_data, replace\n     \nforvalues i = 10/99.9 { \n      quietly regress price weight, level(`i')\n      matrix E = r(table)\n      matrix list E\n      post topost (`i') (1-`i'/100) ( ln(1-`i'/100)/ln(2) * -1) (E[5,1]) (E[6,1])\n    } \n    \npostclose topost\nuse my_new_data, clear\n\ntwoway (pcscatter pvalue lointerval pvalue upinterval, mcolor(maroon)), ytitle(Consonance Level (%)) xtitle(Consonance Limits) scale(0.75) ///\ntitle(Consonance Curve) subtitle(A function comprised of several consonance intervals at various levels.)\ngraph export \"confidence.svg\", replace\n\n\n\nPretty neat, eh? And below is what our surprisal function looks like, which is simply the \\(-\\log_{2}\\)(p) transformation of the observed P-value. For a more comprehensive discussion on surprisals, see this page and check out some of the references at the bottom.\n\nclear\nsysuse auto2\npostfile topost level pvalue svalue lointerval upinterval using my_new_data, replace\n     \nforvalues i = 10/99.9 { \n      quietly regress price weight, level(`i')\n      matrix E = r(table)\n      matrix list E\n      post topost (`i') (1-`i'/100) ( ln(1-`i'/100)/ln(2) * -1) (E[5,1]) (E[6,1])\n    } \n    \npostclose topost\nuse my_new_data, clear\n\ntwoway (pcscatter svalue lointerval svalue upinterval, mcolor(maroon)), ytitle(Consonance Level (%)) xtitle(Consonance Limits)  scale( 0.75) ///\ntitle(Surprisal Curve) subtitle(A function comprised of several consonance intervals at various levels.)\ngraph export \"surprisal.svg\", replace\n\n\n\n\nSurprisal Function\n\n\n\nIt’s clear that in both plots, we’re missing values of intervals with a confidence/consonance level of less than 10%, but unfortunately, this is the best Stata can do, and what we’ll have to work with. It may not look as pretty as an output from R, but it’s far more useful than blankly staring at a 95% interval and thinking that it is the only piece of information we have regarding compatibility of different effect estimates.\nThe code that I have pasted above can be used for most commands in Stata that have an option to calculate a consonance level. Thus, if there’s an option for “level”, then the commands above will work to produce a data set of several consonance intervals. Though I am seriously hoping that a Stata expert will see this post and point out how I am wrong.\nNow, suppose we wished to fit a generalized linear model, here’s what our code would look like.\n\nclear\nsysuse auto2\n\npostfile topost level pvalue svalue lointerval upinterval using my_new_data, replace\n     \nforvalues i = 10/99.9 { \n      quietly glm price mpg, level(`i')\n      matrix E = r(table)\n      matrix list E\n      post topost (`i') (1-`i'/100) ( ln(1-`i'/100)/ln(2) * -1) (E[5,1]) (E[6,1])\n    } \n    \npostclose topost\nuse my_new_data, clear\nlist\n\ntwoway (pcscatter level lointerval level upinterval), \nytitle(Confidence Level (%)) xtitle(Confidence Limits) ///\ntitle(Consonance Curve) \nsubtitle(A function comprised of several consonance intervals at various levels.)\n\nWe simply replace the first line within the loop with our intended command, just as I’ve replaced\n\n\n\nCode\nregress price mpg\n#&gt; . regress price mpg\n#&gt; no variables defined\n#&gt; r(111);\n#&gt; \n#&gt; r(111);\n\n\n\nwith\n\n\n\nCode\nglm price mpg\n#&gt; . glm price mpg\n#&gt; variable price not found\n#&gt; r(111);\n#&gt; \n#&gt; r(111);\n\n\n\nIf we wanted fit something more complex, like a multilevel mixed model that used restricted maximum likelihood, here’s what our code would look like:\n\nclear\nsysuse auto2\npostfile topost level pvalue svalue lointerval upinterval using my_new_data, replace\n     \nforvalues i = 10/99.9 { \n      quietly mixed outcome predictor, reml level(`i')\n      matrix E = r(table)\n      matrix list E\n      post topost (`i') (1-`i'/100) ( ln(1-`i'/100)/ln(2) * -1) (E[5,1]) (E[6,1])\n    } \n    \npostclose topost\nuse my_new_data, clear\nlist\n\ntwoway (pcscatter level lointerval level upinterval), \nytitle(Confidence Level (%)) xtitle(Confidence Limits) ///\ntitle(Consonance Curve) \nsubtitle(A function comprised of several consonance intervals at various levels.)\n\nBasically, our code doesn’t really change that much and with only a few lines of it, we are able to produce graphical tools that can better help us interpret the wide range of effect sizes that are compatible with the model and its assumptions.\nIt is also important to cite the statistical packages that we have used here, as always.\n\n1 Cite R Packages\n\n\nCode\ncitation(\"Statamarkdown\")\n#&gt; To cite package 'Statamarkdown' in publications use:\n#&gt; \n#&gt;   Hemken D (2025). _Statamarkdown: 'Stata' Markdown_.\n#&gt;   doi:10.32614/CRAN.package.Statamarkdown\n#&gt;   &lt;https://doi.org/10.32614/CRAN.package.Statamarkdown&gt;, R package\n#&gt;   version 0.9.4, &lt;https://CRAN.R-project.org/package=Statamarkdown&gt;.\n#&gt; \n#&gt; A BibTeX entry for LaTeX users is\n#&gt; \n#&gt;   @Manual{,\n#&gt;     title = {Statamarkdown: 'Stata' Markdown},\n#&gt;     author = {Doug Hemken},\n#&gt;     year = {2025},\n#&gt;     note = {R package version 0.9.4},\n#&gt;     url = {https://CRAN.R-project.org/package=Statamarkdown},\n#&gt;     doi = {10.32614/CRAN.package.Statamarkdown},\n#&gt;   }\n\n\n\n\n2 Session info\n\n#&gt; R version 4.5.0 (2025-04-11)\n#&gt; Platform: aarch64-apple-darwin20\n#&gt; Running under: macOS Sequoia 15.6.1\n#&gt; \n#&gt; Matrix products: default\n#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#&gt; \n#&gt; locale:\n#&gt; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#&gt; \n#&gt; time zone: America/New_York\n#&gt; tzcode source: internal\n#&gt; \n#&gt; attached base packages:\n#&gt;  [1] splines   grid      stats4    parallel  stats     graphics  grDevices\n#&gt;  [8] utils     datasets  methods   base     \n#&gt; \n#&gt; other attached packages:\n#&gt;  [1] cli_3.6.5             texPreview_2.1.0      tinytex_0.57         \n#&gt;  [4] rmarkdown_2.29        brms_2.22.0           bootImpute_1.2.2     \n#&gt;  [7] knitr_1.50            boot_1.3-32           gtsummary_2.4.0      \n#&gt; [10] reshape2_1.4.4        ProfileLikelihood_1.3 ImputeRobust_1.3-1   \n#&gt; [13] gamlss_5.5-0          gamlss.dist_6.1-1     gamlss.data_6.0-7    \n#&gt; [16] mvtnorm_1.3-3         performance_0.15.1    summarytools_1.1.4   \n#&gt; [19] tidybayes_3.0.7       htmltools_0.5.8.1     Statamarkdown_0.9.4  \n#&gt; [22] car_3.1-3             carData_3.0-5         qqplotr_0.0.7        \n#&gt; [25] ggcorrplot_0.1.4.1    mitml_0.4-5           pbmcapply_1.5.1      \n#&gt; [28] Amelia_1.8.3          Rcpp_1.1.0            blogdown_1.21        \n#&gt; [31] doParallel_1.0.17     iterators_1.0.14      foreach_1.5.2        \n#&gt; [34] lattice_0.22-7        bayesplot_1.14.0      wesanderson_0.3.7    \n#&gt; [37] VIM_6.2.2             colorspace_2.1-1      here_1.0.1           \n#&gt; [40] progress_1.2.3        loo_2.8.0             mi_1.2               \n#&gt; [43] Matrix_1.7-4          broom_1.0.9           yardstick_1.3.2      \n#&gt; [46] svglite_2.2.1         Cairo_1.6-5           cowplot_1.2.0        \n#&gt; [49] mgcv_1.9-3            nlme_3.1-168          xfun_0.53            \n#&gt; [52] broom.mixed_0.2.9.6   reticulate_1.43.0     kableExtra_1.4.0     \n#&gt; [55] posterior_1.6.1       checkmate_2.3.3       parallelly_1.45.1    \n#&gt; [58] miceFast_0.8.5        randomForest_4.7-1.2  missForest_1.5       \n#&gt; [61] miceadds_3.17-44      quantreg_6.1          SparseM_1.84-2       \n#&gt; [64] MCMCpack_1.7-1        MASS_7.3-65           coda_0.19-4.1        \n#&gt; [67] latex2exp_0.9.6       rstan_2.32.7          StanHeaders_2.32.10  \n#&gt; [70] lubridate_1.9.4       forcats_1.0.0         stringr_1.5.1        \n#&gt; [73] dplyr_1.1.4           purrr_1.1.0           readr_2.1.5          \n#&gt; [76] tibble_3.3.0          ggplot2_3.5.2         tidyverse_2.0.0      \n#&gt; [79] ggtext_0.1.2          concurve_2.7.7        showtext_0.9-7       \n#&gt; [82] showtextdb_3.0        sysfonts_0.8.9        future.apply_1.20.0  \n#&gt; [85] future_1.67.0         tidyr_1.3.1           magrittr_2.0.3       \n#&gt; [88] mice_3.18.0           rms_8.0-0             Hmisc_5.2-3          \n#&gt; \n#&gt; loaded via a namespace (and not attached):\n#&gt;   [1] dichromat_2.0-0.1       nnet_7.3-20             TH.data_1.1-4          \n#&gt;   [4] vctrs_0.6.5             digest_0.6.37           png_0.1-8              \n#&gt;   [7] shape_1.4.6.1           proxy_0.4-27            magick_2.8.7           \n#&gt;  [10] fontLiberation_0.1.0    withr_3.0.2             ggpubr_0.6.1           \n#&gt;  [13] survival_3.8-3          doRNG_1.8.6.2           MatrixModels_0.5-4     \n#&gt;  [16] emmeans_1.11.2-8        systemfonts_1.2.3       ragg_1.5.0             \n#&gt;  [19] zoo_1.8-14              V8_7.0.0                ggdist_3.3.3           \n#&gt;  [22] DEoptimR_1.1-4          Formula_1.2-5           prettyunits_1.2.0      \n#&gt;  [25] rematch2_2.1.2          httr_1.4.7              rstatix_0.7.2          \n#&gt;  [28] globals_0.18.0          ps_1.9.1                rstudioapi_0.17.1      \n#&gt;  [31] extremevalues_2.4.1     pan_1.9                 generics_0.1.4         \n#&gt;  [34] processx_3.8.6          base64enc_0.1-3         curl_7.0.0             \n#&gt;  [37] mitools_2.4             desc_1.4.3              xtable_1.8-4           \n#&gt;  [40] svUnit_1.0.8            pracma_2.4.4            evaluate_1.0.5         \n#&gt;  [43] hms_1.1.3               glmnet_4.1-10           rcartocolor_2.1.2      \n#&gt;  [46] lmtest_0.9-40           robustbase_0.99-6       matrixStats_1.5.0      \n#&gt;  [49] svgPanZoom_0.3.4        class_7.3-23            pillar_1.11.0          \n#&gt;  [52] caTools_1.18.3          compiler_4.5.0          stringi_1.8.7          \n#&gt;  [55] jomo_2.7-6              minqa_1.2.8             plyr_1.8.9             \n#&gt;  [58] crayon_1.5.3            abind_1.4-8             metadat_1.4-0          \n#&gt;  [61] sp_2.2-0                mathjaxr_1.8-0          rapportools_1.2        \n#&gt;  [64] twosamples_2.0.1        sandwich_3.1-1          whisker_0.4.1          \n#&gt;  [67] codetools_0.2-20        multcomp_1.4-28         textshaping_1.0.3      \n#&gt;  [70] bcaboot_0.2-3           openssl_2.3.3           flextable_0.9.10       \n#&gt;  [73] QuickJSR_1.8.0          e1071_1.7-16            gridtext_0.1.5         \n#&gt;  [76] lme4_1.1-37             fs_1.6.6                itertools_0.1-3        \n#&gt;  [79] listenv_0.9.1           Rdpack_2.6.4            pkgbuild_1.4.8         \n#&gt;  [82] ggsignif_0.6.4          estimability_1.5.1      callr_3.7.6            \n#&gt;  [85] tzdb_0.5.0              pkgconfig_2.0.3         tools_4.5.0            \n#&gt;  [88] rbibutils_2.3           viridisLite_0.4.2       DBI_1.2.3              \n#&gt;  [91] numDeriv_2016.8-1.1     fastmap_1.2.0           scales_1.4.0           \n#&gt;  [94] officer_0.7.0           opdisDownsampling_1.0.1 insight_1.4.2          \n#&gt;  [97] rpart_4.1.24            farver_2.1.2            reformulas_0.4.1       \n#&gt; [100] survminer_0.5.1         yaml_2.3.10             foreign_0.8-90         \n#&gt; [103] lifecycle_1.0.4         askpass_1.2.1           backports_1.5.0        \n#&gt; [106] Brobdingnag_1.2-9       timechange_0.3.0        gtable_0.3.6           \n#&gt; [109] arrayhelpers_1.1-0      metafor_4.8-0           jsonlite_2.0.0         \n#&gt; [112] bitops_1.0-9            qqconf_1.3.2            zip_2.3.3              \n#&gt; [115] ranger_0.17.0           RcppParallel_5.1.11-1   polspline_1.1.25       \n#&gt; [118] bridgesampling_1.1-2    survMisc_0.5.6          distributional_0.5.0   \n#&gt; [121] pander_0.6.6            details_0.4.0           KMsurv_0.1-6           \n#&gt; [124] formatR_1.14            glue_1.8.0              tcltk_4.5.0            \n#&gt; [127] gdtools_0.4.3           rprojroot_2.1.1         mcmc_0.9-8             \n#&gt; [130] gridExtra_2.3           R6_2.6.1                arm_1.14-4             \n#&gt; [133] km.ci_0.5-6             vcd_1.4-13              clipr_0.8.0            \n#&gt; [136] cluster_2.1.8.1         rngtools_1.5.2          nloptr_2.2.1           \n#&gt; [139] rstantools_2.5.0        tidyselect_1.2.1        htmlTable_2.4.3        \n#&gt; [142] tensorA_0.36.2.1        xml2_1.4.0              inline_0.3.21          \n#&gt; [145] fontBitstreamVera_0.1.1 furrr_0.3.1             laeken_0.5.3           \n#&gt; [148] pryr_0.1.6              fontquiver_0.2.1        data.table_1.17.8      \n#&gt; [151] htmlwidgets_1.6.4       RColorBrewer_1.1-3      rlang_1.1.6            \n#&gt; [154] uuid_1.2-1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a statistician and data scientist focused on statistical methodology and its applications in research. My work explores the foundations of statistical inference, sensitivity analysis, and robust methods.\n\n\n\nStatistical inference and uncertainty quantification\nSensitivity and tipping point analyses\nCausal inference methodologies\nBootstrap and resampling methods\nRobust statistical techniques\n\n\n\n\n\n\n\n\n\nThis website is about statistical science, which may seem like an odd phrase given that it’s rarely used. Unlike ‘statistics’, which may often refer to ‘mathematical statistics’ and ‘applied statistics’, statistical science goes beyond probability theory and mathematics, and incorporates good principles of design and scientific thinking to maximize quantitative inferences drawn from data in the real world. So, why not just call it ‘applied statistics’ to differentiate it from mathematical statistics?\nAs the great John Nelder argued,nelder1999jrssss?\n\n\nFirst ‘applied statistics’ becomes a tautology, for statistics is nothing without its applications. The phrase should be abandoned. It has arisen to distinguish it from ‘mathematical statistics’. However, this is also a misnomer, because it should be ‘statistical mathematics’, as A. C. Aitken entitled his book many years ago.\nTo make this change does not in any way diminish the importance of mathematics. Mathematics remains the source of our tools, but statistical science is not just a branch of mathematics; it is not a purely deductive system, because it is concerned with quantitative inferences from data obtained from the real world.\n\n\nHighly influential statisticians besides Nelder have recognized the need for statistics to break away from mathematics and probability such as Cox, Reid, Efron, & Greenland.Cox_2017?, Cox_Efron_2017?, Greenland_2017?, Reid_1994?, cox2017? I too share their goals and hope to promote good statistical science on this website within the context of medicine and nutrition.\nYou can read past articles by clicking this Archives link or the menu on the top right. You can also see other projects by clicking… Projects or clicking the same link on the top right menu. You can find a list of other individuals in statistical science whose content I often read.\nIf you wish to follow this website, consider subscribing for updates whenever new articles are out. And if you wish to support the website, consider supporting the site here.\nIf you’re interested in knowing what this website is built on, you can read about it here.\nIf you’d like to get in touch with me, please use the contact form but also please feel free to message me on Twitter or interact with me there!\nThanks!\n\n\n\n\n\n\n\n\n\nStatistical Computing: R, Python, Stan, JAGS\nStatistical Methods: Bayesian inference, causal inference, robust statistics\nTools: RMarkdown, Quarto, LaTeX, Git\nPackages: Development of R packages for statistical analysis\n\n\n\n\nFeel free to reach out via Twitter or email for collaborations or questions about statistical methodology."
  },
  {
    "objectID": "about.html#sir-panda",
    "href": "about.html#sir-panda",
    "title": "About",
    "section": "",
    "text": "I am a statistician and data scientist focused on statistical methodology and its applications in research. My work explores the foundations of statistical inference, sensitivity analysis, and robust methods.\n\n\n\nStatistical inference and uncertainty quantification\nSensitivity and tipping point analyses\nCausal inference methodologies\nBootstrap and resampling methods\nRobust statistical techniques\n\n\n\n\n\n\n\n\n\nThis website is about statistical science, which may seem like an odd phrase given that it’s rarely used. Unlike ‘statistics’, which may often refer to ‘mathematical statistics’ and ‘applied statistics’, statistical science goes beyond probability theory and mathematics, and incorporates good principles of design and scientific thinking to maximize quantitative inferences drawn from data in the real world. So, why not just call it ‘applied statistics’ to differentiate it from mathematical statistics?\nAs the great John Nelder argued,nelder1999jrssss?\n\n\nFirst ‘applied statistics’ becomes a tautology, for statistics is nothing without its applications. The phrase should be abandoned. It has arisen to distinguish it from ‘mathematical statistics’. However, this is also a misnomer, because it should be ‘statistical mathematics’, as A. C. Aitken entitled his book many years ago.\nTo make this change does not in any way diminish the importance of mathematics. Mathematics remains the source of our tools, but statistical science is not just a branch of mathematics; it is not a purely deductive system, because it is concerned with quantitative inferences from data obtained from the real world.\n\n\nHighly influential statisticians besides Nelder have recognized the need for statistics to break away from mathematics and probability such as Cox, Reid, Efron, & Greenland.Cox_2017?, Cox_Efron_2017?, Greenland_2017?, Reid_1994?, cox2017? I too share their goals and hope to promote good statistical science on this website within the context of medicine and nutrition.\nYou can read past articles by clicking this Archives link or the menu on the top right. You can also see other projects by clicking… Projects or clicking the same link on the top right menu. You can find a list of other individuals in statistical science whose content I often read.\nIf you wish to follow this website, consider subscribing for updates whenever new articles are out. And if you wish to support the website, consider supporting the site here.\nIf you’re interested in knowing what this website is built on, you can read about it here.\nIf you’d like to get in touch with me, please use the contact form but also please feel free to message me on Twitter or interact with me there!\nThanks!\n\n\n\n\n\n\n\n\n\nStatistical Computing: R, Python, Stan, JAGS\nStatistical Methods: Bayesian inference, causal inference, robust statistics\nTools: RMarkdown, Quarto, LaTeX, Git\nPackages: Development of R packages for statistical analysis\n\n\n\n\nFeel free to reach out via Twitter or email for collaborations or questions about statistical methodology."
  }
]