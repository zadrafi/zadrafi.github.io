<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.34">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Zad Rafi">
<meta name="dcterms.date" content="2018-11-10">
<meta name="keywords" content="p-value, p-values, p value, p values, null hypothesis, test statistic, statistical test, statistical significance, bits, compatibility, results due to chance, evidence, statistical model, null hypothesis significance testing, hypothesis testing, significance testing, neyman, fisher">
<meta name="description" content="An extensive discussion about what P-values are, their properties, common interpretations, misinterpretations, and how a measure called an S-value may better help us interpret them.">

<title>P-values Are Tough And S-values Can Help – Less Likely</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 2em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c8ad9e5dbd60b7b70b38521ab19b7da4.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-befe23ebd2f54d8af2c8a89d1a1611f1.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-c8ad9e5dbd60b7b70b38521ab19b7da4.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6f31dc0de0b27d6330ba5816b520976b.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-851073aeef2233dc7fca7df9ea7de5a3.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-6f31dc0de0b27d6330ba5816b520976b.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": true,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Less Likely</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html#category=statistics"> 
<span class="menu-text">Statistics</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/zadrafi"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/zadrafi"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../blog.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-a-p-value-anyway" id="toc-what-is-a-p-value-anyway" class="nav-link active" data-scroll-target="#what-is-a-p-value-anyway"><span class="header-section-number">1</span> What is a <em>P</em>-value Anyway?</a>
  <ul class="collapse">
  <li><a href="#some-definitions-descriptions" id="toc-some-definitions-descriptions" class="nav-link" data-scroll-target="#some-definitions-descriptions"><span class="header-section-number">1.1</span> Some Definitions &amp; Descriptions</a></li>
  <li><a href="#misleading-definitions" id="toc-misleading-definitions" class="nav-link" data-scroll-target="#misleading-definitions"><span class="header-section-number">1.2</span> Misleading Definitions</a></li>
  <li><a href="#auxilliary-assumptions" id="toc-auxilliary-assumptions" class="nav-link" data-scroll-target="#auxilliary-assumptions"><span class="header-section-number">1.3</span> Auxilliary Assumptions</a></li>
  <li><a href="#probability-of-what" id="toc-probability-of-what" class="nav-link" data-scroll-target="#probability-of-what"><span class="header-section-number">1.4</span> Probability of What?</a></li>
  <li><a href="#properties-uniformity" id="toc-properties-uniformity" class="nav-link" data-scroll-target="#properties-uniformity"><span class="header-section-number">1.5</span> Properties (Uniformity)</a></li>
  </ul></li>
  <li><a href="#the-different-interpretations" id="toc-the-different-interpretations" class="nav-link" data-scroll-target="#the-different-interpretations"><span class="header-section-number">2</span> The Different Interpretations</a>
  <ul class="collapse">
  <li><a href="#the-decision-theoretic-approach" id="toc-the-decision-theoretic-approach" class="nav-link" data-scroll-target="#the-decision-theoretic-approach"><span class="header-section-number">2.1</span> The Decision-Theoretic Approach</a></li>
  <li><a href="#statistical-significance" id="toc-statistical-significance" class="nav-link" data-scroll-target="#statistical-significance"><span class="header-section-number">2.2</span> Statistical Significance</a></li>
  <li><a href="#the-inductive-approach" id="toc-the-inductive-approach" class="nav-link" data-scroll-target="#the-inductive-approach"><span class="header-section-number">2.3</span> The Inductive Approach</a></li>
  <li><a href="#null-hypothesis-significance-testing" id="toc-null-hypothesis-significance-testing" class="nav-link" data-scroll-target="#null-hypothesis-significance-testing"><span class="header-section-number">2.4</span> Null-Hypothesis Significance Testing</a></li>
  <li><a href="#measure-of-compatibility" id="toc-measure-of-compatibility" class="nav-link" data-scroll-target="#measure-of-compatibility"><span class="header-section-number">2.5</span> Measure of Compatibility</a></li>
  </ul></li>
  <li><a href="#common-misleading-criticisms" id="toc-common-misleading-criticisms" class="nav-link" data-scroll-target="#common-misleading-criticisms"><span class="header-section-number">3</span> Common, Misleading Criticisms</a>
  <ul class="collapse">
  <li><a href="#estimation-and-intervals" id="toc-estimation-and-intervals" class="nav-link" data-scroll-target="#estimation-and-intervals"><span class="header-section-number">3.1</span> Estimation and Intervals</a></li>
  <li><a href="#overstating-the-evidence" id="toc-overstating-the-evidence" class="nav-link" data-scroll-target="#overstating-the-evidence"><span class="header-section-number">3.2</span> Overstating the Evidence</a></li>
  </ul></li>
  <li><a href="#some-valid-issues" id="toc-some-valid-issues" class="nav-link" data-scroll-target="#some-valid-issues"><span class="header-section-number">4</span> Some Valid Issues</a>
  <ul class="collapse">
  <li><a href="#mismatch-with-direction" id="toc-mismatch-with-direction" class="nav-link" data-scroll-target="#mismatch-with-direction"><span class="header-section-number">4.1</span> Mismatch With Direction</a></li>
  <li><a href="#difficulties-due-to-scale" id="toc-difficulties-due-to-scale" class="nav-link" data-scroll-target="#difficulties-due-to-scale"><span class="header-section-number">4.2</span> Difficulties Due to Scale</a></li>
  </ul></li>
  <li><a href="#resolution-with-surprisals" id="toc-resolution-with-surprisals" class="nav-link" data-scroll-target="#resolution-with-surprisals"><span class="header-section-number">5</span> Resolution with Surprisals</a>
  <ul class="collapse">
  <li><a href="#some-examples" id="toc-some-examples" class="nav-link" data-scroll-target="#some-examples"><span class="header-section-number">5.0.1</span> Some Examples</a></li>
  </ul></li>
  <li><a href="#s-value-calculator" id="toc-s-value-calculator" class="nav-link" data-scroll-target="#s-value-calculator"><span class="header-section-number">6</span> S-value Calculator</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">7</span> References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/zadrafi/lesslikely/edit/main/posts/statistics/s-values.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/zadrafi/lesslikely/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">P-values Are Tough And S-values Can Help</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>

<div>
  <div class="description">
    An extensive discussion about what P-values are, their properties, common interpretations, misinterpretations, and how a measure called an S-value may better help us interpret them.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Zad Rafi </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 10, 2018</p>
    </div>
  </div>
  
    
  </div>
  

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>p-value, p-values, p value, p values, null hypothesis, test statistic, statistical test, statistical significance, bits, compatibility, results due to chance, evidence, statistical model, null hypothesis significance testing, hypothesis testing, significance testing, neyman, fisher</p>
  </div>
</div>

</header>


<script src="s-values_files/libs/kePrint-0.0.1/kePrint.js"></script>
<p><link href="s-values_files/libs/lightable-0.0.1/lightable.css" rel="stylesheet"></p>
<hr>
<p>The <span class="math inline">\(P\)</span>-value doesn’t have many fans. There are those who don’t understand it, often treating it as a measure it’s not, whether that’s a posterior probability, the probability of getting results due to chance alone, or some other bizarre/incorrect interpretation. [<span class="citation" data-cites="gigerenzerStatisticalRitualsReplication2018"><sup><a href="#ref-gigerenzerStatisticalRitualsReplication2018" role="doc-biblioref">1</a></sup></span>;<span class="citation" data-cites="goodmanDirtyDozenTwelve2008"><sup><a href="#ref-goodmanDirtyDozenTwelve2008" role="doc-biblioref">2</a></sup></span>;<span class="citation" data-cites="greenlandStatisticalTestsValues2016"><sup><a href="#ref-greenlandStatisticalTestsValues2016" role="doc-biblioref">3</a></sup></span>] Then there are those who dislike it because they think the concept is too difficult to understand or because they see it as a noisy statistic we’re not interested in.</p>
<p>However, the groups of people mentioned above aren’t mutually exclusive. Many who dislike and criticize the <span class="math inline">\(P\)</span>-value also do not understand its properties and behavior. This is unfortunate, given how important and widely used they are. In this article, which could also have been titled, <span class="math inline">\(P\)</span>-values: More Than You Ever Wanted to Know, I take on the task of explaining:</p>
<hr>
<ul>
<li>what <span class="math inline">\(P\)</span>-values are</li>
</ul>
<hr>
<ul>
<li>the assumptions behind them</li>
</ul>
<hr>
<ul>
<li>their properties and behavior</li>
</ul>
<hr>
<ul>
<li>different schools of interpretation</li>
</ul>
<hr>
<ul>
<li>misleading criticisms of <span class="math inline">\(P\)</span>-values</li>
</ul>
<hr>
<ul>
<li>some valid issues in interpretation</li>
</ul>
<hr>
<ul>
<li>how these issues can be resolved</li>
</ul>
<hr>
<section id="what-is-a-p-value-anyway" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> What is a <em>P</em>-value Anyway?</h1>
<hr>
<section id="some-definitions-descriptions" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="some-definitions-descriptions"><span class="header-section-number">1.1</span> Some Definitions &amp; Descriptions</h2>
<hr>
<p>The <span class="math inline">\(P\)</span>-value is the probability of getting a result (specifically, a test statistic) at least as extreme as what was observed if <strong>every model assumption</strong>, in addition to the targeted test hypothesis (usually a null hypothesis), used to compute it <strong>were correct</strong>. [<span class="citation" data-cites="rafiSemanticCognitiveTools2020"><sup><a href="#ref-rafiSemanticCognitiveTools2020" role="doc-biblioref">4</a></sup></span>;<span class="citation" data-cites="greenlandStatisticalTestsValues2016"><sup><a href="#ref-greenlandStatisticalTestsValues2016" role="doc-biblioref">3</a></sup></span>;<span class="citation" data-cites="greenlandValidPvaluesBehave2019"><sup><a href="#ref-greenlandValidPvaluesBehave2019" role="doc-biblioref">5</a></sup></span>]</p>
<p>A simple, mathematically rigorous definition of a <span class="math inline">\(P\)</span>-value (for those interested) is given by <a href="https://www.stat.berkeley.edu/~stark/Preprints/pValues.pdf">Stark (2015)</a>.</p>
<hr>
<blockquote class="blockquote">
<p>Let <span class="math inline">\(P\)</span> be the probability distribution of the data <span class="math inline">\(X\)</span>, which takes values in the measurable space <span class="math inline">\(\mathcal{X}\)</span>. Let <span class="math inline">\(\left\{R_{\alpha}\right\}_{\alpha \in[0,1]}\)</span> be a collection of <span class="math inline">\(P\)</span> -measurable subsets of <span class="math inline">\(\mathcal{X}\)</span> such that (1) <span class="math inline">\(P\left(R_{\alpha}\right)=\alpha\)</span> and (2) If <span class="math inline">\(\alpha^{\prime}&lt;\alpha\)</span> then <span class="math inline">\(R_{\alpha^{\prime}} \subset R_{\alpha}\)</span>. Then the <span class="math inline">\(P\)</span>-value of <span class="math inline">\(H_{0}\)</span> for data <span class="math inline">\(X=x\)</span> is inf <span class="math inline">\(_{\alpha \in[0,1]}\left\{\alpha: x \in R_{\alpha}\right\}\)</span>.</p>
</blockquote>
<hr>
<p>A descriptive but technical definition is given by Sander Greenland below. The description can seem dense, so feel free to skip over it for now and revisit it after reading the rest of the post.</p>
<hr>
<blockquote class="blockquote">
<p>A single <span class="math inline">\(P\)</span>-value <span class="math inline">\(p\)</span> is the quantile location of a directional measure of divergence <span class="math inline">\(t\)</span> = <span class="math inline">\(t(y;M)\)</span> of the data point <span class="math inline">\(y\)</span> (usually, the vector in <span class="math inline">\(n\)</span>-space formed by <span class="math inline">\(n\)</span> individual observations) from a test model manifold <span class="math inline">\(M\)</span> in the <span class="math inline">\(n\)</span>-dimensional expectation space defined the logical structure of the data generator (“experiment” or causal structure) that produced the data <span class="math inline">\(y\)</span>. <span class="math inline">\(M\)</span> is the subset of the <span class="math inline">\(Y\)</span>-space into which the conjunction of the model constraints (assumptions) force the data expectation or predict where y would be were there no ‘random’ variability. I also use <span class="math inline">\(M\)</span> to denote the set of all the model constraints, as well as their conjunction.</p>
<p>With this logical set-up, the observed <span class="math inline">\(P\)</span>-value is the quantile <span class="math inline">\(p\)</span> for the observed value <span class="math inline">\(t\)</span> of <span class="math inline">\(T\)</span> = <span class="math inline">\(t(Y;M)\)</span>. This <span class="math inline">\(p\)</span> is read off a reference distribution <span class="math inline">\(F = F(t;M)\)</span> for <span class="math inline">\(T\)</span> derived from <span class="math inline">\(M\)</span>. This formulation is essentially that of the “value of P” appearing in Pearson’s seminal 1900 paper on goodness-of-fit tests. Notably, his famed chi-squared statistic is the squared Euclidean distance from <span class="math inline">\(y\)</span> to <span class="math inline">\(M\)</span>, with coordinates expressed in standard-deviation units derived from <span class="math inline">\(M\)</span>.</p>
<p>More broadly, the statistic <span class="math inline">\(T\)</span> can be taken as a measure of divergence of a more general embedding or background model manifold <span class="math inline">\(A\)</span> (which includes all ‘auxiliary’ assumptions) from a more restrictive model <span class="math inline">\(M\)</span>, with the goodness-of-fit case taking <span class="math inline">\(A\)</span> as a saturated model covering the entire observation space, and the more common “hypothesis testing” case taking M as the conjunction of an unsaturated <span class="math inline">\(A\)</span> with a targeted ‘test’ constraint (or set of constraints) <span class="math inline">\(H\)</span>. This <span class="math inline">\(H\)</span> is logically independent of <span class="math inline">\(A\)</span> and consistent with <span class="math inline">\(A\)</span>, with <span class="math inline">\(M\)</span> = <span class="math inline">\(H\)</span> &amp; <span class="math inline">\(A\)</span> in logical terms, or <span class="math inline">\(M\)</span> = <span class="math inline">\(H\)</span> + <span class="math inline">\(A\)</span> in set-theoretic terms with + being union (in particular, we assume no element in <span class="math inline">\(H\)</span> is entailed or contradicted by <span class="math inline">\(A\)</span> and no element in <span class="math inline">\(A\)</span> is entailed or contradicted by <span class="math inline">\(H\)</span>).</p>
</blockquote>
<hr>
</section>
<section id="misleading-definitions" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="misleading-definitions"><span class="header-section-number">1.2</span> Misleading Definitions</h2>
<hr>
<p>It is very common to see the <span class="math inline">\(P\)</span>-value defined as</p>
<hr>
<blockquote class="blockquote">
<p><strong>The probability of obtaining test results at least as extreme as the results actually observed, under the assumption that the null hypothesis is correct.</strong></p>
</blockquote>
<hr>
<p>Indeed, this is the actual definition currently given on the <a href="https://en.wikipedia.org/wiki/P-value">Wikipedia page for the topic</a>, however, it is inadequate and misleading because it hides and reifies the other assumptions used to compute the <span class="math inline">\(P\)</span>-value and exclusively focuses on the null hypothesis.</p>
<p>The test hypothesis (often the null hypothesis) is only one component of the entire model that is being tested. This is reflected in the first definition I gave above, which explicitly emphasizes that every model assumption must be true. Thus, the <span class="math inline">\(P\)</span>-value is sensitive to all these assumptions and their violation(s).</p>
<hr>
</section>
<section id="auxilliary-assumptions" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="auxilliary-assumptions"><span class="header-section-number">1.3</span> Auxilliary Assumptions</h2>
<hr>
<p>Some of these key <strong>assumptions</strong> behind the computation of a <span class="math inline">\(P\)</span>-value are that some sort of <strong>random process was employed</strong> (random sampling, random assignment, etc.), that there are <strong>no uncontrolled sources of bias</strong> (confounding, programming errors, equipment defects, sparse-data bias)[<span class="citation" data-cites="greenlandSparseDataBias2016"><sup><a href="#ref-greenlandSparseDataBias2016" role="doc-biblioref">6</a></sup></span>] in the results, and that <strong>the test hypothesis</strong> (often the null hypothesis) <strong>is correct</strong>. Some of these assumptions can be seen in the figure below from [<span class="citation" data-cites="greenlandAidScientificInference2020"><sup><a href="#ref-greenlandAidScientificInference2020" role="doc-biblioref">7</a></sup></span>], which will be discussed later on. This entire set of assumptions is generally referred to as the <em>test model</em>, and that is because the entire assumed model is being tested.</p>
<hr>
<figure class="figure">
<p><img src="https://res.cloudinary.com/less-likely/image/upload/v1605000180/Site/pvalueassumptions.svg" alt="P-value assumptions" width="500" style="cursor: zoom-in" class="figure-img"></p>
<figcaption>
Conditional versus unconditional interpretations of P-values, S-values, and compatibility intervals (CIs). (A) Conditional interpretation, in which background model assumptions, such as no systematic error, are assumed to be correct; thus,the information provided by the P-value and S-value is targeted towards the test hypothesis. (B)Unconditional interpretation, in which no aspect of the statistical model is assumed to be correct; thus,the information provided by the P-value and S-value is targeted toward the entire test model.
</figcaption>
</figure>
<hr>
<p>We often start from the position that all those assumptions are correct (hence, we “condition” on them, even though they are often not correct)[<span class="citation" data-cites="greenlandAidScientificInference2020"><sup><a href="#ref-greenlandAidScientificInference2020" role="doc-biblioref">7</a></sup></span>] when calculating the <span class="math inline">\(P\)</span>-value, so that any deviation of the data from what was expected under those assumptions would be <strong>purely random error</strong>. But in reality such deviations could also be the result of <strong>any</strong> assumptions being false, including <em>but not limited to</em> the test hypothesis.</p>
<hr>
<blockquote class="blockquote">
<p>Note: “Conditioning” here refers to taking the assumptions in the model as given, and should not be confused with conditional probability.</p>
</blockquote>
<hr>
<p>For example, in high-energy physics, neutrinos were found in one study to be faster than light due to the resulting large test statistic and corresponding small <span class="math inline">\(P\)</span>-value, but this result was later found to be a result of a defect in the fiber-optic timing system for that experiment.[<span class="citation" data-cites="moskowitzFasterthanlightNeutrinosAren2012"><sup><a href="#ref-moskowitzFasterthanlightNeutrinosAren2012" role="doc-biblioref">8</a></sup></span>] Thus, the low <span class="math inline">\(P\)</span>-value was not because the assumed null hypothesis was false, but instead due to a bias in the procedure.</p>
<p>So the <span class="math inline">\(P\)</span>-value <strong>cannot</strong> be the probability of one of these assumptions, such as <em>“the probability of getting results due to chance alone.”</em> A statement like this is <strong>backwards</strong> because it’s quantifying one of the assumptions behind the computation of a <span class="math inline">\(P\)</span>-value.</p>
<p>This assumption of chance causing the results is assumed to be true (aka 100%) along with several other things, when calculating the <span class="math inline">\(P\)</span>-value, but this does not mean it is actually correct and the calculation of the <span class="math inline">\(P\)</span>-value <em>cannot</em> be the <em>probability</em> of one of those <strong>assumptions</strong>.</p>
<hr>
</section>
<section id="probability-of-what" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="probability-of-what"><span class="header-section-number">1.4</span> Probability of What?</h2>
<hr>
<p>It is also important to clarify that <span class="math inline">\(P\)</span>-values are not <em>probabilities of data</em> or parameter values, which many like to say to differentiate from probabilities of hypotheses. Rather, <span class="math inline">\(P\)</span>-values are probabilities of “data features”, such as test statistics (i.e.&nbsp;a z-score or <span class="math inline">\(\chi^{2}\)</span> statistic) or can be interpreted as the percentile at which the observed test statistic falls within the expected distribution for the test statistic, assuming all the model assumptions are true.[<span class="citation" data-cites="perezgonzalezPvaluesPercentilesCommentary2015"><sup><a href="#ref-perezgonzalezPvaluesPercentilesCommentary2015" role="doc-biblioref">9</a></sup></span>;<span class="citation" data-cites="fraserPvalueFunctionStatistical2019"><sup><a href="#ref-fraserPvalueFunctionStatistical2019" role="doc-biblioref">10</a></sup></span>]</p>
<hr>
</section>
<section id="properties-uniformity" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="properties-uniformity"><span class="header-section-number">1.5</span> Properties (Uniformity)</h2>
<hr>
<p>A <span class="math inline">\(P\)</span>-value is considered to be valid if over repeated trials it would be uniform when the tested hypothesis and all other assumptions used to compute the <span class="math inline">\(P\)</span>-value are correct (see the histogram below to see what this looks like). Typically, this test hypothesis is a null hypothesis where the tested parameter value is usually 0 or 1, but this property applies to any test hypothesis for any parameter value. Thus, there is the random variable <span class="math inline">\(P\)</span>, which (when valid) follows this uniform distribution, and the realization of this random variable, <span class="math inline">\(p\)</span>, which is the observed <span class="math inline">\(P\)</span>-value. The latter is what most researchers are interpreting from studies.</p>
<p>Thus, if we were to simulate two variables that are practically the same (meaning there’s no difference between them) and then compare them, say, using a t-test, and we were to iterate this process 10000 times and plot the distribution of the observed P-values, it would be uniform, indicating that any P-value within the interval from 0-1 is just as likely as any other to be observed.</p>
<hr>
<div class="sourceCode" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#' @title Simulation of valid P-values where test hypothesis is true</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param X The first variable we are simulating </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param Y The second variable we are simulating </span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param n.sim # The number of simulations</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param t The object storing the t-test results</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param t.sim # Empty numeric vector to contain values</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param n.samp # Sample size in each group</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#' @NOTE The null hypothesis does not have to be 0, it can be any value.</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>n.sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>t.sim <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n.sim)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>n.samp <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.sim) {</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n.samp, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n.samp, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X, Y)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  t <span class="ot">&lt;-</span> <span class="fu">t.test</span>(X, Y, <span class="at">mu =</span> <span class="dv">0</span>, <span class="at">paired =</span> <span class="cn">FALSE</span>, </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>              <span class="at">var.equal =</span> <span class="cn">TRUE</span>, <span class="at">data =</span> df)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  t.sim[i] <span class="ot">&lt;-</span> t[[<span class="dv">3</span>]]</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>#&gt; Error in theme_less(): could not find function "theme_less"</code></pre>
<hr>
<p>Many frequentist statisticians do not consider <span class="math inline">\(P\)</span>-values to be valid/useful if they fail to meet this validity criterion of being uniform, hence they do not recognize variants such as the posterior predictive <span class="math inline">\(P\)</span>-value (which concentrates around values such as 0.5, rather than being uniform) to be valid.</p>
<p>Indeed, there have been great efforts to calibrate the <span class="math inline">\(P\)</span>-value which ranges from mathematical solutions such as taking the <span class="math inline">\((1 + [-e*p*\log(p)]^{-1})^{-1}\)</span> which gives the lower bound on the conditional type I error,[<span class="citation" data-cites="sellkeCalibrationValuesTesting2001"><sup><a href="#ref-sellkeCalibrationValuesTesting2001" role="doc-biblioref">11</a></sup></span>;<span class="citation" data-cites="greenlandTechnicalIssuesInterpretation2020"><sup><a href="#ref-greenlandTechnicalIssuesInterpretation2020" role="doc-biblioref">12</a></sup></span>] to taking the <span class="math inline">\(C_{1}(K):=\sqrt{K}-1\)</span> of the <span class="math inline">\(P\)</span>-value (the square-root calibrator), yielding a test martingale,[<span class="citation" data-cites="shaferTestMartingalesBayes2011"><sup><a href="#ref-shaferTestMartingalesBayes2011" role="doc-biblioref">13</a></sup></span>] or even empirically attempting to recalibrate the <span class="math inline">\(P\)</span>-value by collecting observed <span class="math inline">\(P\)</span>-values from observational studies with negative controls (“test-hypotheses where the exposure is not believed to cause the outcome”) and using them to calculate the empirical null distribution.[<span class="citation" data-cites="schuemieRobustEmpiricalCalibration2016"><sup><a href="#ref-schuemieRobustEmpiricalCalibration2016" role="doc-biblioref">14</a></sup></span>]</p>
<p>The latter is done since observational studies are prone to several more biases than controlled, randomized experiments, thus the observed <span class="math inline">\(P\)</span>-values and estimated effect sizes are used to calculate the systematic errors within the sampling distribution and are used for recalibration of the <span class="math inline">\(P\)</span>-value. Whether or not this approach is effective, however, is a different matter.[<span class="citation" data-cites="gruberLimitationsEmpiricalCalibration2016"><sup><a href="#ref-gruberLimitationsEmpiricalCalibration2016" role="doc-biblioref">15</a></sup></span>] In short, calibration is an often sought-out property of <span class="math inline">\(P\)</span>-values.</p>
<p>Many frequentist statisticians do not consider <span class="math inline">\(P\)</span>-values to be valid/useful if they fail to meet this validity criterion of being uniform, hence they do not recognize variants such as the posterior predictive <span class="math inline">\(P\)</span>-value (which concentrates around values such as 0.5, rather than being uniform) to be valid.</p>
<p>Indeed, there have been great efforts to calibrate the <span class="math inline">\(P\)</span>-value which ranges from mathematical solutions such as taking the <span class="math inline">\((1 + [-e*p*\log(p)]^{-1})^{-1}\)</span> which gives the lower bound on the conditional type I error,[<span class="citation" data-cites="sellkeCalibrationValuesTesting2001"><sup><a href="#ref-sellkeCalibrationValuesTesting2001" role="doc-biblioref">11</a></sup></span>;<span class="citation" data-cites="greenlandTechnicalIssuesInterpretation2020"><sup><a href="#ref-greenlandTechnicalIssuesInterpretation2020" role="doc-biblioref">12</a></sup></span>] to taking the <span class="math inline">\(C_{1}(K):=\sqrt{K}-1\)</span> of the <span class="math inline">\(P\)</span>-value (the square-root calibrator), yielding a test martingale,[<span class="citation" data-cites="shaferTestMartingalesBayes2011"><sup><a href="#ref-shaferTestMartingalesBayes2011" role="doc-biblioref">13</a></sup></span>] or even empirically attempting to recalibrate the <span class="math inline">\(P\)</span>-value by collecting observed <span class="math inline">\(P\)</span>-values from observational studies with negative controls (“test-hypotheses where the exposure is not believed to cause the outcome”) and using them to calculate the empirical null distribution.[<span class="citation" data-cites="schuemieRobustEmpiricalCalibration2016"><sup><a href="#ref-schuemieRobustEmpiricalCalibration2016" role="doc-biblioref">14</a></sup></span>]</p>
<p>The latter is done since observational studies are prone to several more biases than controlled, randomized experiments, thus the observed <span class="math inline">\(P\)</span>-values and estimated effect sizes are used to calculate the systematic errors within the sampling distribution and are used for recalibration of the <span class="math inline">\(P\)</span>-value. Whether or not this approach is effective, however, is a different matter.[<span class="citation" data-cites="gruberLimitationsEmpiricalCalibration2016"><sup><a href="#ref-gruberLimitationsEmpiricalCalibration2016" role="doc-biblioref">15</a></sup></span>] In short, calibration is an often sought-out property of <span class="math inline">\(P\)</span>-values.</p>
<hr>
</section>
</section>
<section id="the-different-interpretations" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> The Different Interpretations</h1>
<hr>
<section id="the-decision-theoretic-approach" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="the-decision-theoretic-approach"><span class="header-section-number">2.1</span> The Decision-Theoretic Approach</h2>
<hr>
<p>Many researchers interpret the <span class="math inline">\(P\)</span>-value in a behavioral, decision-guiding way such as being statistically significant or not (defined below) depending on whether observed <em>p</em> from a study (the realization of the random variable <span class="math inline">\(P\)</span>) falls below a fixed cutoff level (<span class="math inline">\(\alpha\)</span>, which is the maximum tolerable type I error rate).[<span class="citation" data-cites="neymanProblemMostEfficient1933"><sup><a href="#ref-neymanProblemMostEfficient1933" role="doc-biblioref">16</a></sup></span>]</p>
<hr>
</section>
<section id="statistical-significance" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="statistical-significance"><span class="header-section-number">2.2</span> Statistical Significance</h2>
<hr>
<p>Thus, in this approach, users do not care how small or large the observed <span class="math inline">\(P\)</span>-value <span class="math inline">\(p\)</span> is, but simply, whether or not it fell beneath the pre-specified <span class="math inline">\(\alpha\)</span> level (often 0.05). If it falls below <span class="math inline">\(\alpha\)</span> they behave inline with the rejection of this test hypothesis, and if it fails to fall below <span class="math inline">\(\alpha\)</span>, then they must behave in a manner where they accept this test hypothesis. The phrase <em>statistical significance</em>, simply indicates that the observed <span class="math inline">\(P\)</span>-value <span class="math inline">\(p\)</span> fell below this pre-specified <span class="math inline">\(\alpha\)</span> level, and nothing else. It does not indicate any meaningful significance on its own.</p>
<p>The pioneers of this approach, Jerzy Neyman and Egon Pearson, define this behavioral guidance in their 1933 paper, “On the Problem of the Most Efficient Tests of Statistical Hypotheses”[<span class="citation" data-cites="neymanProblemMostEfficient1933"><sup><a href="#ref-neymanProblemMostEfficient1933" role="doc-biblioref">16</a></sup></span>]</p>
<hr>
<blockquote class="blockquote">
<p>Without hoping to know whether each separate hypothesis is true or false, we may search for rules to govern our behavior with regard to them, in following which we insure that, in the long run of experience, we shall not be too often wrong.</p>
</blockquote>
<hr>
<p>This decision-making framework may be useful in certain scenarios,[<span class="citation" data-cites="whiteheadCaseFrequentismClinical1993"><sup><a href="#ref-whiteheadCaseFrequentismClinical1993" role="doc-biblioref">17</a></sup></span>] where some sort of randomization is possible, where experiments can be repeated, and where there is large control over the experimental conditions, with one of the most notable historical examples being Egon Pearson (son of Karl Pearson and coauthor of Jerzy Neyman) using it to <a href="https://www.ams.org/journals/bull/1936-42-09/S0002-9904-1936-06365-2/S0002-9904-1936-06365-2.pdf">improve quality control</a> in industrial settings.</p>
<p>Contrary to some claims,[<span class="citation" data-cites="rubinWhatTypeType2019"><sup><a href="#ref-rubinWhatTypeType2019" role="doc-biblioref">18</a></sup></span>] this approach does <strong>NOT</strong> require exact replications of the experiments, instead, it requires that a valid <span class="math inline">\(\alpha\)</span> level is used consistently.[<span class="citation" data-cites="neymanProblemMostEfficient1933"><sup><a href="#ref-neymanProblemMostEfficient1933" role="doc-biblioref">16</a></sup></span>;<span class="citation" data-cites="Lehmann2011-vs"><sup><a href="#ref-Lehmann2011-vs" role="doc-biblioref">19</a></sup></span>] In this approach, the exact, observed <span class="math inline">\(P\)</span>-value from a study is not as relevant and cannot validly be interpreted without an entire set of studies that are compared to the fixed error rate (<span class="math inline">\(\alpha\)</span>).</p>
<hr>
<figure class="figure">
<p><img src="https://res.cloudinary.com/less-likely/image/upload/f_auto,q_auto/v1554700110/Site/classicalgiants.png" alt="Picture of the giants who founded frequentist statistics such as Egon Pearson, Ronald Fisher, and Jerzy Neyman" width="800" style="cursor: zoom-in" class="figure-img"></p>
<figcaption>
From left to right: Ronald A. Fisher, Jerzy Neyman, and Egon Pearson.
</figcaption>
</figure>
<hr>
</section>
<section id="the-inductive-approach" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="the-inductive-approach"><span class="header-section-number">2.3</span> The Inductive Approach</h2>
<hr>
<p>Others interpret the <span class="math inline">\(P\)</span>-value <span class="math inline">\(p\)</span> in an inductive inferential/evidential (<strong>Fisherian</strong>) way,[<span class="citation" data-cites="fisherDesignExperiments1935"><sup><a href="#ref-fisherDesignExperiments1935" role="doc-biblioref">20</a></sup></span>;<span class="citation" data-cites="fisherStatisticalMethodsScientific1955"><sup><a href="#ref-fisherStatisticalMethodsScientific1955" role="doc-biblioref">21</a></sup></span>] as a <strong>continuous</strong> measure of evidence against the very test hypothesis and entire model (all assumptions) used to compute it (let’s go with this for now, even though there are some problems with this interpretation, more on that below).</p>
<p>This interpretation as a continuous measure of evidence <strong>against</strong> the test hypothesis and the entire model used to compute it can be seen in the figure below from[<span class="citation" data-cites="greenlandAidScientificInference2020"><sup><a href="#ref-greenlandAidScientificInference2020" role="doc-biblioref">7</a></sup></span>]. In one framework (left panel), we may assume certain assumptions to be true (“conditioning” on them, i.e, use of random assignment), and in the other (right panel), we question all assumptions, hence the “unconditional” interpretation. Unlike the <strong>Neyman-Pearson</strong> approach, this inferential approach allows interpretation of <span class="math inline">\(P\)</span>-values from single studies, and indeed, lower values of it are taken as more evidence against the tested hypothesis.</p>
<hr>
</section>
<section id="null-hypothesis-significance-testing" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="null-hypothesis-significance-testing"><span class="header-section-number">2.4</span> Null-Hypothesis Significance Testing</h2>
<hr>
<p>However, it is also worth pointing out that most individuals do not interpret <span class="math inline">\(P\)</span>-values from a <strong>Neyman-Pearson</strong> or <strong>Fisherian</strong> standpoint, rather, they fuse both approaches together, which is what we commonly know today as “null-hypothesis significance testing.” This approach is regarded by most as being a incompatible hybrid given that it often confuses error rates (<span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>), which are fixed before a study, with the <span class="math inline">\(P\)</span>-value, which is not a fixed error-rate, and the fusion of these approaches often has been blamed for the replication crisis in science by many statisticians. Though some believe these approaches can be reconciled and are useful.[<span class="citation" data-cites="bickelNullHypothesisSignificance2019"><sup><a href="#ref-bickelNullHypothesisSignificance2019" role="doc-biblioref">22</a></sup></span>]</p>
<hr>
<figure class="figure">
<p><img src="https://res.cloudinary.com/less-likely/image/upload/v1605000180/Site/pvalueassumptions.svg" alt="P-value assumptions" width="500" style="cursor: zoom-in" class="figure-img"></p>
<figcaption>
Conditional versus unconditional interpretations of P-values, S-values, and compatibility intervals (CIs). (A) Conditional interpretation, in which background model assumptions, such as no systematic error, are assumed to be correct; thus,the information provided by the P-value and S-value is targeted towards the test hypothesis. (B)Unconditional interpretation, in which no aspect of the statistical model is assumed to be correct; thus,the information provided by the P-value and S-value is targeted toward the entire test model.
</figcaption>
</figure>
<hr>
<p>Back to the <strong>Fisherian</strong> approach, the interpretation of the <span class="math inline">\(P\)</span>-value as a continuous measure of evidence against the test model that produced it shouldn’t be confused with other statistics that serve as support measures. Likelihood ratios and Bayes factors are <strong>absolute</strong> measures of evidence <strong>for</strong> a model compared to another model, whereas the <span class="math inline">\(P\)</span>-value is a <strong>relative</strong> measure of “evidence” (more on that below) that can be tricky to interpret.[<span class="citation" data-cites="jeffreysTestsSignificanceTreated1935"><sup><a href="#ref-jeffreysTestsSignificanceTreated1935" role="doc-biblioref">23</a></sup></span>;<span class="citation" data-cites="jeffreysTheoryProbability1998"><sup><a href="#ref-jeffreysTheoryProbability1998" role="doc-biblioref">24</a></sup></span>;<span class="citation" data-cites="royallStatisticalEvidenceLikelihood1997"><sup><a href="#ref-royallStatisticalEvidenceLikelihood1997" role="doc-biblioref">25</a></sup></span>] Indeed, this is why the <span class="math inline">\(P\)</span>-value is converted by some Bayesians to a lower bound of the Bayes factor by taking <span class="math inline">\(-e*p*\log(p)\)</span>.[<span class="citation" data-cites="sellkeCalibrationValuesTesting2001"><sup><a href="#ref-sellkeCalibrationValuesTesting2001" role="doc-biblioref">11</a></sup></span>;<span class="citation" data-cites="greenlandTechnicalIssuesInterpretation2020"><sup><a href="#ref-greenlandTechnicalIssuesInterpretation2020" role="doc-biblioref">12</a></sup></span>]</p>
<hr>
</section>
<section id="measure-of-compatibility" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="measure-of-compatibility"><span class="header-section-number">2.5</span> Measure of Compatibility</h2>
<hr>
<p>The <span class="math inline">\(P\)</span>-value is not an absolute measure of evidence for a model (such as the null/alternative model), it is a continuous <strong>measure of the compatibility</strong> of the <strong>observed data</strong> with the <strong>model</strong> used to compute it.[<span class="citation" data-cites="greenlandStatisticalTestsValues2016"><sup><a href="#ref-greenlandStatisticalTestsValues2016" role="doc-biblioref">3</a></sup></span>]</p>
<p>If it’s high, it means the observed data are <strong>very compatible</strong> with the model used to compute it. If it’s very low, then it indicates that the data are <strong>not as compatible</strong> with the model used to calculate it, and this low compatibility may be due to random variation and/or it may be due to a violation of assumptions (such as the null model not being true, not using randomization, a programming error or equipment defect such as that <a href="https://www.washingtonpost.com/blogs/compost/post/faster-than-light-neutrinos-arent/2012/02/23/gIQA5MmjVR_blog.html">seen with neutrinos</a>, etc.).</p>
<p>Low compatibility of the data with the model can be implied as evidence against the test hypothesis, if we accept the rest of the model used to compute the <span class="math inline">\(P\)</span>-value. Thus, lower <span class="math inline">\(P\)</span>-values from a <strong>Fisherian</strong> perspective are seen as stronger evidence against the test hypothesis given the rest of the model.</p>
<hr>
</section>
</section>
<section id="common-misleading-criticisms" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Common, Misleading Criticisms</h1>
<hr>
<section id="estimation-and-intervals" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="estimation-and-intervals"><span class="header-section-number">3.1</span> Estimation and Intervals</h2>
<hr>
<p>A common criticism put forth by many is that <span class="math inline">\(P\)</span>-values are useless, given that they cannot tell you the size of the effect and because they are confounded by sample size and effect size, and that researchers should instead give compatibility (confidence) intervals. However, this criticism is nonsensical as they can both be given and serve different purposes.</p>
<p>A <span class="math inline">\(P\)</span>-value for a particular parameter value gives the compatibility between the test model in question, which will vary from one parameter value to the next, and the data. An interval estimate such as a 95% frequentist interval simply gives the region of parameter values with <span class="math inline">\(P\)</span>-values above the corresponding <span class="math inline">\(\alpha\)</span> level, and which are more consistent with the data than the parameter values outside the interval limits. An interval estimate by itself does not explicitly tell one how consistent a parameter value is with the data, which the <span class="math inline">\(P\)</span>-value does.</p>
<hr>
</section>
<section id="overstating-the-evidence" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="overstating-the-evidence"><span class="header-section-number">3.2</span> Overstating the Evidence</h2>
<hr>
<p><span class="math inline">\(P\)</span>-values are routinely criticized for overstating the amount of evidence from a study. Such statements are also often given using Bayesian arguments, of which many are skeptical. However, the <span class="math inline">\(P\)</span>-value cannot overstate evidence as it is simply providing the location at which the test statistic fell in the expected distribution, given that every model assumption were true. It is simply indicative of how surprising/extreme the observed result was, given certain assumptions.</p>
<p>Any overstating of evidence, is not an issue of the statistic itself, but rather users. If we treat the <span class="math inline">\(P-\)</span> value as nothing more or less than a continuous measure of compatibility of the observed data with the model used to compute it (observed <span class="math inline">\(p\)</span>) given certain model assumptions, we won’t run into some of the common misinterpretations such as “the <span class="math inline">\(P\)</span>-value is the probability of a hypothesis”, or the “probability of chance alone”, or “the probability of being incorrect”.[<span class="citation" data-cites="greenlandStatisticalTestsValues2016"><sup><a href="#ref-greenlandStatisticalTestsValues2016" role="doc-biblioref">3</a></sup></span>]</p>
<p>Indeed, many of the “problems” commonly associated with the <span class="math inline">\(P\)</span>-value are not due to the actual statistic itself, but rather researchers’ misinterpretations of what it is and what it means for a study.</p>
<p>The answer to these misconceptions may be compatibilism, with less compatibility (smaller <span class="math inline">\(P\)</span>-values) indicating a poor fit between the data and the test model and hence more evidence against the test hypothesis.</p>
<p>A <span class="math inline">\(P\)</span>-value of 0.04 means that assuming that <strong>all</strong> the assumptions of the model used to compute the <span class="math inline">\(P\)</span>-value are correct, we won’t get data (a test statistic) at least as extreme as what was observed by random variation more than 4% of the time.</p>
<p>To many, such low compatibility between the data and the model may lead them to reject the test hypothesis (the null hypothesis).</p>
<hr>
</section>
</section>
<section id="some-valid-issues" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Some Valid Issues</h1>
<hr>
<section id="mismatch-with-direction" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="mismatch-with-direction"><span class="header-section-number">4.1</span> Mismatch With Direction</h2>
<hr>
<p>If you recall from above, I wrote that the <span class="math inline">\(P\)</span>-value is seen by many as being a continuous measure of evidence against the test hypothesis and model. Technically speaking, it would be incorrect to define it this way because as the <span class="math inline">\(P\)</span>-value goes up (with the highest value being 1 or 100%), there is <strong>less</strong> evidence against the test hypothesis since the data are <strong>more compatible</strong> with the test model. 1 = perfect compatibility of the data with the test model.</p>
<p>As the <span class="math inline">\(P\)</span>-value gets lower (with the lowest value being 0), there is <strong>less compatibility</strong> between the data and the model, hence <strong>more</strong> evidence against the test hypothesis used to compute <span class="math inline">\(p\)</span>.</p>
<p>Thus, saying that <span class="math inline">\(P\)</span>-values are measures of evidence against the hypothesis used to compute them is a backward definition. This definition would be correct if higher <span class="math inline">\(P\)</span>-values inferred more evidence against the test hypothesis and vice versa.</p>
<hr>
</section>
<section id="difficulties-due-to-scale" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="difficulties-due-to-scale"><span class="header-section-number">4.2</span> Difficulties Due to Scale</h2>
<hr>
<p>Another problem with <span class="math inline">\(P\)</span>-values and their interpretation is scaling. Since the statistic is meant to be a continuous measure of compatibility (and relative evidence against the test model + hypothesis), we would hope that differences between <span class="math inline">\(P\)</span>-values would be equal (on an additive scale), as this makes it easier to interpret.</p>
<p>For example, the difference between 0 and 10 dollars is the same as the difference between 90 and 100 dollars, in that both are a difference of 10 dollars. And this property remains consistent across various intervals, 120 and 130, 1,000,000 and 1,000,010.</p>
<p>Unfortunately, this doesn’t apply to the <span class="math inline">\(P\)</span>-value because it is on the inverse-exponential scale. The difference between a <span class="math inline">\(P\)</span>-value of 0.01 and 0.10 is not the same as the difference between 0.90 and 0.99.</p>
<hr>
<figure class="figure">
<p><img src="../../images/norm.svg" alt="Gaussian distribution" width="680" style="cursor: zoom-in" class="figure-img"></p>
<figcaption>
A gaussian probability densitiy with the standard deviations annotated. Data points further away from the mean, are more extreme and unlikely events. I also must admit that this is one of my favorite figures of a gaussian distribution.
</figcaption>
</figure>
<hr>
<hr>
<p>For example, with a normal distribution (above), a z-score of 0 results in a <span class="math inline">\(P\)</span>-value of 1 (perfect compatibility). If we now move to a z-score of 1, the <span class="math inline">\(P\)</span>-value is 0.31. Thus, we saw a dramatic decrease from a <span class="math inline">\(P\)</span>-value of 1 to 0.31 with one z-score. A 0.69 decrease in the <span class="math inline">\(P\)</span>-value.</p>
<p>Now let’s move from a z-score of 1 to a z-score of 2. We saw a decrease of 0.69 with the change in <strong>one</strong> z-score before, so the new <span class="math inline">\(P\)</span>-value must be 0.31 - 0.69 = -0.38 right? <strong>No</strong>. The <span class="math inline">\(P\)</span>-value for a z-score of 2 is 0.045. The <span class="math inline">\(P\)</span>-value for a z-score of 3 is 0.003. Even though we’ve only been moving by <strong>one</strong> z-score at a time, the changes in <span class="math inline">\(P\)</span>-values don’t remain constant; the decreases become larger and larger.</p>
<p>Thus, the difference between the <span class="math inline">\(P\)</span>-values of 0.01 and 0.10, in terms of z-score, is substantially larger than the difference between 0.90 and 0.99. Again, this makes it difficult to interpret as a statistic across the board, especially as a continuous measure. This can further be seen in the figure from <a href="https://doi.org/10.1186/s12874-020-01105-9">Rafi &amp; Greenland (2020)</a>.</p>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="resolution-with-surprisals" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Resolution with Surprisals</h1>
<hr>
<p>The issues described above such as the backward definition and the problem of scaling can make it difficult to conceptualize the <span class="math inline">\(P\)</span>-value as being an evidence measure against the test hypothesis and test model. However, these issues can be addressed by taking the negative log of the <span class="math inline">\(P\)</span>-value <span class="math inline">\(–\log_{2}(p)\)</span> , which yields something known as the Shannon information value or <em>surprisal (</em><span class="math inline">\(s\)</span>) value,[<span class="citation" data-cites="rafiSemanticCognitiveTools2020"><sup><a href="#ref-rafiSemanticCognitiveTools2020" role="doc-biblioref">4</a></sup></span>;<span class="citation" data-cites="coleSurprise2020"><sup><a href="#ref-coleSurprise2020" role="doc-biblioref">26</a></sup></span>;<span class="citation" data-cites="greenlandValidPvaluesBehave2019"><sup><a href="#ref-greenlandValidPvaluesBehave2019" role="doc-biblioref">5</a></sup></span>] named after <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a>, the father of information theory.[<span class="citation" data-cites="Shannon1948-uq"><sup><a href="#ref-Shannon1948-uq" role="doc-biblioref">27</a></sup></span>]</p>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Unlike the <span class="math inline">\(P\)</span>-value, this value is not a probability but a continuous measure of <em>information</em> in <strong>bits</strong> of information against the test hypothesis and is taken from the observed test statistic computed by the test model.</p>
<p>It also provides a more intuitive way to think about <span class="math inline">\(P\)</span>-values. Imagine that the variable <span class="math inline">\(k\)</span> is always the nearest integer to the calculated value of <span class="math inline">\(s\)</span>. Now, take for example a <span class="math inline">\(P\)</span>-value of 0.05, the <span class="math inline">\(S\)</span>-value for this would be <span class="math inline">\(s\)</span> = <span class="math inline">\(–\log_{2}(0.05)\)</span> which equals 4.3 bits of information embedded in the test statistic, which can be implied as evidence against the test hypothesis.</p>
<p>How much evidence is this? <span class="math inline">\(k\)</span> can help us think about this. The nearest integer to 4.3 is 4. Thus, the data which yield a <span class="math inline">\(P\)</span>-value of 0.05 which results in an <span class="math inline">\(s\)</span> value of 4.3 bits of information is <strong>no more surprising</strong> than getting <strong>all heads</strong> on 4 fair coin tosses.</p>
<p>Another example. Let’s say our study gives us a <span class="math inline">\(P\)</span>-value of 0.005, which would indicate to many very low compatibility between the test model and the observed data; this would yield an <span class="math inline">\(s\)</span> value of <span class="math inline">\(–\log_{2}(0.005) = 7.6\)</span> bits of information. <span class="math inline">\(k\)</span> which is the closest integer to <span class="math inline">\(s\)</span> would be 8. Thus, these data which yield a <span class="math inline">\(P\)</span>-value of 0.005 are no more surprising than getting all heads on 8 fair coin tosses.</p>
<p>A table of various <span class="math inline">\(P\)</span>-values and their corresponding <span class="math inline">\(S\)</span>-values, maximum-likelihood ratios, and likelihood-ratio statistics can be found below from <a href="https://doi.org/10.1186/s12874-020-01105-9">Rafi &amp; Greenland (2020)</a>, which includes the general cutoffs used in different scientific fields such as high-energy physics and genome-wide association studies. It also shows how the traditional cutoffs used in these fields can be problematic.</p>
<p>For example, an <span class="math inline">\(\alpha\)</span> of 0.05, which only corresponds to seeing all heads on 4 fair coin tosses, is practically nothing when compared to the cutoffs used in particle physics and GWAS, which correspond to seeing all heads on 22 and 30 fair coin tosses, respectively.</p>
<hr>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">P-value (compatibility)</th>
<th style="text-align: right;">S-value (bits)</th>
<th style="text-align: right;">Maximum Likelihood Ratio</th>
<th style="text-align: right;">Deviance Statistic 2ln(MLR)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">0.99</td>
<td style="text-align: right;">0.01</td>
<td style="text-align: right;">1.00e+00</td>
<td style="text-align: right;">0.00</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.9</td>
<td style="text-align: right;">0.15</td>
<td style="text-align: right;">1.01e+00</td>
<td style="text-align: right;">0.02</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.5</td>
<td style="text-align: right;">1.00</td>
<td style="text-align: right;">1.26e+00</td>
<td style="text-align: right;">0.45</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.25</td>
<td style="text-align: right;">2.00</td>
<td style="text-align: right;">1.94e+00</td>
<td style="text-align: right;">1.32</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.1</td>
<td style="text-align: right;">3.32</td>
<td style="text-align: right;">3.87e+00</td>
<td style="text-align: right;">2.71</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.05</td>
<td style="text-align: right;">4.32</td>
<td style="text-align: right;">6.83e+00</td>
<td style="text-align: right;">3.84</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.025</td>
<td style="text-align: right;">5.32</td>
<td style="text-align: right;">1.23e+01</td>
<td style="text-align: right;">5.02</td>
</tr>
<tr class="even">
<td style="text-align: left;">0.01</td>
<td style="text-align: right;">6.64</td>
<td style="text-align: right;">2.76e+01</td>
<td style="text-align: right;">6.63</td>
</tr>
<tr class="odd">
<td style="text-align: left;">0.005</td>
<td style="text-align: right;">7.64</td>
<td style="text-align: right;">5.14e+01</td>
<td style="text-align: right;">7.88</td>
</tr>
<tr class="even">
<td style="text-align: left;">1e-04</td>
<td style="text-align: right;">13.29</td>
<td style="text-align: right;">1.94e+03</td>
<td style="text-align: right;">15.10</td>
</tr>
<tr class="odd">
<td style="text-align: left;">5 sigma (~ 2.9 in 10 million)</td>
<td style="text-align: right;">21.70</td>
<td style="text-align: right;">5.20e+05</td>
<td style="text-align: right;">26.30</td>
</tr>
<tr class="even">
<td style="text-align: left;">1 in 100 million (GWAS)</td>
<td style="text-align: right;">26.60</td>
<td style="text-align: right;">1.40e+07</td>
<td style="text-align: right;">32.80</td>
</tr>
<tr class="odd">
<td style="text-align: left;">6 sigma (~ 1 in a billion)</td>
<td style="text-align: right;">29.90</td>
<td style="text-align: right;">1.30e+08</td>
<td style="text-align: right;">37.40</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span style="text-decoration: underline;">Abbreviations: </span></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Table 1: $P$-values and binary $S$-values, with corresponding maximum-likelihood ratios (MLR) and deviance (likelihood-ratio) statistics for a simple test hypothesis H under background assumptions A</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<hr>
<p>Unlike the <span class="math inline">\(P\)</span>-value, the <span class="math inline">\(S\)</span>-value is more intuitive as a measure of refutational evidence against the test hypothesis since its value (bits of information against the test hypothesis) increases with less compatibility, whereas the opposite is true for the <span class="math inline">\(P\)</span>-value.</p>
<hr>
<section id="some-examples" class="level3" data-number="5.0.1">
<h3 data-number="5.0.1" class="anchored" data-anchor-id="some-examples"><span class="header-section-number">5.0.1</span> Some Examples</h3>
<hr>
<p>Let’s try using some data to see this in action. I’ll take a sample experimental dataset from <code>R</code> on the effects of different conditions on dried plant weight. We can plot the data and run a one-way ANOVA.</p>
<hr>
<div class="sourceCode" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>pg <span class="ot">&lt;-</span> <span class="fu">force</span>(PlantGrowth)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>(Hmisc<span class="sc">::</span><span class="fu">describe</span>(pg))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; pg </span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  2  Variables      30  Observations</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; --------------------------------------------------------------------------------</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; weight </span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 </span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       30        0       29        1    5.073     5.09   0.8131    3.983 </span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      .10      .25      .50      .75      .90      .95 </span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    4.170    4.550    5.155    5.530    6.038    6.132 </span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lowest : 3.59 3.83 4.17 4.32 4.41, highest: 5.87 6.03 6.11 6.15 6.31</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; --------------------------------------------------------------------------------</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; group </span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;        n  missing distinct </span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       30        0        3 </span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                             </span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Value       ctrl  trt1  trt2</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Frequency     10    10    10</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Proportion 0.333 0.333 0.333</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; --------------------------------------------------------------------------------</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p>Looks interesting. We can see some differences from the graph. Here’s what our test output gives us,</p>
<hr>
<div class="sourceCode" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">anova</span>(<span class="fu">lm</span>(weight <span class="sc">~</span> group, <span class="at">data =</span> pg))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ztable</span>(res)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Error in ztable(res): could not find function "ztable"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<div class="sourceCode" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>(obs_p <span class="ot">&lt;-</span> res[<span class="dv">1</span>, <span class="dv">5</span>])</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.01590996</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p>If we had set our <span class="math inline">\(\alpha\)</span> to the traditional 0.05 level before the experiment, we can reject the test hypothesis (the null hypothesis), but that is not as interesting from a continuous evidential perspective. How can I interpret this <span class="math inline">\(P\)</span>-value of 0.0159 more intuitively?</p>
<p>Let’s convert it into an <span class="math inline">\(S\)</span>-value.</p>
<hr>
<div class="sourceCode" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fu">log2</span>(obs_p)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 5.97</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
<p><span class="math display">\[–\log_2(0.0159) = 5.97\]</span></p>
<hr>
<p><span class="math display">\[s= 5.97\]</span></p>
<hr>
<p>That is 5.97 bits of information against the null hypothesis.</p>
<p>Remember, <span class="math inline">\(k\)</span> is the nearest integer to the calculated value of <span class="math inline">\(s\)</span> and in this case, would be 6. So these results (the test statistic, <span class="math inline">\(F\)</span>(4.85)) are as surprising as getting all heads on 6 fair coin tosses. Somewhat surprising, depending on the individual interpreting the results.</p>
<p>How would we interpret it within the context of a given confidence interval? The <span class="math inline">\(S\)</span>-value tells us that values within the computed 95% CI: have at most 4.3 bits of information against them. That is because all parameter values within a 95% CI have <span class="math inline">\(P\)</span>-values greater than 0.05.</p>
<p>So those parameter values that are inside the 95% interval estimate have less bits of information against them, than the parameter values that go further and further away from the center of the 95% interval estimate. The point estimate is the most compatible with the data (meaning it has the least refutational information against it), while those values near the limits have more information against them.</p>
<p>In other words, as values head in the directions outside the interval, there is more refutational information against them, as depicted by the following function from <a href="https://doi.org/10.1186/s12874-020-01105-9">Rafi &amp; Greenland, 2020</a>, which is known as the surprisal function.</p>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>The <span class="math inline">\(S\)</span>-value is not meant to replace the <span class="math inline">\(P\)</span>-value, and it isn’t superior to the <span class="math inline">\(P\)</span>-value. It is merely a logarithmic transformation of it that rescales it on an additive scale and tells us how much information is embedded within the test statistic and can be used as evidence against the test hypothesis. It is meant to be a device to help interpret the information one obtains from a calculated <span class="math inline">\(P\)</span>-value.</p>
<hr>
<p>I’ve <a href="https://data.lesslikely.com/concurve/articles/svalues.html">constructed a calculator</a> that converts observed <span class="math inline">\(P\)</span>-values into <span class="math inline">\(S\)</span>-values and provides an intuitive way to think about them. For a more detailed discussion of <span class="math inline">\(S\)</span>-values, see this <a href="../../../statistics/RG2020BMC">article</a>, in addition to the <a href="#references">references</a> below them.</p>
<hr>
</section>
</section>
<section id="s-value-calculator" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> S-value Calculator</h1>
<hr>
<hr>
<blockquote class="blockquote">
<p><strong>Acknowledgments:</strong> I’m very grateful to <a href="https://twitter.com/Lester_Domes">Sander Greenland</a> for his extensive commentary and corrections on several versions of this article. My acknowledgment does not imply endorsement of my views by these colleagues, and I remain solely responsible for the views expressed herein.</p>
</blockquote>
<hr>
<p>The analyses were run on:</p>
<hr>
<pre><code>#&gt; R version 4.5.0 (2025-04-11)
#&gt; Platform: aarch64-apple-darwin20
#&gt; Running under: macOS Sequoia 15.6.1
#&gt; 
#&gt; Matrix products: default
#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib 
#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1
#&gt; 
#&gt; Random number generation:
#&gt;  RNG:     Mersenne-Twister 
#&gt;  Normal:  Inversion 
#&gt;  Sample:  Rejection 
#&gt;  
#&gt; locale:
#&gt; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
#&gt; 
#&gt; time zone: America/New_York
#&gt; tzcode source: internal
#&gt; 
#&gt; attached base packages:
#&gt;  [1] splines   grid      stats4    parallel  stats     graphics  grDevices utils     datasets  methods   base     
#&gt; 
#&gt; other attached packages:
#&gt;  [1] pbmcapply_1.5.1       texPreview_2.1.0      tinytex_0.57          rmarkdown_2.29        brms_2.22.0          
#&gt;  [6] bootImpute_1.2.2      knitr_1.50            boot_1.3-31           reshape2_1.4.4        ProfileLikelihood_1.3
#&gt; [11] ImputeRobust_1.3-1    gamlss_5.4-22         gamlss.dist_6.1-1     gamlss.data_6.0-6     mvtnorm_1.3-3        
#&gt; [16] performance_0.14.0    summarytools_1.1.4    tidybayes_3.0.7       htmltools_0.5.8.1     Statamarkdown_0.9.2  
#&gt; [21] car_3.1-3             carData_3.0-5         qqplotr_0.0.6         ggcorrplot_0.1.4.1    Amelia_1.8.3         
#&gt; [26] Rcpp_1.0.14           blogdown_1.21         doParallel_1.0.17     iterators_1.0.14      foreach_1.5.2        
#&gt; [31] lattice_0.22-7        bayesplot_1.12.0      wesanderson_0.3.7     VIM_6.2.2             colorspace_2.1-1     
#&gt; [36] here_1.0.1            progress_1.2.3        loo_2.8.0             mi_1.1                Matrix_1.7-3         
#&gt; [41] broom_1.0.8           yardstick_1.3.2       svglite_2.2.1         Cairo_1.6-2           cowplot_1.1.3        
#&gt; [46] mgcv_1.9-3            nlme_3.1-168          xfun_0.52             broom.mixed_0.2.9.6   reticulate_1.42.0    
#&gt; [51] kableExtra_1.4.0      posterior_1.6.1       checkmate_2.3.2       parallelly_1.45.0     miceFast_0.8.5       
#&gt; [56] randomForest_4.7-1.2  missForest_1.5        miceadds_3.17-44      mice_3.18.0           quantreg_6.1         
#&gt; [61] SparseM_1.84-2        MCMCpack_1.7-1        MASS_7.3-65           coda_0.19-4.1         latex2exp_0.9.6      
#&gt; [66] rstan_2.32.7          StanHeaders_2.32.10   lubridate_1.9.4       forcats_1.0.0         stringr_1.5.1        
#&gt; [71] dplyr_1.1.4           purrr_1.0.4           readr_2.1.5           tibble_3.2.1          ggplot2_3.5.2        
#&gt; [76] tidyverse_2.0.0       ggtext_0.1.2          concurve_2.7.7        showtext_0.9-7        showtextdb_3.0       
#&gt; [81] sysfonts_0.8.9        future.apply_1.11.3   future_1.58.0         tidyr_1.3.1           magrittr_2.0.3       
#&gt; [86] rms_8.0-0             Hmisc_5.2-3          
#&gt; 
#&gt; loaded via a namespace (and not attached):
#&gt;   [1] dichromat_2.0-0.1       nnet_7.3-20             TH.data_1.1-3           vctrs_0.6.5             digest_0.6.37          
#&gt;   [6] png_0.1-8               shape_1.4.6.1           proxy_0.4-27            magick_2.8.6            fontLiberation_0.1.0   
#&gt;  [11] withr_3.0.2             ggpubr_0.6.0            survival_3.8-3          doRNG_1.8.6.2           emmeans_1.11.1         
#&gt;  [16] MatrixModels_0.5-4      systemfonts_1.2.3       ragg_1.4.0              zoo_1.8-14              V8_6.0.4               
#&gt;  [21] ggdist_3.3.3            DEoptimR_1.1-3-1        Formula_1.2-5           prettyunits_1.2.0       rematch2_2.1.2         
#&gt;  [26] httr_1.4.7              rstatix_0.7.2           globals_0.18.0          ps_1.9.1                rstudioapi_0.17.1      
#&gt;  [31] extremevalues_2.4.1     pan_1.9                 generics_0.1.4          processx_3.8.6          base64enc_0.1-3        
#&gt;  [36] curl_6.2.3              mitools_2.4             desc_1.4.3              xtable_1.8-4            svUnit_1.0.6           
#&gt;  [41] pracma_2.4.4            evaluate_1.0.3          hms_1.1.3               glmnet_4.1-9            rcartocolor_2.1.1      
#&gt;  [46] lmtest_0.9-40           robustbase_0.99-4-1     matrixStats_1.5.0       svgPanZoom_0.3.4        class_7.3-23           
#&gt;  [51] pillar_1.10.2           caTools_1.18.3          compiler_4.5.0          stringi_1.8.7           jomo_2.7-6             
#&gt;  [56] minqa_1.2.8             plyr_1.8.9              crayon_1.5.3            abind_1.4-8             metadat_1.4-0          
#&gt;  [61] sp_2.2-0                mathjaxr_1.8-0          rapportools_1.2         twosamples_2.0.1        sandwich_3.1-1         
#&gt;  [66] whisker_0.4.1           codetools_0.2-20        multcomp_1.4-28         textshaping_1.0.1       bcaboot_0.2-3          
#&gt;  [71] openssl_2.3.3           flextable_0.9.9         QuickJSR_1.7.0          e1071_1.7-16            gridtext_0.1.5         
#&gt;  [76] lme4_1.1-37             fs_1.6.6                itertools_0.1-3         listenv_0.9.1           Rdpack_2.6.4           
#&gt;  [81] pkgbuild_1.4.8          estimability_1.5.1      ggsignif_0.6.4          callr_3.7.6             tzdb_0.5.0             
#&gt;  [86] pkgconfig_2.0.3         tools_4.5.0             rbibutils_2.3           viridisLite_0.4.2       DBI_1.2.3              
#&gt;  [91] numDeriv_2016.8-1.1     fastmap_1.2.0           scales_1.4.0            officer_0.6.10          opdisDownsampling_1.0.1
#&gt;  [96] insight_1.3.0           rpart_4.1.24            farver_2.1.2            reformulas_0.4.1        survminer_0.5.0        
#&gt;  [ reached 'max' / getOption("max.print") -- omitted 55 entries ]</code></pre>
<hr>
</section>
<section id="references" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> References</h1>
<hr>


<!-- -->


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="2" data-line-spacing="2" role="list">
<div id="ref-gigerenzerStatisticalRitualsReplication2018" class="csl-entry" role="listitem">
1. Gigerenzer G. (2018). <span>“Statistical <span>Rituals</span>: <span>The Replication Delusion</span> and <span>How We Got There</span>.”</span> <em>Advances in Methods and Practices in Psychological Science</em>. <strong>1</strong>:198–218. doi: <a href="https://doi.org/10.1177/2515245918771329">10.1177/2515245918771329</a>.
</div>
<div id="ref-goodmanDirtyDozenTwelve2008" class="csl-entry" role="listitem">
2. Goodman S. (2008). <span>“A <span>Dirty Dozen</span>: <span>Twelve P</span>-<span>Value Misconceptions</span>.”</span> <em>Seminars in Hematology</em>. <strong>45</strong>:135–140. doi: <a href="https://doi.org/10.1053/j.seminhematol.2008.04.003">10.1053/j.seminhematol.2008.04.003</a>.
</div>
<div id="ref-greenlandStatisticalTestsValues2016" class="csl-entry" role="listitem">
3. Greenland S, Senn SJ, Rothman KJ, Carlin JB, Poole C, Goodman SN, et al. (2016). <span>“Statistical tests, <span>P</span> values, confidence intervals, and power: <span>A</span> guide to misinterpretations.”</span> <em>European Journal of Epidemiology</em>. <strong>31</strong>:337–350. doi: <a href="https://doi.org/10.1007/s10654-016-0149-3">10.1007/s10654-016-0149-3</a>.
</div>
<div id="ref-rafiSemanticCognitiveTools2020" class="csl-entry" role="listitem">
4. Rafi Z, Greenland S. (2020). <span>“Semantic and cognitive tools to aid statistical science: Replace confidence and significance by compatibility and surprise.”</span> <em>BMC Medical Research Methodology</em>. <strong>20</strong>:244. doi: <a href="https://doi.org/10.1186/s12874-020-01105-9">10.1186/s12874-020-01105-9</a>.
</div>
<div id="ref-greenlandValidPvaluesBehave2019" class="csl-entry" role="listitem">
5. Greenland S. (2019). <span>“Valid <span>P</span>-values behave exactly as they should: <span>Some</span> misleading criticisms of <span>P</span>-values and their resolution with <span>S</span>-values.”</span> <em>The American Statistician</em>. <strong>73</strong>:106–114. doi: <a href="https://doi.org/10.1080/00031305.2018.1529625">10.1080/00031305.2018.1529625</a>.
</div>
<div id="ref-greenlandSparseDataBias2016" class="csl-entry" role="listitem">
6. Greenland S, Mansournia MA, Altman DG. (2016). <span>“Sparse data bias: A problem hiding in plain sight.”</span> <em>BMJ</em>. <strong>352</strong>:i1981. doi: <a href="https://doi.org/10.1136/bmj.i1981">10.1136/bmj.i1981</a>.
</div>
<div id="ref-greenlandAidScientificInference2020" class="csl-entry" role="listitem">
7. Greenland S, Rafi Z. (2020). <span>“To <span>Aid Scientific Inference</span>, <span>Emphasize Unconditional Descriptions</span> of <span>Statistics</span>.”</span> <em>arXiv:190908583 [statME]</em>. <a href="https://arxiv.org/abs/1909.08583">https://arxiv.org/abs/1909.08583</a>.
</div>
<div id="ref-moskowitzFasterthanlightNeutrinosAren2012" class="csl-entry" role="listitem">
8. Moskowitz C. (2012). <span>“Faster-than-light neutrinos aren’t.”</span> <em>Scientific American</em>.
</div>
<div id="ref-perezgonzalezPvaluesPercentilesCommentary2015" class="csl-entry" role="listitem">
9. Perezgonzalez JD. (2015). <span>“P-values as percentiles. <span>Commentary</span> on: <span>‘<span>Null</span> hypothesis significance tests. <span>A</span> mixup of two different theories: The basis for widespread confusion and numerous misinterpretations’</span>.”</span> <em>Frontiers in Psychology</em>. <strong>6</strong>. doi: <a href="https://doi.org/10.3389/fpsyg.2015.00341">10.3389/fpsyg.2015.00341</a>.
</div>
<div id="ref-fraserPvalueFunctionStatistical2019" class="csl-entry" role="listitem">
10. Fraser DAS. (2019). <span>“The <span>P</span>-value function and statistical inference.”</span> <em>The American Statistician</em>. <strong>73</strong>:135–147. doi: <a href="https://doi.org/10.1080/00031305.2018.1556735">10.1080/00031305.2018.1556735</a>.
</div>
<div id="ref-sellkeCalibrationValuesTesting2001" class="csl-entry" role="listitem">
11. Sellke T, Bayarri MJ, Berger JO. (2001). <span>“Calibration of <span><span class="math inline">\(\rho\)</span></span> values for testing precise null hypotheses.”</span> <em>The American Statistician</em>. <strong>55</strong>:62–71. doi: <a href="https://doi.org/10.1198/000313001300339950">10.1198/000313001300339950</a>.
</div>
<div id="ref-greenlandTechnicalIssuesInterpretation2020" class="csl-entry" role="listitem">
12. Greenland S, Rafi Z. (2020). <span>“Technical <span>Issues</span> in the <span>Interpretation</span> of <span>S</span>-values and <span>Their Relation</span> to <span>Other Information Measures</span>.”</span> <em>arXiv:200812991 [statME]</em>. <a href="https://arxiv.org/abs/2008.12991">https://arxiv.org/abs/2008.12991</a>.
</div>
<div id="ref-shaferTestMartingalesBayes2011" class="csl-entry" role="listitem">
13. Shafer G, Shen A, Vereshchagin N, Vovk V. (2011). <span>“Test <span>Martingales</span>, <span>Bayes Factors</span> and p-<span>Values</span>.”</span> <em>Statistical Science</em>. <strong>26</strong>:84–101. doi: <a href="https://doi.org/10/fkcvt5">10/fkcvt5</a>.
</div>
<div id="ref-schuemieRobustEmpiricalCalibration2016" class="csl-entry" role="listitem">
14. Schuemie MJ, Hripcsak G, Ryan PB, Madigan D, Suchard MA. (2016). <span>“Robust empirical calibration of p-values using observational data.”</span> <em>Statistics in Medicine</em>. <strong>35</strong>:3883–3888. doi: <a href="https://doi.org/10/ghqmsb">10/ghqmsb</a>.
</div>
<div id="ref-gruberLimitationsEmpiricalCalibration2016" class="csl-entry" role="listitem">
15. Gruber S, Tchetgen ET. (2016). <span>“Limitations of empirical calibration of p-values using observational data.”</span> <em>Statistics in Medicine</em>. <strong>35</strong>:3869–3882. doi: <a href="https://doi.org/10/ghqmtn">10/ghqmtn</a>.
</div>
<div id="ref-neymanProblemMostEfficient1933" class="csl-entry" role="listitem">
16. Neyman J, Pearson ES. (1933). <span>“On the <span>Problem</span> of the <span>Most Efficient Tests</span> of <span>Statistical Hypotheses</span>.”</span> <em>Philosophical Transactions of the Royal Society of London Series A, Containing Papers of a Mathematical or Physical Character</em>. <strong>231</strong>:289–337. doi: <a href="https://doi.org/10.1098/rsta.1933.0009">10.1098/rsta.1933.0009</a>.
</div>
<div id="ref-whiteheadCaseFrequentismClinical1993" class="csl-entry" role="listitem">
17. Whitehead J. (1993). <span>“The case for frequentism in clinical trials.”</span> <em>Statistics in Medicine</em>. <strong>12</strong>:1405–1413. doi: <a href="https://doi.org/10.1002/sim.4780121506">10.1002/sim.4780121506</a>.
</div>
<div id="ref-rubinWhatTypeType2019" class="csl-entry" role="listitem">
18. Rubin M. (2019). <span>“What type of <span>Type I</span> error? <span>Contrasting</span> the <span>Neyman</span> and <span>Fisherian</span> approaches in the context of exact and direct replications.”</span> <em>Synthese</em>. doi: <a href="https://doi.org/10.1007/s11229-019-02433-0">10.1007/s11229-019-02433-0</a>.
</div>
<div id="ref-Lehmann2011-vs" class="csl-entry" role="listitem">
19. Lehmann EL. (2011). <span>“Fisher, <span>Neyman</span>, and the <span>Creation</span> of <span>Classical Statistics</span>.”</span> <span>Springer New York</span>. doi: <a href="https://doi.org/10.1007/978-1-4419-9500-1">10.1007/978-1-4419-9500-1</a>.
</div>
<div id="ref-fisherDesignExperiments1935" class="csl-entry" role="listitem">
20. Fisher RA. (1935). <span>“The <span>Design</span> of <span>Experiments</span>.”</span> <span>Oxford, England</span>: <span>Oliver &amp; Boyd</span>.
</div>
<div id="ref-fisherStatisticalMethodsScientific1955" class="csl-entry" role="listitem">
21. Fisher R. (1955). <span>“Statistical <span>Methods</span> and <span>Scientific Induction</span>.”</span> <em>Journal of the Royal Statistical Society Series B (Methodological)</em>. <strong>17</strong>:69–78. doi: <a href="https://doi.org/10.1111/j.2517-6161.1955.tb00180.x">10.1111/j.2517-6161.1955.tb00180.x</a>.
</div>
<div id="ref-bickelNullHypothesisSignificance2019" class="csl-entry" role="listitem">
22. Bickel DR. (2019). <span>“Null <span>Hypothesis Significance Testing Defended</span> and <span>Calibrated</span> by <span>Bayesian Model Checking</span>.”</span> <em>The American Statistician</em>. <strong>0</strong>:1–16. doi: <a href="https://doi.org/10.1080/00031305.2019.1699443">10.1080/00031305.2019.1699443</a>.
</div>
<div id="ref-jeffreysTestsSignificanceTreated1935" class="csl-entry" role="listitem">
23. Jeffreys H. (1935). <span>“Some <span>Tests</span> of <span>Significance</span>, <span>Treated</span> by the <span>Theory</span> of <span>Probability</span>.”</span> <em>Mathematical Proceedings of the Cambridge Philosophical Society</em>. <strong>31</strong>:203–222. doi: <a href="https://doi.org/10.1017/S030500410001330X">10.1017/S030500410001330X</a>.
</div>
<div id="ref-jeffreysTheoryProbability1998" class="csl-entry" role="listitem">
24. Jeffreys H. (1998). <span>“The <span>Theory</span> of <span>Probability</span>.”</span> <span>OUP Oxford</span>.
</div>
<div id="ref-royallStatisticalEvidenceLikelihood1997" class="csl-entry" role="listitem">
25. Royall R. (1997). <span>“Statistical <span>Evidence</span>: <span>A Likelihood Paradigm</span>.”</span> <span>CRC Press</span>.
</div>
<div id="ref-coleSurprise2020" class="csl-entry" role="listitem">
26. Cole SR, Edwards JK, Greenland S. (2020). <span>“Surprise!”</span> <em>American Journal of Epidemiology</em>. doi: <a href="https://doi.org/10/gg63md">10/gg63md</a>.
</div>
<div id="ref-Shannon1948-uq" class="csl-entry" role="listitem">
27. Shannon CE. (1948). <span>“A mathematical theory of communication.”</span> <em>The Bell System Technical Journal</em>. <strong>27</strong>:379–423. doi: <a href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">10.1002/j.1538-7305.1948.tb01338.x</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/lesslikely\.com");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb8" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="an">layout:</span><span class="co"> post</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> P-values Are Tough And S-values Can Help</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> Zad Rafi</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2018-11-11T00:00:00.000Z</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="an">lastmod:</span><span class="co"> '`r Sys.Date()`'</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> &gt;-</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">  An extensive discussion about what P-values are, their properties, common</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">  interpretations, misinterpretations, and how a measure called an S-value may</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co">  better help us interpret them.</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> &gt;-</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">  https://res.cloudinary.com/less-likely/image/upload/f_auto,q_auto/v1605000180/Site/pvalueassumptions.png</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="an">og_image:</span><span class="co"> &gt;-</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co">  https://res.cloudinary.com/less-likely/image/upload/f_auto,q_auto/v1605000180/Site/pvalueassumptions.png</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="an">slug:</span><span class="co"> s-values</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="an">archives:</span><span class="co"> statistics</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="an">url:</span><span class="co"> statistics/s-values</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="an">zotero:</span><span class="co"> true</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="co"> </span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="an">tags:</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="co">  - accuracy</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co">  - math</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="co">  - statistical reporting</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co">  - p-value</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="co">  - p-values</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co">  - p value</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="co">  - p values</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="co">  - null hypothesis</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="co">  - test statistic</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="co">  - statistical test</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="co">  - statistical significance</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="co">  - bits</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="co">  - compatibility</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="co">  - results due to chance</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="co">  - evidence</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="co">  - statistical model</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a><span class="co">  - null hypothesis significance testing</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a><span class="co">  - hypothesis testing</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a><span class="co">  - significance testing</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a><span class="co">  - neyman</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a><span class="co">  - fisher</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a><span class="an">output:</span></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a><span class="co">  blogdown::html_page:</span></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a><span class="co">    df_print: kable</span></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">script</span><span class="ot"> src</span><span class="op">=</span><span class="st">"s-values_files/libs/kePrint-0.0.1/kePrint.js"</span><span class="dt">&gt;&lt;/</span><span class="kw">script</span><span class="dt">&gt;</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">link</span><span class="ot"> href</span><span class="op">=</span><span class="st">"s-values_files/libs/lightable-0.0.1/lightable.css"</span><span class="ot"> rel</span><span class="op">=</span><span class="st">"stylesheet"</span><span class="ot"> </span><span class="dt">/&gt;</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>The $P$-value doesn't have many fans. There are those who don't</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>understand it, often treating it as a measure it's not, whether that's a</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>posterior probability, the probability of getting results due to chance</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>alone, or some other bizarre/incorrect interpretation.</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a><span class="sc">\[</span>@gigerenzerStatisticalRitualsReplication2018;</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>@goodmanDirtyDozenTwelve2008; @greenlandStatisticalTestsValues2016<span class="sc">\]</span> Then</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>there are those who dislike it because they think the concept is too</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>difficult to understand or because they see it as a noisy statistic</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>we're not interested in.</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>However, the groups of people mentioned above aren't mutually exclusive.</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>Many who dislike and criticize the $P$-value also do not understand its</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>properties and behavior. This is unfortunate, given how important and</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>widely used they are. In this article, which could also have been</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>titled, $P$-values: More Than You Ever Wanted to Know, I take on the</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>task of explaining:</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>what $P$-values are</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the assumptions behind them</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>their properties and behavior</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>different schools of interpretation</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>misleading criticisms of $P$-values</span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>some valid issues in interpretation</span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>how these issues can be resolved</span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a><span class="fu"># What is a *P*-value Anyway?</span></span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a><span class="fu">## Some Definitions &amp; Descriptions</span></span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a>The $P$-value is the probability of getting a result (specifically, a</span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a>test statistic) at least as extreme as what was observed if **every</span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a>model assumption**, in addition to the targeted test hypothesis (usually</span>
<span id="cb8-113"><a href="#cb8-113" aria-hidden="true" tabindex="-1"></a>a null hypothesis), used to compute it **were correct**.</span>
<span id="cb8-114"><a href="#cb8-114" aria-hidden="true" tabindex="-1"></a><span class="sc">\[</span>@rafiSemanticCognitiveTools2020; @greenlandStatisticalTestsValues2016;</span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true" tabindex="-1"></a>@greenlandValidPvaluesBehave2019<span class="sc">\]</span></span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-117"><a href="#cb8-117" aria-hidden="true" tabindex="-1"></a>A simple, mathematically rigorous definition of a $P$-value (for those</span>
<span id="cb8-118"><a href="#cb8-118" aria-hidden="true" tabindex="-1"></a>interested) is given by [Stark</span>
<span id="cb8-119"><a href="#cb8-119" aria-hidden="true" tabindex="-1"></a>(2015)](https://www.stat.berkeley.edu/~stark/Preprints/pValues.pdf).</span>
<span id="cb8-120"><a href="#cb8-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-121"><a href="#cb8-121" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-122"><a href="#cb8-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-123"><a href="#cb8-123" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Let $P$ be the probability distribution of the data $X$, which takes</span></span>
<span id="cb8-124"><a href="#cb8-124" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; values in the measurable space $\mathcal{X}$. Let</span></span>
<span id="cb8-125"><a href="#cb8-125" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $\left</span><span class="sc">\{</span><span class="at">R_{\alpha}\right\}_{\alpha \in</span><span class="co">[</span><span class="ot">0,1</span><span class="co">]</span><span class="at">}$ be a collection of $P$</span></span>
<span id="cb8-126"><a href="#cb8-126" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; -measurable subsets of $\mathcal{X}$ such that (1)</span></span>
<span id="cb8-127"><a href="#cb8-127" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $P\left(R_{\alpha}\right)=\alpha$ and (2) If $\alpha^{\prime}&lt;\alpha$</span></span>
<span id="cb8-128"><a href="#cb8-128" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; then $R_{\alpha^{\prime}} \subset R_{\alpha}$. Then the $P$-value of</span></span>
<span id="cb8-129"><a href="#cb8-129" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $H_{0}$ for data $X=x$ is inf</span></span>
<span id="cb8-130"><a href="#cb8-130" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $_{\alpha \in[0,1]}\left\{\alpha: x \in R_{\alpha}\right</span><span class="sc">\}</span><span class="at">$.</span></span>
<span id="cb8-131"><a href="#cb8-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-132"><a href="#cb8-132" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-133"><a href="#cb8-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-134"><a href="#cb8-134" aria-hidden="true" tabindex="-1"></a>A descriptive but technical definition is given by Sander Greenland</span>
<span id="cb8-135"><a href="#cb8-135" aria-hidden="true" tabindex="-1"></a>below. The description can seem dense, so feel free to skip over it for</span>
<span id="cb8-136"><a href="#cb8-136" aria-hidden="true" tabindex="-1"></a>now and revisit it after reading the rest of the post.</span>
<span id="cb8-137"><a href="#cb8-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-138"><a href="#cb8-138" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-139"><a href="#cb8-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-140"><a href="#cb8-140" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; A single $P$-value $p$ is the quantile location of a directional</span></span>
<span id="cb8-141"><a href="#cb8-141" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; measure of divergence $t$ = $t(y;M)$ of the data point $y$ (usually,</span></span>
<span id="cb8-142"><a href="#cb8-142" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; the vector in $n$-space formed by $n$ individual observations) from a</span></span>
<span id="cb8-143"><a href="#cb8-143" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; test model manifold $M$ in the $n$-dimensional expectation space</span></span>
<span id="cb8-144"><a href="#cb8-144" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; defined the logical structure of the data generator ("experiment" or</span></span>
<span id="cb8-145"><a href="#cb8-145" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; causal structure) that produced the data $y$. $M$ is the subset of the</span></span>
<span id="cb8-146"><a href="#cb8-146" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $Y$-space into which the conjunction of the model constraints</span></span>
<span id="cb8-147"><a href="#cb8-147" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; (assumptions) force the data expectation or predict where y would be</span></span>
<span id="cb8-148"><a href="#cb8-148" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; were there no 'random' variability. I also use $M$ to denote the set</span></span>
<span id="cb8-149"><a href="#cb8-149" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; of all the model constraints, as well as their conjunction.</span></span>
<span id="cb8-150"><a href="#cb8-150" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb8-151"><a href="#cb8-151" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; With this logical set-up, the observed $P$-value is the quantile $p$</span></span>
<span id="cb8-152"><a href="#cb8-152" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; for the observed value $t$ of $T$ = $t(Y;M)$. This $p$ is read off a</span></span>
<span id="cb8-153"><a href="#cb8-153" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; reference distribution $F = F(t;M)$ for $T$ derived from $M$. This</span></span>
<span id="cb8-154"><a href="#cb8-154" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; formulation is essentially that of the "value of P" appearing in</span></span>
<span id="cb8-155"><a href="#cb8-155" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Pearson's seminal 1900 paper on goodness-of-fit tests. Notably, his</span></span>
<span id="cb8-156"><a href="#cb8-156" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; famed chi-squared statistic is the squared Euclidean distance from $y$</span></span>
<span id="cb8-157"><a href="#cb8-157" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; to $M$, with coordinates expressed in standard-deviation units derived</span></span>
<span id="cb8-158"><a href="#cb8-158" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; from $M$.</span></span>
<span id="cb8-159"><a href="#cb8-159" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb8-160"><a href="#cb8-160" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; More broadly, the statistic $T$ can be taken as a measure of</span></span>
<span id="cb8-161"><a href="#cb8-161" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; divergence of a more general embedding or background model manifold</span></span>
<span id="cb8-162"><a href="#cb8-162" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $A$ (which includes all 'auxiliary' assumptions) from a more</span></span>
<span id="cb8-163"><a href="#cb8-163" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; restrictive model $M$, with the goodness-of-fit case taking $A$ as a</span></span>
<span id="cb8-164"><a href="#cb8-164" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; saturated model covering the entire observation space, and the more</span></span>
<span id="cb8-165"><a href="#cb8-165" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; common "hypothesis testing" case taking M as the conjunction of an</span></span>
<span id="cb8-166"><a href="#cb8-166" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; unsaturated $A$ with a targeted 'test' constraint (or set of</span></span>
<span id="cb8-167"><a href="#cb8-167" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; constraints) $H$. This $H$ is logically independent of $A$ and</span></span>
<span id="cb8-168"><a href="#cb8-168" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; consistent with $A$, with $M$ = $H$ &amp; $A$ in logical terms, or $M$ =</span></span>
<span id="cb8-169"><a href="#cb8-169" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $H$ + $A$ in set-theoretic terms with + being union (in particular, we</span></span>
<span id="cb8-170"><a href="#cb8-170" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; assume no element in $H$ is entailed or contradicted by $A$ and no</span></span>
<span id="cb8-171"><a href="#cb8-171" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; element in $A$ is entailed or contradicted by $H$).</span></span>
<span id="cb8-172"><a href="#cb8-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-173"><a href="#cb8-173" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-174"><a href="#cb8-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-175"><a href="#cb8-175" aria-hidden="true" tabindex="-1"></a><span class="fu">## Misleading Definitions</span></span>
<span id="cb8-176"><a href="#cb8-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-177"><a href="#cb8-177" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-178"><a href="#cb8-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-179"><a href="#cb8-179" aria-hidden="true" tabindex="-1"></a>It is very common to see the $P$-value defined as</span>
<span id="cb8-180"><a href="#cb8-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-181"><a href="#cb8-181" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-182"><a href="#cb8-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-183"><a href="#cb8-183" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **The probability of obtaining test results at least as extreme as the</span></span>
<span id="cb8-184"><a href="#cb8-184" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; results actually observed, under the assumption that the null</span></span>
<span id="cb8-185"><a href="#cb8-185" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; hypothesis is correct.**</span></span>
<span id="cb8-186"><a href="#cb8-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-187"><a href="#cb8-187" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-188"><a href="#cb8-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-189"><a href="#cb8-189" aria-hidden="true" tabindex="-1"></a>Indeed, this is the actual definition currently given on the [Wikipedia</span>
<span id="cb8-190"><a href="#cb8-190" aria-hidden="true" tabindex="-1"></a>page for the topic](https://en.wikipedia.org/wiki/P-value), however, it</span>
<span id="cb8-191"><a href="#cb8-191" aria-hidden="true" tabindex="-1"></a>is inadequate and misleading because it hides and reifies the other</span>
<span id="cb8-192"><a href="#cb8-192" aria-hidden="true" tabindex="-1"></a>assumptions used to compute the $P$-value and exclusively focuses on the</span>
<span id="cb8-193"><a href="#cb8-193" aria-hidden="true" tabindex="-1"></a>null hypothesis.</span>
<span id="cb8-194"><a href="#cb8-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-195"><a href="#cb8-195" aria-hidden="true" tabindex="-1"></a>The test hypothesis (often the null hypothesis) is only one component of</span>
<span id="cb8-196"><a href="#cb8-196" aria-hidden="true" tabindex="-1"></a>the entire model that is being tested. This is reflected in the first</span>
<span id="cb8-197"><a href="#cb8-197" aria-hidden="true" tabindex="-1"></a>definition I gave above, which explicitly emphasizes that every model</span>
<span id="cb8-198"><a href="#cb8-198" aria-hidden="true" tabindex="-1"></a>assumption must be true. Thus, the $P$-value is sensitive to all these</span>
<span id="cb8-199"><a href="#cb8-199" aria-hidden="true" tabindex="-1"></a>assumptions and their violation(s).</span>
<span id="cb8-200"><a href="#cb8-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-201"><a href="#cb8-201" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-202"><a href="#cb8-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-203"><a href="#cb8-203" aria-hidden="true" tabindex="-1"></a><span class="fu">## Auxilliary Assumptions</span></span>
<span id="cb8-204"><a href="#cb8-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-205"><a href="#cb8-205" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-206"><a href="#cb8-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-207"><a href="#cb8-207" aria-hidden="true" tabindex="-1"></a>Some of these key **assumptions** behind the computation of a $P$-value</span>
<span id="cb8-208"><a href="#cb8-208" aria-hidden="true" tabindex="-1"></a>are that some sort of **random process was employed** (random sampling,</span>
<span id="cb8-209"><a href="#cb8-209" aria-hidden="true" tabindex="-1"></a>random assignment, etc.), that there are **no uncontrolled sources of</span>
<span id="cb8-210"><a href="#cb8-210" aria-hidden="true" tabindex="-1"></a>bias** (confounding, programming errors, equipment defects, sparse-data</span>
<span id="cb8-211"><a href="#cb8-211" aria-hidden="true" tabindex="-1"></a>bias)<span class="sc">\[</span>@greenlandSparseDataBias2016<span class="sc">\]</span> in the results, and that **the test</span>
<span id="cb8-212"><a href="#cb8-212" aria-hidden="true" tabindex="-1"></a>hypothesis** (often the null hypothesis) **is correct**. Some of these</span>
<span id="cb8-213"><a href="#cb8-213" aria-hidden="true" tabindex="-1"></a>assumptions can be seen in the figure below from</span>
<span id="cb8-214"><a href="#cb8-214" aria-hidden="true" tabindex="-1"></a><span class="sc">\[</span>@greenlandAidScientificInference2020<span class="sc">\]</span>, which will be discussed later</span>
<span id="cb8-215"><a href="#cb8-215" aria-hidden="true" tabindex="-1"></a>on. This entire set of assumptions is generally referred to as the *test</span>
<span id="cb8-216"><a href="#cb8-216" aria-hidden="true" tabindex="-1"></a>model*, and that is because the entire assumed model is being tested.</span>
<span id="cb8-217"><a href="#cb8-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-218"><a href="#cb8-218" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-219"><a href="#cb8-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-220"><a href="#cb8-220" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb8-221"><a href="#cb8-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-222"><a href="#cb8-222" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"https://res.cloudinary.com/less-likely/image/upload/v1605000180/Site/pvalueassumptions.svg"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">"P-value assumptions"</span><span class="ot"> width</span><span class="op">=</span><span class="st">"500"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"cursor: zoom-in"</span><span class="dt">/&gt;</span></span>
<span id="cb8-223"><a href="#cb8-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-224"><a href="#cb8-224" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">figcaption</span><span class="dt">&gt;</span></span>
<span id="cb8-225"><a href="#cb8-225" aria-hidden="true" tabindex="-1"></a>Conditional versus unconditional interpretations of</span>
<span id="cb8-226"><a href="#cb8-226" aria-hidden="true" tabindex="-1"></a>P-values, S-values, and compatibility intervals (CIs). (A) Conditional</span>
<span id="cb8-227"><a href="#cb8-227" aria-hidden="true" tabindex="-1"></a>interpretation, in which background model assumptions, such as no</span>
<span id="cb8-228"><a href="#cb8-228" aria-hidden="true" tabindex="-1"></a>systematic error, are assumed to be correct; thus,the information</span>
<span id="cb8-229"><a href="#cb8-229" aria-hidden="true" tabindex="-1"></a>provided by the P-value and S-value is targeted towards the test</span>
<span id="cb8-230"><a href="#cb8-230" aria-hidden="true" tabindex="-1"></a>hypothesis. (B)Unconditional interpretation, in which no aspect of the</span>
<span id="cb8-231"><a href="#cb8-231" aria-hidden="true" tabindex="-1"></a>statistical model is assumed to be correct; thus,the information</span>
<span id="cb8-232"><a href="#cb8-232" aria-hidden="true" tabindex="-1"></a>provided by the P-value and S-value is targeted toward the entire test</span>
<span id="cb8-233"><a href="#cb8-233" aria-hidden="true" tabindex="-1"></a>model.</span>
<span id="cb8-234"><a href="#cb8-234" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">figcaption</span><span class="dt">&gt;</span></span>
<span id="cb8-235"><a href="#cb8-235" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb8-236"><a href="#cb8-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-237"><a href="#cb8-237" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-238"><a href="#cb8-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-239"><a href="#cb8-239" aria-hidden="true" tabindex="-1"></a>We often start from the position that all those assumptions are correct</span>
<span id="cb8-240"><a href="#cb8-240" aria-hidden="true" tabindex="-1"></a>(hence, we "condition" on them, even though they are often not</span>
<span id="cb8-241"><a href="#cb8-241" aria-hidden="true" tabindex="-1"></a>correct)<span class="sc">\[</span>@greenlandAidScientificInference2020<span class="sc">\]</span> when calculating the</span>
<span id="cb8-242"><a href="#cb8-242" aria-hidden="true" tabindex="-1"></a>$P$-value, so that any deviation of the data from what was expected</span>
<span id="cb8-243"><a href="#cb8-243" aria-hidden="true" tabindex="-1"></a>under those assumptions would be **purely random error**. But in reality</span>
<span id="cb8-244"><a href="#cb8-244" aria-hidden="true" tabindex="-1"></a>such deviations could also be the result of **any** assumptions being</span>
<span id="cb8-245"><a href="#cb8-245" aria-hidden="true" tabindex="-1"></a>false, including *but not limited to* the test hypothesis.</span>
<span id="cb8-246"><a href="#cb8-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-247"><a href="#cb8-247" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-248"><a href="#cb8-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-249"><a href="#cb8-249" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Note: "Conditioning" here refers to taking the assumptions in the</span></span>
<span id="cb8-250"><a href="#cb8-250" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; model as given, and should not be confused with conditional</span></span>
<span id="cb8-251"><a href="#cb8-251" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; probability.</span></span>
<span id="cb8-252"><a href="#cb8-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-253"><a href="#cb8-253" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-254"><a href="#cb8-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-255"><a href="#cb8-255" aria-hidden="true" tabindex="-1"></a>For example, in high-energy physics, neutrinos were found in one study</span>
<span id="cb8-256"><a href="#cb8-256" aria-hidden="true" tabindex="-1"></a>to be faster than light due to the resulting large test statistic and</span>
<span id="cb8-257"><a href="#cb8-257" aria-hidden="true" tabindex="-1"></a>corresponding small $P$-value, but this result was later found to be a</span>
<span id="cb8-258"><a href="#cb8-258" aria-hidden="true" tabindex="-1"></a>result of a defect in the fiber-optic timing system for that</span>
<span id="cb8-259"><a href="#cb8-259" aria-hidden="true" tabindex="-1"></a>experiment.<span class="sc">\[</span>@moskowitzFasterthanlightNeutrinosAren2012<span class="sc">\]</span> Thus, the low</span>
<span id="cb8-260"><a href="#cb8-260" aria-hidden="true" tabindex="-1"></a>$P$-value was not because the assumed null hypothesis was false, but</span>
<span id="cb8-261"><a href="#cb8-261" aria-hidden="true" tabindex="-1"></a>instead due to a bias in the procedure.</span>
<span id="cb8-262"><a href="#cb8-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-263"><a href="#cb8-263" aria-hidden="true" tabindex="-1"></a>So the $P$-value **cannot** be the probability of one of these</span>
<span id="cb8-264"><a href="#cb8-264" aria-hidden="true" tabindex="-1"></a>assumptions, such as *"the probability of getting results due to chance</span>
<span id="cb8-265"><a href="#cb8-265" aria-hidden="true" tabindex="-1"></a>alone."* A statement like this is **backwards** because it's quantifying</span>
<span id="cb8-266"><a href="#cb8-266" aria-hidden="true" tabindex="-1"></a>one of the assumptions behind the computation of a $P$-value.</span>
<span id="cb8-267"><a href="#cb8-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-268"><a href="#cb8-268" aria-hidden="true" tabindex="-1"></a>This assumption of chance causing the results is assumed to be true (aka</span>
<span id="cb8-269"><a href="#cb8-269" aria-hidden="true" tabindex="-1"></a>100%) along with several other things, when calculating the $P$-value,</span>
<span id="cb8-270"><a href="#cb8-270" aria-hidden="true" tabindex="-1"></a>but this does not mean it is actually correct and the calculation of the</span>
<span id="cb8-271"><a href="#cb8-271" aria-hidden="true" tabindex="-1"></a>$P$-value *cannot* be the *probability* of one of those **assumptions**.</span>
<span id="cb8-272"><a href="#cb8-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-273"><a href="#cb8-273" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-274"><a href="#cb8-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-275"><a href="#cb8-275" aria-hidden="true" tabindex="-1"></a><span class="fu">## Probability of What?</span></span>
<span id="cb8-276"><a href="#cb8-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-277"><a href="#cb8-277" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-278"><a href="#cb8-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-279"><a href="#cb8-279" aria-hidden="true" tabindex="-1"></a>It is also important to clarify that $P$-values are not *probabilities</span>
<span id="cb8-280"><a href="#cb8-280" aria-hidden="true" tabindex="-1"></a>of data* or parameter values, which many like to say to differentiate</span>
<span id="cb8-281"><a href="#cb8-281" aria-hidden="true" tabindex="-1"></a>from probabilities of hypotheses. Rather, $P$-values are probabilities</span>
<span id="cb8-282"><a href="#cb8-282" aria-hidden="true" tabindex="-1"></a>of "data features", such as test statistics (i.e.&nbsp;a z-score or</span>
<span id="cb8-283"><a href="#cb8-283" aria-hidden="true" tabindex="-1"></a>$\chi^{2}$ statistic) or can be interpreted as the percentile at which</span>
<span id="cb8-284"><a href="#cb8-284" aria-hidden="true" tabindex="-1"></a>the observed test statistic falls within the expected distribution for</span>
<span id="cb8-285"><a href="#cb8-285" aria-hidden="true" tabindex="-1"></a>the test statistic, assuming all the model assumptions are</span>
<span id="cb8-286"><a href="#cb8-286" aria-hidden="true" tabindex="-1"></a>true.<span class="sc">\[</span>@perezgonzalezPvaluesPercentilesCommentary2015;</span>
<span id="cb8-287"><a href="#cb8-287" aria-hidden="true" tabindex="-1"></a>@fraserPvalueFunctionStatistical2019<span class="sc">\]</span></span>
<span id="cb8-288"><a href="#cb8-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-289"><a href="#cb8-289" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-290"><a href="#cb8-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-291"><a href="#cb8-291" aria-hidden="true" tabindex="-1"></a><span class="fu">## Properties (Uniformity)</span></span>
<span id="cb8-292"><a href="#cb8-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-293"><a href="#cb8-293" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-294"><a href="#cb8-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-295"><a href="#cb8-295" aria-hidden="true" tabindex="-1"></a>A $P$-value is considered to be valid if over repeated trials it would</span>
<span id="cb8-296"><a href="#cb8-296" aria-hidden="true" tabindex="-1"></a>be uniform when the tested hypothesis and all other assumptions used to</span>
<span id="cb8-297"><a href="#cb8-297" aria-hidden="true" tabindex="-1"></a>compute the $P$-value are correct (see the histogram below to see what</span>
<span id="cb8-298"><a href="#cb8-298" aria-hidden="true" tabindex="-1"></a>this looks like). Typically, this test hypothesis is a null hypothesis</span>
<span id="cb8-299"><a href="#cb8-299" aria-hidden="true" tabindex="-1"></a>where the tested parameter value is usually 0 or 1, but this property</span>
<span id="cb8-300"><a href="#cb8-300" aria-hidden="true" tabindex="-1"></a>applies to any test hypothesis for any parameter value. Thus, there is</span>
<span id="cb8-301"><a href="#cb8-301" aria-hidden="true" tabindex="-1"></a>the random variable $P$, which (when valid) follows this uniform</span>
<span id="cb8-302"><a href="#cb8-302" aria-hidden="true" tabindex="-1"></a>distribution, and the realization of this random variable, $p$, which is</span>
<span id="cb8-303"><a href="#cb8-303" aria-hidden="true" tabindex="-1"></a>the observed $P$-value. The latter is what most researchers are</span>
<span id="cb8-304"><a href="#cb8-304" aria-hidden="true" tabindex="-1"></a>interpreting from studies.</span>
<span id="cb8-305"><a href="#cb8-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-306"><a href="#cb8-306" aria-hidden="true" tabindex="-1"></a>Thus, if we were to simulate two variables that are practically the same</span>
<span id="cb8-307"><a href="#cb8-307" aria-hidden="true" tabindex="-1"></a>(meaning there's no difference between them) and then compare them, say,</span>
<span id="cb8-308"><a href="#cb8-308" aria-hidden="true" tabindex="-1"></a>using a t-test, and we were to iterate this process 10000 times and plot</span>
<span id="cb8-309"><a href="#cb8-309" aria-hidden="true" tabindex="-1"></a>the distribution of the observed P-values, it would be uniform,</span>
<span id="cb8-310"><a href="#cb8-310" aria-hidden="true" tabindex="-1"></a>indicating that any P-value within the interval from 0-1 is just as</span>
<span id="cb8-311"><a href="#cb8-311" aria-hidden="true" tabindex="-1"></a>likely as any other to be observed.</span>
<span id="cb8-312"><a href="#cb8-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-313"><a href="#cb8-313" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-314"><a href="#cb8-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-315"><a href="#cb8-315" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb8-316"><a href="#cb8-316" aria-hidden="true" tabindex="-1"></a><span class="co">#' @title Simulation of valid P-values where test hypothesis is true</span></span>
<span id="cb8-317"><a href="#cb8-317" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param X The first variable we are simulating </span></span>
<span id="cb8-318"><a href="#cb8-318" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param Y The second variable we are simulating </span></span>
<span id="cb8-319"><a href="#cb8-319" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param n.sim # The number of simulations</span></span>
<span id="cb8-320"><a href="#cb8-320" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param t The object storing the t-test results</span></span>
<span id="cb8-321"><a href="#cb8-321" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param t.sim # Empty numeric vector to contain values</span></span>
<span id="cb8-322"><a href="#cb8-322" aria-hidden="true" tabindex="-1"></a><span class="co">#' @param n.samp # Sample size in each group</span></span>
<span id="cb8-323"><a href="#cb8-323" aria-hidden="true" tabindex="-1"></a><span class="co">#' @NOTE The null hypothesis does not have to be 0, it can be any value.</span></span>
<span id="cb8-324"><a href="#cb8-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-325"><a href="#cb8-325" aria-hidden="true" tabindex="-1"></a>n.sim <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb8-326"><a href="#cb8-326" aria-hidden="true" tabindex="-1"></a>t.sim <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n.sim)</span>
<span id="cb8-327"><a href="#cb8-327" aria-hidden="true" tabindex="-1"></a>n.samp <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb8-328"><a href="#cb8-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-329"><a href="#cb8-329" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n.sim) {</span>
<span id="cb8-330"><a href="#cb8-330" aria-hidden="true" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n.samp, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb8-331"><a href="#cb8-331" aria-hidden="true" tabindex="-1"></a>  Y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n.samp, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb8-332"><a href="#cb8-332" aria-hidden="true" tabindex="-1"></a>  df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(X, Y)</span>
<span id="cb8-333"><a href="#cb8-333" aria-hidden="true" tabindex="-1"></a>  t <span class="ot">&lt;-</span> <span class="fu">t.test</span>(X, Y, <span class="at">mu =</span> <span class="dv">0</span>, <span class="at">paired =</span> <span class="cn">FALSE</span>, </span>
<span id="cb8-334"><a href="#cb8-334" aria-hidden="true" tabindex="-1"></a>              <span class="at">var.equal =</span> <span class="cn">TRUE</span>, <span class="at">data =</span> df)</span>
<span id="cb8-335"><a href="#cb8-335" aria-hidden="true" tabindex="-1"></a>  t.sim[i] <span class="ot">&lt;-</span> t[[<span class="dv">3</span>]]</span>
<span id="cb8-336"><a href="#cb8-336" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-337"><a href="#cb8-337" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-338"><a href="#cb8-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-339"><a href="#cb8-339" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; Error in theme_less(): could not find function "theme_less"</span></span>
<span id="cb8-340"><a href="#cb8-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-341"><a href="#cb8-341" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-342"><a href="#cb8-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-343"><a href="#cb8-343" aria-hidden="true" tabindex="-1"></a>Many frequentist statisticians do not consider $P$-values to be</span>
<span id="cb8-344"><a href="#cb8-344" aria-hidden="true" tabindex="-1"></a>valid/useful if they fail to meet this validity criterion of being</span>
<span id="cb8-345"><a href="#cb8-345" aria-hidden="true" tabindex="-1"></a>uniform, hence they do not recognize variants such as the posterior</span>
<span id="cb8-346"><a href="#cb8-346" aria-hidden="true" tabindex="-1"></a>predictive $P$-value (which concentrates around values such as 0.5,</span>
<span id="cb8-347"><a href="#cb8-347" aria-hidden="true" tabindex="-1"></a>rather than being uniform) to be valid.</span>
<span id="cb8-348"><a href="#cb8-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-349"><a href="#cb8-349" aria-hidden="true" tabindex="-1"></a>Indeed, there have been great efforts to calibrate the $P$-value which</span>
<span id="cb8-350"><a href="#cb8-350" aria-hidden="true" tabindex="-1"></a>ranges from mathematical solutions such as taking the</span>
<span id="cb8-351"><a href="#cb8-351" aria-hidden="true" tabindex="-1"></a>$(1 + <span class="co">[</span><span class="ot">-e*p*\log(p)</span><span class="co">]</span>^{-1})^{-1}$ which gives the lower bound on the</span>
<span id="cb8-352"><a href="#cb8-352" aria-hidden="true" tabindex="-1"></a>conditional type I error,<span class="sc">\[</span>@sellkeCalibrationValuesTesting2001;</span>
<span id="cb8-353"><a href="#cb8-353" aria-hidden="true" tabindex="-1"></a>@greenlandTechnicalIssuesInterpretation2020<span class="sc">\]</span> to taking the</span>
<span id="cb8-354"><a href="#cb8-354" aria-hidden="true" tabindex="-1"></a>$C_{1}(K):=\sqrt{K}-1$ of the $P$-value (the square-root calibrator),</span>
<span id="cb8-355"><a href="#cb8-355" aria-hidden="true" tabindex="-1"></a>yielding a test martingale,<span class="sc">\[</span>@shaferTestMartingalesBayes2011<span class="sc">\]</span> or even</span>
<span id="cb8-356"><a href="#cb8-356" aria-hidden="true" tabindex="-1"></a>empirically attempting to recalibrate the $P$-value by collecting</span>
<span id="cb8-357"><a href="#cb8-357" aria-hidden="true" tabindex="-1"></a>observed $P$-values from observational studies with negative controls</span>
<span id="cb8-358"><a href="#cb8-358" aria-hidden="true" tabindex="-1"></a>("test-hypotheses where the exposure is not believed to cause the</span>
<span id="cb8-359"><a href="#cb8-359" aria-hidden="true" tabindex="-1"></a>outcome") and using them to calculate the empirical null</span>
<span id="cb8-360"><a href="#cb8-360" aria-hidden="true" tabindex="-1"></a>distribution.<span class="sc">\[</span>@schuemieRobustEmpiricalCalibration2016<span class="sc">\]</span></span>
<span id="cb8-361"><a href="#cb8-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-362"><a href="#cb8-362" aria-hidden="true" tabindex="-1"></a>The latter is done since observational studies are prone to several more</span>
<span id="cb8-363"><a href="#cb8-363" aria-hidden="true" tabindex="-1"></a>biases than controlled, randomized experiments, thus the observed</span>
<span id="cb8-364"><a href="#cb8-364" aria-hidden="true" tabindex="-1"></a>$P$-values and estimated effect sizes are used to calculate the</span>
<span id="cb8-365"><a href="#cb8-365" aria-hidden="true" tabindex="-1"></a>systematic errors within the sampling distribution and are used for</span>
<span id="cb8-366"><a href="#cb8-366" aria-hidden="true" tabindex="-1"></a>recalibration of the $P$-value. Whether or not this approach is</span>
<span id="cb8-367"><a href="#cb8-367" aria-hidden="true" tabindex="-1"></a>effective, however, is a different</span>
<span id="cb8-368"><a href="#cb8-368" aria-hidden="true" tabindex="-1"></a>matter.<span class="sc">\[</span>@gruberLimitationsEmpiricalCalibration2016<span class="sc">\]</span> In short,</span>
<span id="cb8-369"><a href="#cb8-369" aria-hidden="true" tabindex="-1"></a>calibration is an often sought-out property of $P$-values.</span>
<span id="cb8-370"><a href="#cb8-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-371"><a href="#cb8-371" aria-hidden="true" tabindex="-1"></a>Many frequentist statisticians do not consider $P$-values to be</span>
<span id="cb8-372"><a href="#cb8-372" aria-hidden="true" tabindex="-1"></a>valid/useful if they fail to meet this validity criterion of being</span>
<span id="cb8-373"><a href="#cb8-373" aria-hidden="true" tabindex="-1"></a>uniform, hence they do not recognize variants such as the posterior</span>
<span id="cb8-374"><a href="#cb8-374" aria-hidden="true" tabindex="-1"></a>predictive $P$-value (which concentrates around values such as 0.5,</span>
<span id="cb8-375"><a href="#cb8-375" aria-hidden="true" tabindex="-1"></a>rather than being uniform) to be valid.</span>
<span id="cb8-376"><a href="#cb8-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-377"><a href="#cb8-377" aria-hidden="true" tabindex="-1"></a>Indeed, there have been great efforts to calibrate the $P$-value which</span>
<span id="cb8-378"><a href="#cb8-378" aria-hidden="true" tabindex="-1"></a>ranges from mathematical solutions such as taking the</span>
<span id="cb8-379"><a href="#cb8-379" aria-hidden="true" tabindex="-1"></a>$(1 + <span class="co">[</span><span class="ot">-e*p*\log(p)</span><span class="co">]</span>^{-1})^{-1}$ which gives the lower bound on the</span>
<span id="cb8-380"><a href="#cb8-380" aria-hidden="true" tabindex="-1"></a>conditional type I error,<span class="sc">\[</span>@sellkeCalibrationValuesTesting2001;</span>
<span id="cb8-381"><a href="#cb8-381" aria-hidden="true" tabindex="-1"></a>@greenlandTechnicalIssuesInterpretation2020<span class="sc">\]</span> to taking the</span>
<span id="cb8-382"><a href="#cb8-382" aria-hidden="true" tabindex="-1"></a>$C_{1}(K):=\sqrt{K}-1$ of the $P$-value (the square-root calibrator),</span>
<span id="cb8-383"><a href="#cb8-383" aria-hidden="true" tabindex="-1"></a>yielding a test martingale,<span class="sc">\[</span>@shaferTestMartingalesBayes2011<span class="sc">\]</span> or even</span>
<span id="cb8-384"><a href="#cb8-384" aria-hidden="true" tabindex="-1"></a>empirically attempting to recalibrate the $P$-value by collecting</span>
<span id="cb8-385"><a href="#cb8-385" aria-hidden="true" tabindex="-1"></a>observed $P$-values from observational studies with negative controls</span>
<span id="cb8-386"><a href="#cb8-386" aria-hidden="true" tabindex="-1"></a>("test-hypotheses where the exposure is not believed to cause the</span>
<span id="cb8-387"><a href="#cb8-387" aria-hidden="true" tabindex="-1"></a>outcome") and using them to calculate the empirical null</span>
<span id="cb8-388"><a href="#cb8-388" aria-hidden="true" tabindex="-1"></a>distribution.<span class="sc">\[</span>@schuemieRobustEmpiricalCalibration2016<span class="sc">\]</span></span>
<span id="cb8-389"><a href="#cb8-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-390"><a href="#cb8-390" aria-hidden="true" tabindex="-1"></a>The latter is done since observational studies are prone to several more</span>
<span id="cb8-391"><a href="#cb8-391" aria-hidden="true" tabindex="-1"></a>biases than controlled, randomized experiments, thus the observed</span>
<span id="cb8-392"><a href="#cb8-392" aria-hidden="true" tabindex="-1"></a>$P$-values and estimated effect sizes are used to calculate the</span>
<span id="cb8-393"><a href="#cb8-393" aria-hidden="true" tabindex="-1"></a>systematic errors within the sampling distribution and are used for</span>
<span id="cb8-394"><a href="#cb8-394" aria-hidden="true" tabindex="-1"></a>recalibration of the $P$-value. Whether or not this approach is</span>
<span id="cb8-395"><a href="#cb8-395" aria-hidden="true" tabindex="-1"></a>effective, however, is a different</span>
<span id="cb8-396"><a href="#cb8-396" aria-hidden="true" tabindex="-1"></a>matter.<span class="sc">\[</span>@gruberLimitationsEmpiricalCalibration2016<span class="sc">\]</span> In short,</span>
<span id="cb8-397"><a href="#cb8-397" aria-hidden="true" tabindex="-1"></a>calibration is an often sought-out property of $P$-values.</span>
<span id="cb8-398"><a href="#cb8-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-399"><a href="#cb8-399" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-400"><a href="#cb8-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-401"><a href="#cb8-401" aria-hidden="true" tabindex="-1"></a><span class="fu"># The Different Interpretations</span></span>
<span id="cb8-402"><a href="#cb8-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-403"><a href="#cb8-403" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-404"><a href="#cb8-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-405"><a href="#cb8-405" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Decision-Theoretic Approach</span></span>
<span id="cb8-406"><a href="#cb8-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-407"><a href="#cb8-407" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-408"><a href="#cb8-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-409"><a href="#cb8-409" aria-hidden="true" tabindex="-1"></a>Many researchers interpret the $P$-value in a behavioral,</span>
<span id="cb8-410"><a href="#cb8-410" aria-hidden="true" tabindex="-1"></a>decision-guiding way such as being statistically significant or not</span>
<span id="cb8-411"><a href="#cb8-411" aria-hidden="true" tabindex="-1"></a>(defined below) depending on whether observed *p* from a study (the</span>
<span id="cb8-412"><a href="#cb8-412" aria-hidden="true" tabindex="-1"></a>realization of the random variable $P$) falls below a fixed cutoff level</span>
<span id="cb8-413"><a href="#cb8-413" aria-hidden="true" tabindex="-1"></a>($\alpha$, which is the maximum tolerable type I error</span>
<span id="cb8-414"><a href="#cb8-414" aria-hidden="true" tabindex="-1"></a>rate).<span class="sc">\[</span>@neymanProblemMostEfficient1933<span class="sc">\]</span></span>
<span id="cb8-415"><a href="#cb8-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-416"><a href="#cb8-416" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-417"><a href="#cb8-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-418"><a href="#cb8-418" aria-hidden="true" tabindex="-1"></a><span class="fu">## Statistical Significance</span></span>
<span id="cb8-419"><a href="#cb8-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-420"><a href="#cb8-420" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-421"><a href="#cb8-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-422"><a href="#cb8-422" aria-hidden="true" tabindex="-1"></a>Thus, in this approach, users do not care how small or large the</span>
<span id="cb8-423"><a href="#cb8-423" aria-hidden="true" tabindex="-1"></a>observed $P$-value $p$ is, but simply, whether or not it fell beneath</span>
<span id="cb8-424"><a href="#cb8-424" aria-hidden="true" tabindex="-1"></a>the pre-specified $\alpha$ level (often 0.05). If it falls below</span>
<span id="cb8-425"><a href="#cb8-425" aria-hidden="true" tabindex="-1"></a>$\alpha$ they behave inline with the rejection of this test hypothesis,</span>
<span id="cb8-426"><a href="#cb8-426" aria-hidden="true" tabindex="-1"></a>and if it fails to fall below $\alpha$, then they must behave in a</span>
<span id="cb8-427"><a href="#cb8-427" aria-hidden="true" tabindex="-1"></a>manner where they accept this test hypothesis. The phrase *statistical</span>
<span id="cb8-428"><a href="#cb8-428" aria-hidden="true" tabindex="-1"></a>significance*, simply indicates that the observed $P$-value $p$ fell</span>
<span id="cb8-429"><a href="#cb8-429" aria-hidden="true" tabindex="-1"></a>below this pre-specified $\alpha$ level, and nothing else. It does not</span>
<span id="cb8-430"><a href="#cb8-430" aria-hidden="true" tabindex="-1"></a>indicate any meaningful significance on its own.</span>
<span id="cb8-431"><a href="#cb8-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-432"><a href="#cb8-432" aria-hidden="true" tabindex="-1"></a>The pioneers of this approach, Jerzy Neyman and Egon Pearson, define</span>
<span id="cb8-433"><a href="#cb8-433" aria-hidden="true" tabindex="-1"></a>this behavioral guidance in their 1933 paper, "On the Problem of the</span>
<span id="cb8-434"><a href="#cb8-434" aria-hidden="true" tabindex="-1"></a>Most Efficient Tests of Statistical</span>
<span id="cb8-435"><a href="#cb8-435" aria-hidden="true" tabindex="-1"></a>Hypotheses"<span class="sc">\[</span>@neymanProblemMostEfficient1933<span class="sc">\]</span></span>
<span id="cb8-436"><a href="#cb8-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-437"><a href="#cb8-437" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-438"><a href="#cb8-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-439"><a href="#cb8-439" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Without hoping to know whether each separate hypothesis is true or</span></span>
<span id="cb8-440"><a href="#cb8-440" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; false, we may search for rules to govern our behavior with regard to</span></span>
<span id="cb8-441"><a href="#cb8-441" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; them, in following which we insure that, in the long run of</span></span>
<span id="cb8-442"><a href="#cb8-442" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; experience, we shall not be too often wrong.</span></span>
<span id="cb8-443"><a href="#cb8-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-444"><a href="#cb8-444" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-445"><a href="#cb8-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-446"><a href="#cb8-446" aria-hidden="true" tabindex="-1"></a>This decision-making framework may be useful in certain</span>
<span id="cb8-447"><a href="#cb8-447" aria-hidden="true" tabindex="-1"></a>scenarios,<span class="sc">\[</span>@whiteheadCaseFrequentismClinical1993<span class="sc">\]</span> where some sort of</span>
<span id="cb8-448"><a href="#cb8-448" aria-hidden="true" tabindex="-1"></a>randomization is possible, where experiments can be repeated, and where</span>
<span id="cb8-449"><a href="#cb8-449" aria-hidden="true" tabindex="-1"></a>there is large control over the experimental conditions, with one of the</span>
<span id="cb8-450"><a href="#cb8-450" aria-hidden="true" tabindex="-1"></a>most notable historical examples being Egon Pearson (son of Karl Pearson</span>
<span id="cb8-451"><a href="#cb8-451" aria-hidden="true" tabindex="-1"></a>and coauthor of Jerzy Neyman) using it to [improve quality</span>
<span id="cb8-452"><a href="#cb8-452" aria-hidden="true" tabindex="-1"></a>control](https://www.ams.org/journals/bull/1936-42-09/S0002-9904-1936-06365-2/S0002-9904-1936-06365-2.pdf)</span>
<span id="cb8-453"><a href="#cb8-453" aria-hidden="true" tabindex="-1"></a>in industrial settings.</span>
<span id="cb8-454"><a href="#cb8-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-455"><a href="#cb8-455" aria-hidden="true" tabindex="-1"></a>Contrary to some claims,<span class="sc">\[</span>@rubinWhatTypeType2019<span class="sc">\]</span> this approach does</span>
<span id="cb8-456"><a href="#cb8-456" aria-hidden="true" tabindex="-1"></a>**NOT** require exact replications of the experiments, instead, it</span>
<span id="cb8-457"><a href="#cb8-457" aria-hidden="true" tabindex="-1"></a>requires that a valid $\alpha$ level is used</span>
<span id="cb8-458"><a href="#cb8-458" aria-hidden="true" tabindex="-1"></a>consistently.<span class="sc">\[</span>@neymanProblemMostEfficient1933; @Lehmann2011-vs<span class="sc">\]</span> In this</span>
<span id="cb8-459"><a href="#cb8-459" aria-hidden="true" tabindex="-1"></a>approach, the exact, observed $P$-value from a study is not as relevant</span>
<span id="cb8-460"><a href="#cb8-460" aria-hidden="true" tabindex="-1"></a>and cannot validly be interpreted without an entire set of studies that</span>
<span id="cb8-461"><a href="#cb8-461" aria-hidden="true" tabindex="-1"></a>are compared to the fixed error rate ($\alpha$).</span>
<span id="cb8-462"><a href="#cb8-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-463"><a href="#cb8-463" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-464"><a href="#cb8-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-465"><a href="#cb8-465" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb8-466"><a href="#cb8-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-467"><a href="#cb8-467" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"https://res.cloudinary.com/less-likely/image/upload/f_auto,q_auto/v1554700110/Site/classicalgiants.png"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">"Picture of the giants who founded frequentist statistics such as Egon Pearson, Ronald Fisher, and Jerzy Neyman"</span><span class="ot"> width</span><span class="op">=</span><span class="st">"800"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"cursor: zoom-in"</span><span class="dt">/&gt;</span></span>
<span id="cb8-468"><a href="#cb8-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-469"><a href="#cb8-469" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">figcaption</span><span class="dt">&gt;</span></span>
<span id="cb8-470"><a href="#cb8-470" aria-hidden="true" tabindex="-1"></a>From left to right: Ronald A. Fisher, Jerzy Neyman, and Egon</span>
<span id="cb8-471"><a href="#cb8-471" aria-hidden="true" tabindex="-1"></a>Pearson.</span>
<span id="cb8-472"><a href="#cb8-472" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">figcaption</span><span class="dt">&gt;</span></span>
<span id="cb8-473"><a href="#cb8-473" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb8-474"><a href="#cb8-474" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-475"><a href="#cb8-475" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-476"><a href="#cb8-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-477"><a href="#cb8-477" aria-hidden="true" tabindex="-1"></a><span class="fu">## The Inductive Approach</span></span>
<span id="cb8-478"><a href="#cb8-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-479"><a href="#cb8-479" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-480"><a href="#cb8-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-481"><a href="#cb8-481" aria-hidden="true" tabindex="-1"></a>Others interpret the $P$-value $p$ in an inductive</span>
<span id="cb8-482"><a href="#cb8-482" aria-hidden="true" tabindex="-1"></a>inferential/evidential (**Fisherian**)</span>
<span id="cb8-483"><a href="#cb8-483" aria-hidden="true" tabindex="-1"></a>way,<span class="sc">\[</span>@fisherDesignExperiments1935;</span>
<span id="cb8-484"><a href="#cb8-484" aria-hidden="true" tabindex="-1"></a>@fisherStatisticalMethodsScientific1955<span class="sc">\]</span> as a **continuous** measure of</span>
<span id="cb8-485"><a href="#cb8-485" aria-hidden="true" tabindex="-1"></a>evidence against the very test hypothesis and entire model (all</span>
<span id="cb8-486"><a href="#cb8-486" aria-hidden="true" tabindex="-1"></a>assumptions) used to compute it (let's go with this for now, even though</span>
<span id="cb8-487"><a href="#cb8-487" aria-hidden="true" tabindex="-1"></a>there are some problems with this interpretation, more on that below).</span>
<span id="cb8-488"><a href="#cb8-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-489"><a href="#cb8-489" aria-hidden="true" tabindex="-1"></a>This interpretation as a continuous measure of evidence **against** the</span>
<span id="cb8-490"><a href="#cb8-490" aria-hidden="true" tabindex="-1"></a>test hypothesis and the entire model used to compute it can be seen in</span>
<span id="cb8-491"><a href="#cb8-491" aria-hidden="true" tabindex="-1"></a>the figure below from<span class="sc">\[</span>@greenlandAidScientificInference2020<span class="sc">\]</span>. In one</span>
<span id="cb8-492"><a href="#cb8-492" aria-hidden="true" tabindex="-1"></a>framework (left panel), we may assume certain assumptions to be true</span>
<span id="cb8-493"><a href="#cb8-493" aria-hidden="true" tabindex="-1"></a>("conditioning" on them, i.e, use of random assignment), and in the</span>
<span id="cb8-494"><a href="#cb8-494" aria-hidden="true" tabindex="-1"></a>other (right panel), we question all assumptions, hence the</span>
<span id="cb8-495"><a href="#cb8-495" aria-hidden="true" tabindex="-1"></a>"unconditional" interpretation. Unlike the **Neyman-Pearson** approach,</span>
<span id="cb8-496"><a href="#cb8-496" aria-hidden="true" tabindex="-1"></a>this inferential approach allows interpretation of $P$-values from</span>
<span id="cb8-497"><a href="#cb8-497" aria-hidden="true" tabindex="-1"></a>single studies, and indeed, lower values of it are taken as more</span>
<span id="cb8-498"><a href="#cb8-498" aria-hidden="true" tabindex="-1"></a>evidence against the tested hypothesis.</span>
<span id="cb8-499"><a href="#cb8-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-500"><a href="#cb8-500" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-501"><a href="#cb8-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-502"><a href="#cb8-502" aria-hidden="true" tabindex="-1"></a><span class="fu">## Null-Hypothesis Significance Testing</span></span>
<span id="cb8-503"><a href="#cb8-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-504"><a href="#cb8-504" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-505"><a href="#cb8-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-506"><a href="#cb8-506" aria-hidden="true" tabindex="-1"></a>However, it is also worth pointing out that most individuals do not</span>
<span id="cb8-507"><a href="#cb8-507" aria-hidden="true" tabindex="-1"></a>interpret $P$-values from a **Neyman-Pearson** or **Fisherian**</span>
<span id="cb8-508"><a href="#cb8-508" aria-hidden="true" tabindex="-1"></a>standpoint, rather, they fuse both approaches together, which is what we</span>
<span id="cb8-509"><a href="#cb8-509" aria-hidden="true" tabindex="-1"></a>commonly know today as "null-hypothesis significance testing." This</span>
<span id="cb8-510"><a href="#cb8-510" aria-hidden="true" tabindex="-1"></a>approach is regarded by most as being a incompatible hybrid given that</span>
<span id="cb8-511"><a href="#cb8-511" aria-hidden="true" tabindex="-1"></a>it often confuses error rates ($\alpha$, $\beta$), which are fixed</span>
<span id="cb8-512"><a href="#cb8-512" aria-hidden="true" tabindex="-1"></a>before a study, with the $P$-value, which is not a fixed error-rate, and</span>
<span id="cb8-513"><a href="#cb8-513" aria-hidden="true" tabindex="-1"></a>the fusion of these approaches often has been blamed for the replication</span>
<span id="cb8-514"><a href="#cb8-514" aria-hidden="true" tabindex="-1"></a>crisis in science by many statisticians. Though some believe these</span>
<span id="cb8-515"><a href="#cb8-515" aria-hidden="true" tabindex="-1"></a>approaches can be reconciled and are</span>
<span id="cb8-516"><a href="#cb8-516" aria-hidden="true" tabindex="-1"></a>useful.<span class="sc">\[</span>@bickelNullHypothesisSignificance2019<span class="sc">\]</span></span>
<span id="cb8-517"><a href="#cb8-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-518"><a href="#cb8-518" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-519"><a href="#cb8-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-520"><a href="#cb8-520" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb8-521"><a href="#cb8-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-522"><a href="#cb8-522" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"https://res.cloudinary.com/less-likely/image/upload/v1605000180/Site/pvalueassumptions.svg"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">"P-value assumptions"</span><span class="ot"> width</span><span class="op">=</span><span class="st">"500"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"cursor: zoom-in"</span><span class="dt">/&gt;</span></span>
<span id="cb8-523"><a href="#cb8-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-524"><a href="#cb8-524" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">figcaption</span><span class="dt">&gt;</span></span>
<span id="cb8-525"><a href="#cb8-525" aria-hidden="true" tabindex="-1"></a>Conditional versus unconditional interpretations of</span>
<span id="cb8-526"><a href="#cb8-526" aria-hidden="true" tabindex="-1"></a>P-values, S-values, and compatibility intervals (CIs). (A) Conditional</span>
<span id="cb8-527"><a href="#cb8-527" aria-hidden="true" tabindex="-1"></a>interpretation, in which background model assumptions, such as no</span>
<span id="cb8-528"><a href="#cb8-528" aria-hidden="true" tabindex="-1"></a>systematic error, are assumed to be correct; thus,the information</span>
<span id="cb8-529"><a href="#cb8-529" aria-hidden="true" tabindex="-1"></a>provided by the P-value and S-value is targeted towards the test</span>
<span id="cb8-530"><a href="#cb8-530" aria-hidden="true" tabindex="-1"></a>hypothesis. (B)Unconditional interpretation, in which no aspect of the</span>
<span id="cb8-531"><a href="#cb8-531" aria-hidden="true" tabindex="-1"></a>statistical model is assumed to be correct; thus,the information</span>
<span id="cb8-532"><a href="#cb8-532" aria-hidden="true" tabindex="-1"></a>provided by the P-value and S-value is targeted toward the entire test</span>
<span id="cb8-533"><a href="#cb8-533" aria-hidden="true" tabindex="-1"></a>model.</span>
<span id="cb8-534"><a href="#cb8-534" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">figcaption</span><span class="dt">&gt;</span></span>
<span id="cb8-535"><a href="#cb8-535" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb8-536"><a href="#cb8-536" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-537"><a href="#cb8-537" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-538"><a href="#cb8-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-539"><a href="#cb8-539" aria-hidden="true" tabindex="-1"></a>Back to the **Fisherian** approach, the interpretation of the $P$-value</span>
<span id="cb8-540"><a href="#cb8-540" aria-hidden="true" tabindex="-1"></a>as a continuous measure of evidence against the test model that produced</span>
<span id="cb8-541"><a href="#cb8-541" aria-hidden="true" tabindex="-1"></a>it shouldn't be confused with other statistics that serve as support</span>
<span id="cb8-542"><a href="#cb8-542" aria-hidden="true" tabindex="-1"></a>measures. Likelihood ratios and Bayes factors are **absolute** measures</span>
<span id="cb8-543"><a href="#cb8-543" aria-hidden="true" tabindex="-1"></a>of evidence **for** a model compared to another model, whereas the</span>
<span id="cb8-544"><a href="#cb8-544" aria-hidden="true" tabindex="-1"></a>$P$-value is a **relative** measure of "evidence" (more on that below)</span>
<span id="cb8-545"><a href="#cb8-545" aria-hidden="true" tabindex="-1"></a>that can be tricky to interpret.<span class="sc">\[</span>@jeffreysTestsSignificanceTreated1935;</span>
<span id="cb8-546"><a href="#cb8-546" aria-hidden="true" tabindex="-1"></a>@jeffreysTheoryProbability1998;</span>
<span id="cb8-547"><a href="#cb8-547" aria-hidden="true" tabindex="-1"></a>@royallStatisticalEvidenceLikelihood1997<span class="sc">\]</span> Indeed, this is why the</span>
<span id="cb8-548"><a href="#cb8-548" aria-hidden="true" tabindex="-1"></a>$P$-value is converted by some Bayesians to a lower bound of the Bayes</span>
<span id="cb8-549"><a href="#cb8-549" aria-hidden="true" tabindex="-1"></a>factor by taking $-e*p*\log(p)$.<span class="sc">\[</span>@sellkeCalibrationValuesTesting2001;</span>
<span id="cb8-550"><a href="#cb8-550" aria-hidden="true" tabindex="-1"></a>@greenlandTechnicalIssuesInterpretation2020<span class="sc">\]</span></span>
<span id="cb8-551"><a href="#cb8-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-552"><a href="#cb8-552" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-553"><a href="#cb8-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-554"><a href="#cb8-554" aria-hidden="true" tabindex="-1"></a><span class="fu">## Measure of Compatibility</span></span>
<span id="cb8-555"><a href="#cb8-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-556"><a href="#cb8-556" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-557"><a href="#cb8-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-558"><a href="#cb8-558" aria-hidden="true" tabindex="-1"></a>The $P$-value is not an absolute measure of evidence for a model (such</span>
<span id="cb8-559"><a href="#cb8-559" aria-hidden="true" tabindex="-1"></a>as the null/alternative model), it is a continuous **measure of the</span>
<span id="cb8-560"><a href="#cb8-560" aria-hidden="true" tabindex="-1"></a>compatibility** of the **observed data** with the **model** used to</span>
<span id="cb8-561"><a href="#cb8-561" aria-hidden="true" tabindex="-1"></a>compute it.<span class="sc">\[</span>@greenlandStatisticalTestsValues2016<span class="sc">\]</span></span>
<span id="cb8-562"><a href="#cb8-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-563"><a href="#cb8-563" aria-hidden="true" tabindex="-1"></a>If it's high, it means the observed data are **very compatible** with</span>
<span id="cb8-564"><a href="#cb8-564" aria-hidden="true" tabindex="-1"></a>the model used to compute it. If it's very low, then it indicates that</span>
<span id="cb8-565"><a href="#cb8-565" aria-hidden="true" tabindex="-1"></a>the data are **not as compatible** with the model used to calculate it,</span>
<span id="cb8-566"><a href="#cb8-566" aria-hidden="true" tabindex="-1"></a>and this low compatibility may be due to random variation and/or it may</span>
<span id="cb8-567"><a href="#cb8-567" aria-hidden="true" tabindex="-1"></a>be due to a violation of assumptions (such as the null model not being</span>
<span id="cb8-568"><a href="#cb8-568" aria-hidden="true" tabindex="-1"></a>true, not using randomization, a programming error or equipment defect</span>
<span id="cb8-569"><a href="#cb8-569" aria-hidden="true" tabindex="-1"></a>such as that [seen with</span>
<span id="cb8-570"><a href="#cb8-570" aria-hidden="true" tabindex="-1"></a>neutrinos](https://www.washingtonpost.com/blogs/compost/post/faster-than-light-neutrinos-arent/2012/02/23/gIQA5MmjVR_blog.html),</span>
<span id="cb8-571"><a href="#cb8-571" aria-hidden="true" tabindex="-1"></a>etc.).</span>
<span id="cb8-572"><a href="#cb8-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-573"><a href="#cb8-573" aria-hidden="true" tabindex="-1"></a>Low compatibility of the data with the model can be implied as evidence</span>
<span id="cb8-574"><a href="#cb8-574" aria-hidden="true" tabindex="-1"></a>against the test hypothesis, if we accept the rest of the model used to</span>
<span id="cb8-575"><a href="#cb8-575" aria-hidden="true" tabindex="-1"></a>compute the $P$-value. Thus, lower $P$-values from a **Fisherian**</span>
<span id="cb8-576"><a href="#cb8-576" aria-hidden="true" tabindex="-1"></a>perspective are seen as stronger evidence against the test hypothesis</span>
<span id="cb8-577"><a href="#cb8-577" aria-hidden="true" tabindex="-1"></a>given the rest of the model.</span>
<span id="cb8-578"><a href="#cb8-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-579"><a href="#cb8-579" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-580"><a href="#cb8-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-581"><a href="#cb8-581" aria-hidden="true" tabindex="-1"></a><span class="fu"># Common, Misleading Criticisms</span></span>
<span id="cb8-582"><a href="#cb8-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-583"><a href="#cb8-583" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-584"><a href="#cb8-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-585"><a href="#cb8-585" aria-hidden="true" tabindex="-1"></a><span class="fu">## Estimation and Intervals</span></span>
<span id="cb8-586"><a href="#cb8-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-587"><a href="#cb8-587" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-588"><a href="#cb8-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-589"><a href="#cb8-589" aria-hidden="true" tabindex="-1"></a>A common criticism put forth by many is that $P$-values are useless,</span>
<span id="cb8-590"><a href="#cb8-590" aria-hidden="true" tabindex="-1"></a>given that they cannot tell you the size of the effect and because they</span>
<span id="cb8-591"><a href="#cb8-591" aria-hidden="true" tabindex="-1"></a>are confounded by sample size and effect size, and that researchers</span>
<span id="cb8-592"><a href="#cb8-592" aria-hidden="true" tabindex="-1"></a>should instead give compatibility (confidence) intervals. However, this</span>
<span id="cb8-593"><a href="#cb8-593" aria-hidden="true" tabindex="-1"></a>criticism is nonsensical as they can both be given and serve different</span>
<span id="cb8-594"><a href="#cb8-594" aria-hidden="true" tabindex="-1"></a>purposes.</span>
<span id="cb8-595"><a href="#cb8-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-596"><a href="#cb8-596" aria-hidden="true" tabindex="-1"></a>A $P$-value for a particular parameter value gives the compatibility</span>
<span id="cb8-597"><a href="#cb8-597" aria-hidden="true" tabindex="-1"></a>between the test model in question, which will vary from one parameter</span>
<span id="cb8-598"><a href="#cb8-598" aria-hidden="true" tabindex="-1"></a>value to the next, and the data. An interval estimate such as a 95%</span>
<span id="cb8-599"><a href="#cb8-599" aria-hidden="true" tabindex="-1"></a>frequentist interval simply gives the region of parameter values with</span>
<span id="cb8-600"><a href="#cb8-600" aria-hidden="true" tabindex="-1"></a>$P$-values above the corresponding $\alpha$ level, and which are more</span>
<span id="cb8-601"><a href="#cb8-601" aria-hidden="true" tabindex="-1"></a>consistent with the data than the parameter values outside the interval</span>
<span id="cb8-602"><a href="#cb8-602" aria-hidden="true" tabindex="-1"></a>limits. An interval estimate by itself does not explicitly tell one how</span>
<span id="cb8-603"><a href="#cb8-603" aria-hidden="true" tabindex="-1"></a>consistent a parameter value is with the data, which the $P$-value does.</span>
<span id="cb8-604"><a href="#cb8-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-605"><a href="#cb8-605" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-606"><a href="#cb8-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-607"><a href="#cb8-607" aria-hidden="true" tabindex="-1"></a><span class="fu">## Overstating the Evidence</span></span>
<span id="cb8-608"><a href="#cb8-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-609"><a href="#cb8-609" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-610"><a href="#cb8-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-611"><a href="#cb8-611" aria-hidden="true" tabindex="-1"></a>$P$-values are routinely criticized for overstating the amount of</span>
<span id="cb8-612"><a href="#cb8-612" aria-hidden="true" tabindex="-1"></a>evidence from a study. Such statements are also often given using</span>
<span id="cb8-613"><a href="#cb8-613" aria-hidden="true" tabindex="-1"></a>Bayesian arguments, of which many are skeptical. However, the $P$-value</span>
<span id="cb8-614"><a href="#cb8-614" aria-hidden="true" tabindex="-1"></a>cannot overstate evidence as it is simply providing the location at</span>
<span id="cb8-615"><a href="#cb8-615" aria-hidden="true" tabindex="-1"></a>which the test statistic fell in the expected distribution, given that</span>
<span id="cb8-616"><a href="#cb8-616" aria-hidden="true" tabindex="-1"></a>every model assumption were true. It is simply indicative of how</span>
<span id="cb8-617"><a href="#cb8-617" aria-hidden="true" tabindex="-1"></a>surprising/extreme the observed result was, given certain assumptions.</span>
<span id="cb8-618"><a href="#cb8-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-619"><a href="#cb8-619" aria-hidden="true" tabindex="-1"></a>Any overstating of evidence, is not an issue of the statistic itself,</span>
<span id="cb8-620"><a href="#cb8-620" aria-hidden="true" tabindex="-1"></a>but rather users. If we treat the $P-$ value as nothing more or less</span>
<span id="cb8-621"><a href="#cb8-621" aria-hidden="true" tabindex="-1"></a>than a continuous measure of compatibility of the observed data with the</span>
<span id="cb8-622"><a href="#cb8-622" aria-hidden="true" tabindex="-1"></a>model used to compute it (observed $p$) given certain model assumptions,</span>
<span id="cb8-623"><a href="#cb8-623" aria-hidden="true" tabindex="-1"></a>we won't run into some of the common misinterpretations such as "the</span>
<span id="cb8-624"><a href="#cb8-624" aria-hidden="true" tabindex="-1"></a>$P$-value is the probability of a hypothesis", or the "probability of</span>
<span id="cb8-625"><a href="#cb8-625" aria-hidden="true" tabindex="-1"></a>chance alone", or "the probability of being</span>
<span id="cb8-626"><a href="#cb8-626" aria-hidden="true" tabindex="-1"></a>incorrect".<span class="sc">\[</span>@greenlandStatisticalTestsValues2016<span class="sc">\]</span></span>
<span id="cb8-627"><a href="#cb8-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-628"><a href="#cb8-628" aria-hidden="true" tabindex="-1"></a>Indeed, many of the "problems" commonly associated with the $P$-value</span>
<span id="cb8-629"><a href="#cb8-629" aria-hidden="true" tabindex="-1"></a>are not due to the actual statistic itself, but rather researchers'</span>
<span id="cb8-630"><a href="#cb8-630" aria-hidden="true" tabindex="-1"></a>misinterpretations of what it is and what it means for a study.</span>
<span id="cb8-631"><a href="#cb8-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-632"><a href="#cb8-632" aria-hidden="true" tabindex="-1"></a>The answer to these misconceptions may be compatibilism, with less</span>
<span id="cb8-633"><a href="#cb8-633" aria-hidden="true" tabindex="-1"></a>compatibility (smaller $P$-values) indicating a poor fit between the</span>
<span id="cb8-634"><a href="#cb8-634" aria-hidden="true" tabindex="-1"></a>data and the test model and hence more evidence against the test</span>
<span id="cb8-635"><a href="#cb8-635" aria-hidden="true" tabindex="-1"></a>hypothesis.</span>
<span id="cb8-636"><a href="#cb8-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-637"><a href="#cb8-637" aria-hidden="true" tabindex="-1"></a>A $P$-value of 0.04 means that assuming that **all** the assumptions of</span>
<span id="cb8-638"><a href="#cb8-638" aria-hidden="true" tabindex="-1"></a>the model used to compute the $P$-value are correct, we won't get data</span>
<span id="cb8-639"><a href="#cb8-639" aria-hidden="true" tabindex="-1"></a>(a test statistic) at least as extreme as what was observed by random</span>
<span id="cb8-640"><a href="#cb8-640" aria-hidden="true" tabindex="-1"></a>variation more than 4% of the time.</span>
<span id="cb8-641"><a href="#cb8-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-642"><a href="#cb8-642" aria-hidden="true" tabindex="-1"></a>To many, such low compatibility between the data and the model may lead</span>
<span id="cb8-643"><a href="#cb8-643" aria-hidden="true" tabindex="-1"></a>them to reject the test hypothesis (the null hypothesis).</span>
<span id="cb8-644"><a href="#cb8-644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-645"><a href="#cb8-645" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-646"><a href="#cb8-646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-647"><a href="#cb8-647" aria-hidden="true" tabindex="-1"></a><span class="fu"># Some Valid Issues</span></span>
<span id="cb8-648"><a href="#cb8-648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-649"><a href="#cb8-649" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-650"><a href="#cb8-650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-651"><a href="#cb8-651" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mismatch With Direction</span></span>
<span id="cb8-652"><a href="#cb8-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-653"><a href="#cb8-653" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-654"><a href="#cb8-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-655"><a href="#cb8-655" aria-hidden="true" tabindex="-1"></a>If you recall from above, I wrote that the $P$-value is seen by many as</span>
<span id="cb8-656"><a href="#cb8-656" aria-hidden="true" tabindex="-1"></a>being a continuous measure of evidence against the test hypothesis and</span>
<span id="cb8-657"><a href="#cb8-657" aria-hidden="true" tabindex="-1"></a>model. Technically speaking, it would be incorrect to define it this way</span>
<span id="cb8-658"><a href="#cb8-658" aria-hidden="true" tabindex="-1"></a>because as the $P$-value goes up (with the highest value being 1 or</span>
<span id="cb8-659"><a href="#cb8-659" aria-hidden="true" tabindex="-1"></a>100%), there is **less** evidence against the test hypothesis since the</span>
<span id="cb8-660"><a href="#cb8-660" aria-hidden="true" tabindex="-1"></a>data are **more compatible** with the test model. 1 = perfect</span>
<span id="cb8-661"><a href="#cb8-661" aria-hidden="true" tabindex="-1"></a>compatibility of the data with the test model.</span>
<span id="cb8-662"><a href="#cb8-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-663"><a href="#cb8-663" aria-hidden="true" tabindex="-1"></a>As the $P$-value gets lower (with the lowest value being 0), there is</span>
<span id="cb8-664"><a href="#cb8-664" aria-hidden="true" tabindex="-1"></a>**less compatibility** between the data and the model, hence **more**</span>
<span id="cb8-665"><a href="#cb8-665" aria-hidden="true" tabindex="-1"></a>evidence against the test hypothesis used to compute $p$.</span>
<span id="cb8-666"><a href="#cb8-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-667"><a href="#cb8-667" aria-hidden="true" tabindex="-1"></a>Thus, saying that $P$-values are measures of evidence against the</span>
<span id="cb8-668"><a href="#cb8-668" aria-hidden="true" tabindex="-1"></a>hypothesis used to compute them is a backward definition. This</span>
<span id="cb8-669"><a href="#cb8-669" aria-hidden="true" tabindex="-1"></a>definition would be correct if higher $P$-values inferred more evidence</span>
<span id="cb8-670"><a href="#cb8-670" aria-hidden="true" tabindex="-1"></a>against the test hypothesis and vice versa.</span>
<span id="cb8-671"><a href="#cb8-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-672"><a href="#cb8-672" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-673"><a href="#cb8-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-674"><a href="#cb8-674" aria-hidden="true" tabindex="-1"></a><span class="fu">## Difficulties Due to Scale</span></span>
<span id="cb8-675"><a href="#cb8-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-676"><a href="#cb8-676" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-677"><a href="#cb8-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-678"><a href="#cb8-678" aria-hidden="true" tabindex="-1"></a>Another problem with $P$-values and their interpretation is scaling.</span>
<span id="cb8-679"><a href="#cb8-679" aria-hidden="true" tabindex="-1"></a>Since the statistic is meant to be a continuous measure of compatibility</span>
<span id="cb8-680"><a href="#cb8-680" aria-hidden="true" tabindex="-1"></a>(and relative evidence against the test model + hypothesis), we would</span>
<span id="cb8-681"><a href="#cb8-681" aria-hidden="true" tabindex="-1"></a>hope that differences between $P$-values would be equal (on an additive</span>
<span id="cb8-682"><a href="#cb8-682" aria-hidden="true" tabindex="-1"></a>scale), as this makes it easier to interpret.</span>
<span id="cb8-683"><a href="#cb8-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-684"><a href="#cb8-684" aria-hidden="true" tabindex="-1"></a>For example, the difference between 0 and 10 dollars is the same as the</span>
<span id="cb8-685"><a href="#cb8-685" aria-hidden="true" tabindex="-1"></a>difference between 90 and 100 dollars, in that both are a difference of</span>
<span id="cb8-686"><a href="#cb8-686" aria-hidden="true" tabindex="-1"></a>10 dollars. And this property remains consistent across various</span>
<span id="cb8-687"><a href="#cb8-687" aria-hidden="true" tabindex="-1"></a>intervals, 120 and 130, 1,000,000 and 1,000,010.</span>
<span id="cb8-688"><a href="#cb8-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-689"><a href="#cb8-689" aria-hidden="true" tabindex="-1"></a>Unfortunately, this doesn't apply to the $P$-value because it is on the</span>
<span id="cb8-690"><a href="#cb8-690" aria-hidden="true" tabindex="-1"></a>inverse-exponential scale. The difference between a $P$-value of 0.01</span>
<span id="cb8-691"><a href="#cb8-691" aria-hidden="true" tabindex="-1"></a>and 0.10 is not the same as the difference between 0.90 and 0.99.</span>
<span id="cb8-692"><a href="#cb8-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-693"><a href="#cb8-693" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-694"><a href="#cb8-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-695"><a href="#cb8-695" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb8-696"><a href="#cb8-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-697"><a href="#cb8-697" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">img</span><span class="ot"> src</span><span class="op">=</span><span class="st">"/images/norm.svg"</span><span class="ot"> alt</span><span class="op">=</span><span class="st">"Gaussian distribution"</span><span class="ot"> width</span><span class="op">=</span><span class="st">"680"</span><span class="ot"> style</span><span class="op">=</span><span class="st">"cursor: zoom-in"</span><span class="dt">/&gt;</span></span>
<span id="cb8-698"><a href="#cb8-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-699"><a href="#cb8-699" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;</span><span class="kw">figcaption</span><span class="dt">&gt;</span></span>
<span id="cb8-700"><a href="#cb8-700" aria-hidden="true" tabindex="-1"></a>A gaussian probability densitiy with the standard deviations</span>
<span id="cb8-701"><a href="#cb8-701" aria-hidden="true" tabindex="-1"></a>annotated. Data points further away from the mean, are more extreme and</span>
<span id="cb8-702"><a href="#cb8-702" aria-hidden="true" tabindex="-1"></a>unlikely events. I also must admit that this is one of my favorite</span>
<span id="cb8-703"><a href="#cb8-703" aria-hidden="true" tabindex="-1"></a>figures of a gaussian distribution.</span>
<span id="cb8-704"><a href="#cb8-704" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">figcaption</span><span class="dt">&gt;</span></span>
<span id="cb8-705"><a href="#cb8-705" aria-hidden="true" tabindex="-1"></a><span class="dt">&lt;/</span><span class="kw">figure</span><span class="dt">&gt;</span></span>
<span id="cb8-706"><a href="#cb8-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-707"><a href="#cb8-707" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-708"><a href="#cb8-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-709"><a href="#cb8-709" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-710"><a href="#cb8-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-711"><a href="#cb8-711" aria-hidden="true" tabindex="-1"></a>For example, with a normal distribution (above), a z-score of 0 results</span>
<span id="cb8-712"><a href="#cb8-712" aria-hidden="true" tabindex="-1"></a>in a $P$-value of 1 (perfect compatibility). If we now move to a z-score</span>
<span id="cb8-713"><a href="#cb8-713" aria-hidden="true" tabindex="-1"></a>of 1, the $P$-value is 0.31. Thus, we saw a dramatic decrease from a</span>
<span id="cb8-714"><a href="#cb8-714" aria-hidden="true" tabindex="-1"></a>$P$-value of 1 to 0.31 with one z-score. A 0.69 decrease in the</span>
<span id="cb8-715"><a href="#cb8-715" aria-hidden="true" tabindex="-1"></a>$P$-value.</span>
<span id="cb8-716"><a href="#cb8-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-717"><a href="#cb8-717" aria-hidden="true" tabindex="-1"></a>Now let's move from a z-score of 1 to a z-score of 2. We saw a decrease</span>
<span id="cb8-718"><a href="#cb8-718" aria-hidden="true" tabindex="-1"></a>of 0.69 with the change in **one** z-score before, so the new $P$-value</span>
<span id="cb8-719"><a href="#cb8-719" aria-hidden="true" tabindex="-1"></a>must be 0.31 - 0.69 = -0.38 right? **No**. The $P$-value for a z-score</span>
<span id="cb8-720"><a href="#cb8-720" aria-hidden="true" tabindex="-1"></a>of 2 is 0.045. The $P$-value for a z-score of 3 is 0.003. Even though</span>
<span id="cb8-721"><a href="#cb8-721" aria-hidden="true" tabindex="-1"></a>we've only been moving by **one** z-score at a time, the changes in</span>
<span id="cb8-722"><a href="#cb8-722" aria-hidden="true" tabindex="-1"></a>$P$-values don't remain constant; the decreases become larger and</span>
<span id="cb8-723"><a href="#cb8-723" aria-hidden="true" tabindex="-1"></a>larger.</span>
<span id="cb8-724"><a href="#cb8-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-725"><a href="#cb8-725" aria-hidden="true" tabindex="-1"></a>Thus, the difference between the $P$-values of 0.01 and 0.10, in terms</span>
<span id="cb8-726"><a href="#cb8-726" aria-hidden="true" tabindex="-1"></a>of z-score, is substantially larger than the difference between 0.90 and</span>
<span id="cb8-727"><a href="#cb8-727" aria-hidden="true" tabindex="-1"></a>0.99. Again, this makes it difficult to interpret as a statistic across</span>
<span id="cb8-728"><a href="#cb8-728" aria-hidden="true" tabindex="-1"></a>the board, especially as a continuous measure. This can further be seen</span>
<span id="cb8-729"><a href="#cb8-729" aria-hidden="true" tabindex="-1"></a>in the figure from [Rafi &amp; Greenland</span>
<span id="cb8-730"><a href="#cb8-730" aria-hidden="true" tabindex="-1"></a>(2020)](https://doi.org/10.1186/s12874-020-01105-9).</span>
<span id="cb8-731"><a href="#cb8-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-732"><a href="#cb8-732" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>     <span class="pp">|</span></span>
<span id="cb8-733"><a href="#cb8-733" aria-hidden="true" tabindex="-1"></a><span class="pp">|:----|</span></span>
<span id="cb8-734"><a href="#cb8-734" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>     <span class="pp">|</span></span>
<span id="cb8-735"><a href="#cb8-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-736"><a href="#cb8-736" aria-hidden="true" tabindex="-1"></a><span class="fu"># Resolution with Surprisals</span></span>
<span id="cb8-737"><a href="#cb8-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-738"><a href="#cb8-738" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-739"><a href="#cb8-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-740"><a href="#cb8-740" aria-hidden="true" tabindex="-1"></a>The issues described above such as the backward definition and the</span>
<span id="cb8-741"><a href="#cb8-741" aria-hidden="true" tabindex="-1"></a>problem of scaling can make it difficult to conceptualize the $P$-value</span>
<span id="cb8-742"><a href="#cb8-742" aria-hidden="true" tabindex="-1"></a>as being an evidence measure against the test hypothesis and test model.</span>
<span id="cb8-743"><a href="#cb8-743" aria-hidden="true" tabindex="-1"></a>However, these issues can be addressed by taking the negative log of the</span>
<span id="cb8-744"><a href="#cb8-744" aria-hidden="true" tabindex="-1"></a>$P$-value $–\log_{2}(p)$ , which yields something known as the Shannon</span>
<span id="cb8-745"><a href="#cb8-745" aria-hidden="true" tabindex="-1"></a>information value or *surprisal (*$s$)</span>
<span id="cb8-746"><a href="#cb8-746" aria-hidden="true" tabindex="-1"></a>value,<span class="sc">\[</span>@rafiSemanticCognitiveTools2020; @coleSurprise2020;</span>
<span id="cb8-747"><a href="#cb8-747" aria-hidden="true" tabindex="-1"></a>@greenlandValidPvaluesBehave2019<span class="sc">\]</span> named after [Claude</span>
<span id="cb8-748"><a href="#cb8-748" aria-hidden="true" tabindex="-1"></a>Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), the father of</span>
<span id="cb8-749"><a href="#cb8-749" aria-hidden="true" tabindex="-1"></a>information theory.<span class="sc">\[</span>@Shannon1948-uq<span class="sc">\]</span></span>
<span id="cb8-750"><a href="#cb8-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-751"><a href="#cb8-751" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>     <span class="pp">|</span></span>
<span id="cb8-752"><a href="#cb8-752" aria-hidden="true" tabindex="-1"></a><span class="pp">|:----|</span></span>
<span id="cb8-753"><a href="#cb8-753" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>     <span class="pp">|</span></span>
<span id="cb8-754"><a href="#cb8-754" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-755"><a href="#cb8-755" aria-hidden="true" tabindex="-1"></a>Unlike the $P$-value, this value is not a probability but a continuous</span>
<span id="cb8-756"><a href="#cb8-756" aria-hidden="true" tabindex="-1"></a>measure of *information* in **bits** of information against the test</span>
<span id="cb8-757"><a href="#cb8-757" aria-hidden="true" tabindex="-1"></a>hypothesis and is taken from the observed test statistic computed by the</span>
<span id="cb8-758"><a href="#cb8-758" aria-hidden="true" tabindex="-1"></a>test model.</span>
<span id="cb8-759"><a href="#cb8-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-760"><a href="#cb8-760" aria-hidden="true" tabindex="-1"></a>It also provides a more intuitive way to think about $P$-values. Imagine</span>
<span id="cb8-761"><a href="#cb8-761" aria-hidden="true" tabindex="-1"></a>that the variable $k$ is always the nearest integer to the calculated</span>
<span id="cb8-762"><a href="#cb8-762" aria-hidden="true" tabindex="-1"></a>value of $s$. Now, take for example a $P$-value of 0.05, the $S$-value</span>
<span id="cb8-763"><a href="#cb8-763" aria-hidden="true" tabindex="-1"></a>for this would be $s$ = $–\log_{2}(0.05)$ which equals 4.3 bits of</span>
<span id="cb8-764"><a href="#cb8-764" aria-hidden="true" tabindex="-1"></a>information embedded in the test statistic, which can be implied as</span>
<span id="cb8-765"><a href="#cb8-765" aria-hidden="true" tabindex="-1"></a>evidence against the test hypothesis.</span>
<span id="cb8-766"><a href="#cb8-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-767"><a href="#cb8-767" aria-hidden="true" tabindex="-1"></a>How much evidence is this? $k$ can help us think about this. The nearest</span>
<span id="cb8-768"><a href="#cb8-768" aria-hidden="true" tabindex="-1"></a>integer to 4.3 is 4. Thus, the data which yield a $P$-value of 0.05</span>
<span id="cb8-769"><a href="#cb8-769" aria-hidden="true" tabindex="-1"></a>which results in an $s$ value of 4.3 bits of information is **no more</span>
<span id="cb8-770"><a href="#cb8-770" aria-hidden="true" tabindex="-1"></a>surprising** than getting **all heads** on 4 fair coin tosses.</span>
<span id="cb8-771"><a href="#cb8-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-772"><a href="#cb8-772" aria-hidden="true" tabindex="-1"></a>Another example. Let's say our study gives us a $P$-value of 0.005,</span>
<span id="cb8-773"><a href="#cb8-773" aria-hidden="true" tabindex="-1"></a>which would indicate to many very low compatibility between the test</span>
<span id="cb8-774"><a href="#cb8-774" aria-hidden="true" tabindex="-1"></a>model and the observed data; this would yield an $s$ value of</span>
<span id="cb8-775"><a href="#cb8-775" aria-hidden="true" tabindex="-1"></a>$–\log_{2}(0.005) = 7.6$ bits of information. $k$ which is the closest</span>
<span id="cb8-776"><a href="#cb8-776" aria-hidden="true" tabindex="-1"></a>integer to $s$ would be 8. Thus, these data which yield a $P$-value of</span>
<span id="cb8-777"><a href="#cb8-777" aria-hidden="true" tabindex="-1"></a>0.005 are no more surprising than getting all heads on 8 fair coin</span>
<span id="cb8-778"><a href="#cb8-778" aria-hidden="true" tabindex="-1"></a>tosses.</span>
<span id="cb8-779"><a href="#cb8-779" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-780"><a href="#cb8-780" aria-hidden="true" tabindex="-1"></a>A table of various $P$-values and their corresponding $S$-values,</span>
<span id="cb8-781"><a href="#cb8-781" aria-hidden="true" tabindex="-1"></a>maximum-likelihood ratios, and likelihood-ratio statistics can be found</span>
<span id="cb8-782"><a href="#cb8-782" aria-hidden="true" tabindex="-1"></a>below from [Rafi &amp; Greenland</span>
<span id="cb8-783"><a href="#cb8-783" aria-hidden="true" tabindex="-1"></a>(2020)](https://doi.org/10.1186/s12874-020-01105-9), which includes the</span>
<span id="cb8-784"><a href="#cb8-784" aria-hidden="true" tabindex="-1"></a>general cutoffs used in different scientific fields such as high-energy</span>
<span id="cb8-785"><a href="#cb8-785" aria-hidden="true" tabindex="-1"></a>physics and genome-wide association studies. It also shows how the</span>
<span id="cb8-786"><a href="#cb8-786" aria-hidden="true" tabindex="-1"></a>traditional cutoffs used in these fields can be problematic.</span>
<span id="cb8-787"><a href="#cb8-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-788"><a href="#cb8-788" aria-hidden="true" tabindex="-1"></a>For example, an $\alpha$ of 0.05, which only corresponds to seeing all</span>
<span id="cb8-789"><a href="#cb8-789" aria-hidden="true" tabindex="-1"></a>heads on 4 fair coin tosses, is practically nothing when compared to the</span>
<span id="cb8-790"><a href="#cb8-790" aria-hidden="true" tabindex="-1"></a>cutoffs used in particle physics and GWAS, which correspond to seeing</span>
<span id="cb8-791"><a href="#cb8-791" aria-hidden="true" tabindex="-1"></a>all heads on 22 and 30 fair coin tosses, respectively.</span>
<span id="cb8-792"><a href="#cb8-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-793"><a href="#cb8-793" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-794"><a href="#cb8-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-795"><a href="#cb8-795" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> P-value (compatibility) <span class="pp">|</span> S-value (bits) <span class="pp">|</span> Maximum Likelihood Ratio <span class="pp">|</span> Deviance Statistic 2ln(MLR) <span class="pp">|</span></span>
<span id="cb8-796"><a href="#cb8-796" aria-hidden="true" tabindex="-1"></a><span class="pp">|:---|---:|---:|---:|</span></span>
<span id="cb8-797"><a href="#cb8-797" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 0.99 <span class="pp">|</span> 0.01 <span class="pp">|</span> 1.00e+00 <span class="pp">|</span> 0.00 <span class="pp">|</span></span>
<span id="cb8-798"><a href="#cb8-798" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 0.9 <span class="pp">|</span> 0.15 <span class="pp">|</span> 1.01e+00 <span class="pp">|</span> 0.02 <span class="pp">|</span></span>
<span id="cb8-799"><a href="#cb8-799" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 0.5 <span class="pp">|</span> 1.00 <span class="pp">|</span> 1.26e+00 <span class="pp">|</span> 0.45 <span class="pp">|</span></span>
<span id="cb8-800"><a href="#cb8-800" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 0.25 <span class="pp">|</span> 2.00 <span class="pp">|</span> 1.94e+00 <span class="pp">|</span> 1.32 <span class="pp">|</span></span>
<span id="cb8-801"><a href="#cb8-801" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 0.1 <span class="pp">|</span> 3.32 <span class="pp">|</span> 3.87e+00 <span class="pp">|</span> 2.71 <span class="pp">|</span></span>
<span id="cb8-802"><a href="#cb8-802" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 0.05 <span class="pp">|</span> 4.32 <span class="pp">|</span> 6.83e+00 <span class="pp">|</span> 3.84 <span class="pp">|</span></span>
<span id="cb8-803"><a href="#cb8-803" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 0.025 <span class="pp">|</span> 5.32 <span class="pp">|</span> 1.23e+01 <span class="pp">|</span> 5.02 <span class="pp">|</span></span>
<span id="cb8-804"><a href="#cb8-804" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 0.01 <span class="pp">|</span> 6.64 <span class="pp">|</span> 2.76e+01 <span class="pp">|</span> 6.63 <span class="pp">|</span></span>
<span id="cb8-805"><a href="#cb8-805" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 0.005 <span class="pp">|</span> 7.64 <span class="pp">|</span> 5.14e+01 <span class="pp">|</span> 7.88 <span class="pp">|</span></span>
<span id="cb8-806"><a href="#cb8-806" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 1e-04 <span class="pp">|</span> 13.29 <span class="pp">|</span> 1.94e+03 <span class="pp">|</span> 15.10 <span class="pp">|</span></span>
<span id="cb8-807"><a href="#cb8-807" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 5 sigma (~ 2.9 in 10 million) <span class="pp">|</span> 21.70 <span class="pp">|</span> 5.20e+05 <span class="pp">|</span> 26.30 <span class="pp">|</span></span>
<span id="cb8-808"><a href="#cb8-808" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 1 in 100 million (GWAS) <span class="pp">|</span> 26.60 <span class="pp">|</span> 1.40e+07 <span class="pp">|</span> 32.80 <span class="pp">|</span></span>
<span id="cb8-809"><a href="#cb8-809" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> 6 sigma (~ 1 in a billion) <span class="pp">|</span> 29.90 <span class="pp">|</span> 1.30e+08 <span class="pp">|</span> 37.40 <span class="pp">|</span></span>
<span id="cb8-810"><a href="#cb8-810" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span> <span class="dt">&lt;</span><span class="kw">span</span><span class="ot"> style</span><span class="op">=</span><span class="st">"text-decoration: underline;"</span><span class="dt">&gt;</span>Abbreviations: <span class="dt">&lt;/</span><span class="kw">span</span><span class="dt">&gt;</span> <span class="pp">|</span>  <span class="pp">|</span>  <span class="pp">|</span>  <span class="pp">|</span></span>
<span id="cb8-811"><a href="#cb8-811" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>  Table 1: \$P\$-values and binary \$S\$-values, with corresponding maximum-likelihood ratios (MLR) and deviance (likelihood-ratio) statistics for a simple test hypothesis H under background assumptions A <span class="pp">|</span>  <span class="pp">|</span>  <span class="pp">|</span>  <span class="pp">|</span></span>
<span id="cb8-812"><a href="#cb8-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-813"><a href="#cb8-813" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-814"><a href="#cb8-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-815"><a href="#cb8-815" aria-hidden="true" tabindex="-1"></a>Unlike the $P$-value, the $S$-value is more intuitive as a measure of</span>
<span id="cb8-816"><a href="#cb8-816" aria-hidden="true" tabindex="-1"></a>refutational evidence against the test hypothesis since its value (bits</span>
<span id="cb8-817"><a href="#cb8-817" aria-hidden="true" tabindex="-1"></a>of information against the test hypothesis) increases with less</span>
<span id="cb8-818"><a href="#cb8-818" aria-hidden="true" tabindex="-1"></a>compatibility, whereas the opposite is true for the $P$-value.</span>
<span id="cb8-819"><a href="#cb8-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-820"><a href="#cb8-820" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-821"><a href="#cb8-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-822"><a href="#cb8-822" aria-hidden="true" tabindex="-1"></a><span class="fu">### Some Examples</span></span>
<span id="cb8-823"><a href="#cb8-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-824"><a href="#cb8-824" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-825"><a href="#cb8-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-826"><a href="#cb8-826" aria-hidden="true" tabindex="-1"></a>Let's try using some data to see this in action. I'll take a sample</span>
<span id="cb8-827"><a href="#cb8-827" aria-hidden="true" tabindex="-1"></a>experimental dataset from <span class="in">`R`</span> on the effects of different conditions on</span>
<span id="cb8-828"><a href="#cb8-828" aria-hidden="true" tabindex="-1"></a>dried plant weight. We can plot the data and run a one-way ANOVA.</span>
<span id="cb8-829"><a href="#cb8-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-830"><a href="#cb8-830" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-831"><a href="#cb8-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-832"><a href="#cb8-832" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb8-833"><a href="#cb8-833" aria-hidden="true" tabindex="-1"></a>pg <span class="ot">&lt;-</span> <span class="fu">force</span>(PlantGrowth)</span>
<span id="cb8-834"><a href="#cb8-834" aria-hidden="true" tabindex="-1"></a>(Hmisc<span class="sc">::</span><span class="fu">describe</span>(pg))</span>
<span id="cb8-835"><a href="#cb8-835" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; pg </span></span>
<span id="cb8-836"><a href="#cb8-836" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb8-837"><a href="#cb8-837" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  2  Variables      30  Observations</span></span>
<span id="cb8-838"><a href="#cb8-838" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; --------------------------------------------------------------------------------</span></span>
<span id="cb8-839"><a href="#cb8-839" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; weight </span></span>
<span id="cb8-840"><a href="#cb8-840" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;        n  missing distinct     Info     Mean  pMedian      Gmd      .05 </span></span>
<span id="cb8-841"><a href="#cb8-841" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       30        0       29        1    5.073     5.09   0.8131    3.983 </span></span>
<span id="cb8-842"><a href="#cb8-842" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      .10      .25      .50      .75      .90      .95 </span></span>
<span id="cb8-843"><a href="#cb8-843" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    4.170    4.550    5.155    5.530    6.038    6.132 </span></span>
<span id="cb8-844"><a href="#cb8-844" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb8-845"><a href="#cb8-845" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lowest : 3.59 3.83 4.17 4.32 4.41, highest: 5.87 6.03 6.11 6.15 6.31</span></span>
<span id="cb8-846"><a href="#cb8-846" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; --------------------------------------------------------------------------------</span></span>
<span id="cb8-847"><a href="#cb8-847" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; group </span></span>
<span id="cb8-848"><a href="#cb8-848" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;        n  missing distinct </span></span>
<span id="cb8-849"><a href="#cb8-849" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;       30        0        3 </span></span>
<span id="cb8-850"><a href="#cb8-850" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                             </span></span>
<span id="cb8-851"><a href="#cb8-851" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Value       ctrl  trt1  trt2</span></span>
<span id="cb8-852"><a href="#cb8-852" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Frequency     10    10    10</span></span>
<span id="cb8-853"><a href="#cb8-853" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Proportion 0.333 0.333 0.333</span></span>
<span id="cb8-854"><a href="#cb8-854" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; --------------------------------------------------------------------------------</span></span>
<span id="cb8-855"><a href="#cb8-855" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-856"><a href="#cb8-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-857"><a href="#cb8-857" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-858"><a href="#cb8-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-859"><a href="#cb8-859" aria-hidden="true" tabindex="-1"></a>Looks interesting. We can see some differences from the graph. Here's</span>
<span id="cb8-860"><a href="#cb8-860" aria-hidden="true" tabindex="-1"></a>what our test output gives us,</span>
<span id="cb8-861"><a href="#cb8-861" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-862"><a href="#cb8-862" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-863"><a href="#cb8-863" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-864"><a href="#cb8-864" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb8-865"><a href="#cb8-865" aria-hidden="true" tabindex="-1"></a>res <span class="ot">&lt;-</span> <span class="fu">anova</span>(<span class="fu">lm</span>(weight <span class="sc">~</span> group, <span class="at">data =</span> pg))</span>
<span id="cb8-866"><a href="#cb8-866" aria-hidden="true" tabindex="-1"></a><span class="fu">ztable</span>(res)</span>
<span id="cb8-867"><a href="#cb8-867" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Error in ztable(res): could not find function "ztable"</span></span>
<span id="cb8-868"><a href="#cb8-868" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-869"><a href="#cb8-869" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-870"><a href="#cb8-870" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-871"><a href="#cb8-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-872"><a href="#cb8-872" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb8-873"><a href="#cb8-873" aria-hidden="true" tabindex="-1"></a>(obs_p <span class="ot">&lt;-</span> res[<span class="dv">1</span>, <span class="dv">5</span>])</span>
<span id="cb8-874"><a href="#cb8-874" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.01590996</span></span>
<span id="cb8-875"><a href="#cb8-875" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-876"><a href="#cb8-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-877"><a href="#cb8-877" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-878"><a href="#cb8-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-879"><a href="#cb8-879" aria-hidden="true" tabindex="-1"></a>If we had set our $\alpha$ to the traditional 0.05 level before the</span>
<span id="cb8-880"><a href="#cb8-880" aria-hidden="true" tabindex="-1"></a>experiment, we can reject the test hypothesis (the null hypothesis), but</span>
<span id="cb8-881"><a href="#cb8-881" aria-hidden="true" tabindex="-1"></a>that is not as interesting from a continuous evidential perspective. How</span>
<span id="cb8-882"><a href="#cb8-882" aria-hidden="true" tabindex="-1"></a>can I interpret this $P$-value of 0.0159 more intuitively?</span>
<span id="cb8-883"><a href="#cb8-883" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-884"><a href="#cb8-884" aria-hidden="true" tabindex="-1"></a>Let's convert it into an $S$-value.</span>
<span id="cb8-885"><a href="#cb8-885" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-886"><a href="#cb8-886" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-887"><a href="#cb8-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-888"><a href="#cb8-888" aria-hidden="true" tabindex="-1"></a><span class="in">``` r</span></span>
<span id="cb8-889"><a href="#cb8-889" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fu">log2</span>(obs_p)</span>
<span id="cb8-890"><a href="#cb8-890" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 5.97</span></span>
<span id="cb8-891"><a href="#cb8-891" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb8-892"><a href="#cb8-892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-893"><a href="#cb8-893" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-894"><a href="#cb8-894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-895"><a href="#cb8-895" aria-hidden="true" tabindex="-1"></a>$$–\log_2(0.0159) = 5.97$$</span>
<span id="cb8-896"><a href="#cb8-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-897"><a href="#cb8-897" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-898"><a href="#cb8-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-899"><a href="#cb8-899" aria-hidden="true" tabindex="-1"></a>$$s= 5.97$$</span>
<span id="cb8-900"><a href="#cb8-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-901"><a href="#cb8-901" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-902"><a href="#cb8-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-903"><a href="#cb8-903" aria-hidden="true" tabindex="-1"></a>That is 5.97 bits of information against the null hypothesis.</span>
<span id="cb8-904"><a href="#cb8-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-905"><a href="#cb8-905" aria-hidden="true" tabindex="-1"></a>Remember, $k$ is the nearest integer to the calculated value of $s$ and</span>
<span id="cb8-906"><a href="#cb8-906" aria-hidden="true" tabindex="-1"></a>in this case, would be 6. So these results (the test statistic,</span>
<span id="cb8-907"><a href="#cb8-907" aria-hidden="true" tabindex="-1"></a>$F$(4.85)) are as surprising as getting all heads on 6 fair coin tosses.</span>
<span id="cb8-908"><a href="#cb8-908" aria-hidden="true" tabindex="-1"></a>Somewhat surprising, depending on the individual interpreting the</span>
<span id="cb8-909"><a href="#cb8-909" aria-hidden="true" tabindex="-1"></a>results.</span>
<span id="cb8-910"><a href="#cb8-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-911"><a href="#cb8-911" aria-hidden="true" tabindex="-1"></a>How would we interpret it within the context of a given confidence</span>
<span id="cb8-912"><a href="#cb8-912" aria-hidden="true" tabindex="-1"></a>interval? The $S$-value tells us that values within the computed 95% CI:</span>
<span id="cb8-913"><a href="#cb8-913" aria-hidden="true" tabindex="-1"></a>have at most 4.3 bits of information against them. That is because all</span>
<span id="cb8-914"><a href="#cb8-914" aria-hidden="true" tabindex="-1"></a>parameter values within a 95% CI have $P$-values greater than 0.05.</span>
<span id="cb8-915"><a href="#cb8-915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-916"><a href="#cb8-916" aria-hidden="true" tabindex="-1"></a>So those parameter values that are inside the 95% interval estimate have</span>
<span id="cb8-917"><a href="#cb8-917" aria-hidden="true" tabindex="-1"></a>less bits of information against them, than the parameter values that go</span>
<span id="cb8-918"><a href="#cb8-918" aria-hidden="true" tabindex="-1"></a>further and further away from the center of the 95% interval estimate.</span>
<span id="cb8-919"><a href="#cb8-919" aria-hidden="true" tabindex="-1"></a>The point estimate is the most compatible with the data (meaning it has</span>
<span id="cb8-920"><a href="#cb8-920" aria-hidden="true" tabindex="-1"></a>the least refutational information against it), while those values near</span>
<span id="cb8-921"><a href="#cb8-921" aria-hidden="true" tabindex="-1"></a>the limits have more information against them.</span>
<span id="cb8-922"><a href="#cb8-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-923"><a href="#cb8-923" aria-hidden="true" tabindex="-1"></a>In other words, as values head in the directions outside the interval,</span>
<span id="cb8-924"><a href="#cb8-924" aria-hidden="true" tabindex="-1"></a>there is more refutational information against them, as depicted by the</span>
<span id="cb8-925"><a href="#cb8-925" aria-hidden="true" tabindex="-1"></a>following function from [Rafi &amp; Greenland,</span>
<span id="cb8-926"><a href="#cb8-926" aria-hidden="true" tabindex="-1"></a>2020](https://doi.org/10.1186/s12874-020-01105-9), which is known as the</span>
<span id="cb8-927"><a href="#cb8-927" aria-hidden="true" tabindex="-1"></a>surprisal function.</span>
<span id="cb8-928"><a href="#cb8-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-929"><a href="#cb8-929" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>     <span class="pp">|</span></span>
<span id="cb8-930"><a href="#cb8-930" aria-hidden="true" tabindex="-1"></a><span class="pp">|:----|</span></span>
<span id="cb8-931"><a href="#cb8-931" aria-hidden="true" tabindex="-1"></a><span class="pp">|</span>     <span class="pp">|</span></span>
<span id="cb8-932"><a href="#cb8-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-933"><a href="#cb8-933" aria-hidden="true" tabindex="-1"></a>The $S$-value is not meant to replace the $P$-value, and it isn't</span>
<span id="cb8-934"><a href="#cb8-934" aria-hidden="true" tabindex="-1"></a>superior to the $P$-value. It is merely a logarithmic transformation of</span>
<span id="cb8-935"><a href="#cb8-935" aria-hidden="true" tabindex="-1"></a>it that rescales it on an additive scale and tells us how much</span>
<span id="cb8-936"><a href="#cb8-936" aria-hidden="true" tabindex="-1"></a>information is embedded within the test statistic and can be used as</span>
<span id="cb8-937"><a href="#cb8-937" aria-hidden="true" tabindex="-1"></a>evidence against the test hypothesis. It is meant to be a device to help</span>
<span id="cb8-938"><a href="#cb8-938" aria-hidden="true" tabindex="-1"></a>interpret the information one obtains from a calculated $P$-value.</span>
<span id="cb8-939"><a href="#cb8-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-940"><a href="#cb8-940" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-941"><a href="#cb8-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-942"><a href="#cb8-942" aria-hidden="true" tabindex="-1"></a>I've [constructed a</span>
<span id="cb8-943"><a href="#cb8-943" aria-hidden="true" tabindex="-1"></a>calculator](https://data.lesslikely.com/concurve/articles/svalues.html)</span>
<span id="cb8-944"><a href="#cb8-944" aria-hidden="true" tabindex="-1"></a>that converts observed $P$-values into $S$-values and provides an</span>
<span id="cb8-945"><a href="#cb8-945" aria-hidden="true" tabindex="-1"></a>intuitive way to think about them. For a more detailed discussion of</span>
<span id="cb8-946"><a href="#cb8-946" aria-hidden="true" tabindex="-1"></a>$S$-values, see this <span class="co">[</span><span class="ot">article</span><span class="co">](../../../statistics/RG2020BMC)</span>, in addition to</span>
<span id="cb8-947"><a href="#cb8-947" aria-hidden="true" tabindex="-1"></a>the <span class="co">[</span><span class="ot">references</span><span class="co">](#references)</span> below them.</span>
<span id="cb8-948"><a href="#cb8-948" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-949"><a href="#cb8-949" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-950"><a href="#cb8-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-951"><a href="#cb8-951" aria-hidden="true" tabindex="-1"></a><span class="fu"># S-value Calculator</span></span>
<span id="cb8-952"><a href="#cb8-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-953"><a href="#cb8-953" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-954"><a href="#cb8-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-955"><a href="#cb8-955" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-956"><a href="#cb8-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-957"><a href="#cb8-957" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; **Acknowledgments:** I'm very grateful to [Sander</span></span>
<span id="cb8-958"><a href="#cb8-958" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Greenland](https://twitter.com/Lester_Domes) for his extensive</span></span>
<span id="cb8-959"><a href="#cb8-959" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; commentary and corrections on several versions of this article. My</span></span>
<span id="cb8-960"><a href="#cb8-960" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; acknowledgment does not imply endorsement of my views by these</span></span>
<span id="cb8-961"><a href="#cb8-961" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; colleagues, and I remain solely responsible for the views expressed</span></span>
<span id="cb8-962"><a href="#cb8-962" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; herein.</span></span>
<span id="cb8-963"><a href="#cb8-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-964"><a href="#cb8-964" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-965"><a href="#cb8-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-966"><a href="#cb8-966" aria-hidden="true" tabindex="-1"></a>The analyses were run on:</span>
<span id="cb8-967"><a href="#cb8-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-968"><a href="#cb8-968" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-969"><a href="#cb8-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-970"><a href="#cb8-970" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; R version 4.5.0 (2025-04-11)</span></span>
<span id="cb8-971"><a href="#cb8-971" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; Platform: aarch64-apple-darwin20</span></span>
<span id="cb8-972"><a href="#cb8-972" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; Running under: macOS Sequoia 15.6.1</span></span>
<span id="cb8-973"><a href="#cb8-973" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; </span></span>
<span id="cb8-974"><a href="#cb8-974" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; Matrix products: default</span></span>
<span id="cb8-975"><a href="#cb8-975" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span id="cb8-976"><a href="#cb8-976" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1</span></span>
<span id="cb8-977"><a href="#cb8-977" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; </span></span>
<span id="cb8-978"><a href="#cb8-978" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; Random number generation:</span></span>
<span id="cb8-979"><a href="#cb8-979" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  RNG:     Mersenne-Twister </span></span>
<span id="cb8-980"><a href="#cb8-980" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  Normal:  Inversion </span></span>
<span id="cb8-981"><a href="#cb8-981" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  Sample:  Rejection </span></span>
<span id="cb8-982"><a href="#cb8-982" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  </span></span>
<span id="cb8-983"><a href="#cb8-983" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; locale:</span></span>
<span id="cb8-984"><a href="#cb8-984" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8</span></span>
<span id="cb8-985"><a href="#cb8-985" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; </span></span>
<span id="cb8-986"><a href="#cb8-986" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; time zone: America/New_York</span></span>
<span id="cb8-987"><a href="#cb8-987" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; tzcode source: internal</span></span>
<span id="cb8-988"><a href="#cb8-988" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; </span></span>
<span id="cb8-989"><a href="#cb8-989" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; attached base packages:</span></span>
<span id="cb8-990"><a href="#cb8-990" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [1] splines   grid      stats4    parallel  stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span id="cb8-991"><a href="#cb8-991" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; </span></span>
<span id="cb8-992"><a href="#cb8-992" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; other attached packages:</span></span>
<span id="cb8-993"><a href="#cb8-993" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [1] pbmcapply_1.5.1       texPreview_2.1.0      tinytex_0.57          rmarkdown_2.29        brms_2.22.0          </span></span>
<span id="cb8-994"><a href="#cb8-994" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [6] bootImpute_1.2.2      knitr_1.50            boot_1.3-31           reshape2_1.4.4        ProfileLikelihood_1.3</span></span>
<span id="cb8-995"><a href="#cb8-995" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [11] ImputeRobust_1.3-1    gamlss_5.4-22         gamlss.dist_6.1-1     gamlss.data_6.0-6     mvtnorm_1.3-3        </span></span>
<span id="cb8-996"><a href="#cb8-996" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [16] performance_0.14.0    summarytools_1.1.4    tidybayes_3.0.7       htmltools_0.5.8.1     Statamarkdown_0.9.2  </span></span>
<span id="cb8-997"><a href="#cb8-997" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [21] car_3.1-3             carData_3.0-5         qqplotr_0.0.6         ggcorrplot_0.1.4.1    Amelia_1.8.3         </span></span>
<span id="cb8-998"><a href="#cb8-998" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [26] Rcpp_1.0.14           blogdown_1.21         doParallel_1.0.17     iterators_1.0.14      foreach_1.5.2        </span></span>
<span id="cb8-999"><a href="#cb8-999" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [31] lattice_0.22-7        bayesplot_1.12.0      wesanderson_0.3.7     VIM_6.2.2             colorspace_2.1-1     </span></span>
<span id="cb8-1000"><a href="#cb8-1000" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [36] here_1.0.1            progress_1.2.3        loo_2.8.0             mi_1.1                Matrix_1.7-3         </span></span>
<span id="cb8-1001"><a href="#cb8-1001" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [41] broom_1.0.8           yardstick_1.3.2       svglite_2.2.1         Cairo_1.6-2           cowplot_1.1.3        </span></span>
<span id="cb8-1002"><a href="#cb8-1002" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [46] mgcv_1.9-3            nlme_3.1-168          xfun_0.52             broom.mixed_0.2.9.6   reticulate_1.42.0    </span></span>
<span id="cb8-1003"><a href="#cb8-1003" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [51] kableExtra_1.4.0      posterior_1.6.1       checkmate_2.3.2       parallelly_1.45.0     miceFast_0.8.5       </span></span>
<span id="cb8-1004"><a href="#cb8-1004" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [56] randomForest_4.7-1.2  missForest_1.5        miceadds_3.17-44      mice_3.18.0           quantreg_6.1         </span></span>
<span id="cb8-1005"><a href="#cb8-1005" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [61] SparseM_1.84-2        MCMCpack_1.7-1        MASS_7.3-65           coda_0.19-4.1         latex2exp_0.9.6      </span></span>
<span id="cb8-1006"><a href="#cb8-1006" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [66] rstan_2.32.7          StanHeaders_2.32.10   lubridate_1.9.4       forcats_1.0.0         stringr_1.5.1        </span></span>
<span id="cb8-1007"><a href="#cb8-1007" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [71] dplyr_1.1.4           purrr_1.0.4           readr_2.1.5           tibble_3.2.1          ggplot2_3.5.2        </span></span>
<span id="cb8-1008"><a href="#cb8-1008" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [76] tidyverse_2.0.0       ggtext_0.1.2          concurve_2.7.7        showtext_0.9-7        showtextdb_3.0       </span></span>
<span id="cb8-1009"><a href="#cb8-1009" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [81] sysfonts_0.8.9        future.apply_1.11.3   future_1.58.0         tidyr_1.3.1           magrittr_2.0.3       </span></span>
<span id="cb8-1010"><a href="#cb8-1010" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; [86] rms_8.0-0             Hmisc_5.2-3          </span></span>
<span id="cb8-1011"><a href="#cb8-1011" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; </span></span>
<span id="cb8-1012"><a href="#cb8-1012" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt; loaded via a namespace (and not attached):</span></span>
<span id="cb8-1013"><a href="#cb8-1013" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;   [1] dichromat_2.0-0.1       nnet_7.3-20             TH.data_1.1-3           vctrs_0.6.5             digest_0.6.37          </span></span>
<span id="cb8-1014"><a href="#cb8-1014" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;   [6] png_0.1-8               shape_1.4.6.1           proxy_0.4-27            magick_2.8.6            fontLiberation_0.1.0   </span></span>
<span id="cb8-1015"><a href="#cb8-1015" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [11] withr_3.0.2             ggpubr_0.6.0            survival_3.8-3          doRNG_1.8.6.2           emmeans_1.11.1         </span></span>
<span id="cb8-1016"><a href="#cb8-1016" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [16] MatrixModels_0.5-4      systemfonts_1.2.3       ragg_1.4.0              zoo_1.8-14              V8_6.0.4               </span></span>
<span id="cb8-1017"><a href="#cb8-1017" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [21] ggdist_3.3.3            DEoptimR_1.1-3-1        Formula_1.2-5           prettyunits_1.2.0       rematch2_2.1.2         </span></span>
<span id="cb8-1018"><a href="#cb8-1018" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [26] httr_1.4.7              rstatix_0.7.2           globals_0.18.0          ps_1.9.1                rstudioapi_0.17.1      </span></span>
<span id="cb8-1019"><a href="#cb8-1019" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [31] extremevalues_2.4.1     pan_1.9                 generics_0.1.4          processx_3.8.6          base64enc_0.1-3        </span></span>
<span id="cb8-1020"><a href="#cb8-1020" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [36] curl_6.2.3              mitools_2.4             desc_1.4.3              xtable_1.8-4            svUnit_1.0.6           </span></span>
<span id="cb8-1021"><a href="#cb8-1021" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [41] pracma_2.4.4            evaluate_1.0.3          hms_1.1.3               glmnet_4.1-9            rcartocolor_2.1.1      </span></span>
<span id="cb8-1022"><a href="#cb8-1022" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [46] lmtest_0.9-40           robustbase_0.99-4-1     matrixStats_1.5.0       svgPanZoom_0.3.4        class_7.3-23           </span></span>
<span id="cb8-1023"><a href="#cb8-1023" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [51] pillar_1.10.2           caTools_1.18.3          compiler_4.5.0          stringi_1.8.7           jomo_2.7-6             </span></span>
<span id="cb8-1024"><a href="#cb8-1024" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [56] minqa_1.2.8             plyr_1.8.9              crayon_1.5.3            abind_1.4-8             metadat_1.4-0          </span></span>
<span id="cb8-1025"><a href="#cb8-1025" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [61] sp_2.2-0                mathjaxr_1.8-0          rapportools_1.2         twosamples_2.0.1        sandwich_3.1-1         </span></span>
<span id="cb8-1026"><a href="#cb8-1026" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [66] whisker_0.4.1           codetools_0.2-20        multcomp_1.4-28         textshaping_1.0.1       bcaboot_0.2-3          </span></span>
<span id="cb8-1027"><a href="#cb8-1027" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [71] openssl_2.3.3           flextable_0.9.9         QuickJSR_1.7.0          e1071_1.7-16            gridtext_0.1.5         </span></span>
<span id="cb8-1028"><a href="#cb8-1028" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [76] lme4_1.1-37             fs_1.6.6                itertools_0.1-3         listenv_0.9.1           Rdpack_2.6.4           </span></span>
<span id="cb8-1029"><a href="#cb8-1029" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [81] pkgbuild_1.4.8          estimability_1.5.1      ggsignif_0.6.4          callr_3.7.6             tzdb_0.5.0             </span></span>
<span id="cb8-1030"><a href="#cb8-1030" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [86] pkgconfig_2.0.3         tools_4.5.0             rbibutils_2.3           viridisLite_0.4.2       DBI_1.2.3              </span></span>
<span id="cb8-1031"><a href="#cb8-1031" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [91] numDeriv_2016.8-1.1     fastmap_1.2.0           scales_1.4.0            officer_0.6.10          opdisDownsampling_1.0.1</span></span>
<span id="cb8-1032"><a href="#cb8-1032" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [96] insight_1.3.0           rpart_4.1.24            farver_2.1.2            reformulas_0.4.1        survminer_0.5.0        </span></span>
<span id="cb8-1033"><a href="#cb8-1033" aria-hidden="true" tabindex="-1"></a><span class="in">    #&gt;  [ reached 'max' / getOption("max.print") -- omitted 55 entries ]</span></span>
<span id="cb8-1034"><a href="#cb8-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1035"><a href="#cb8-1035" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb8-1036"><a href="#cb8-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1037"><a href="#cb8-1037" aria-hidden="true" tabindex="-1"></a><span class="fu"># References</span></span>
<span id="cb8-1038"><a href="#cb8-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-1039"><a href="#cb8-1039" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span></code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2025, Less Likely</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/zadrafi/lesslikely/edit/main/posts/statistics/s-values.md" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/zadrafi/lesslikely/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/Lester_Domes">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/yourusername">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>